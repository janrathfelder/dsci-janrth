{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a63d990-b9b5-43ef-9b53-3a7263441e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from statsforecast import StatsForecast\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean\n",
    "from window_ops.ewm import ewm_mean\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.auto import AutoNHITS, AutoLSTM, LSTM\n",
    "from neuralforecast.losses.pytorch import MQLoss, RMSE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsforecast.models import (\n",
    "    # SeasonalNaive: A model that uses the previous season's data as the forecast\n",
    "    SeasonalNaive\n",
    ")\n",
    "\n",
    "def create_date_format(\n",
    "        df: pd.DataFrame, original_date_column: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a pandas datetime object from a date string.\n",
    "        Very specifiy to the output of the mdb notebook yrmo column.\n",
    "        \"\"\"\n",
    "        df[\"yyyymm\"] = df[original_date_column].apply(\n",
    "            lambda x: str(x)[:4] + \"-\" + str(x)[4:] + \"-01\"\n",
    "        )\n",
    "        df[\"yyyymm\"] = pd.to_datetime(df.yyyymm)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "102a72e4-53ef-4310-91e1-1466482a98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('s3://dsaa-cph-ai-s3-dev/jan_rathfelder/impact_data/fra_eylea_20230215.csv')\n",
    "df = create_date_format(df, 'yrmo')\n",
    "\n",
    "dfx = df.groupby(['country_cd', 'yyyymm'])[['sales_unit','f2f_calls', 'remote_calls','ae_sent','evnt_invited']].sum().reset_index()\n",
    "static_df = df.pivot_table(values='total_hcp_cnt', index=['country_cd'], columns='cstmr_1_id', aggfunc='sum', fill_value=0).sum(axis=1).reset_index(name='total_hcp_cnt')\n",
    "static_df.rename(columns={'country_cd': 'unique_id', 'yyyymm': 'ds'}, inplace=True)\n",
    "\n",
    "df_nixtla = dfx[['country_cd', 'yyyymm', 'sales_unit','f2f_calls', 'remote_calls','ae_sent','evnt_invited']]\n",
    "df_nixtla.rename(columns={'country_cd': 'unique_id', 'yyyymm': 'ds', 'sales_unit': 'y'}, inplace=True)\n",
    "\n",
    "futr_df = df_nixtla[['unique_id', 'ds', 'f2f_calls', 'remote_calls','ae_sent','evnt_invited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3167db09-7fc5-4295-9649-95bd5c180428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm search spacce:\n",
    "\n",
    "horizon=6\n",
    "\n",
    "config_lstm = dict(\n",
    "    input_size=tune.choice([horizon*1, horizon*2]),              # Length of input window\n",
    "    encoder_hidden_size=tune.randint(100, 300),            # Hidden size of LSTM cells\n",
    "    encoder_n_layers=tune.randint(2,5),                   # Number of layers in LSTM\n",
    "    encoder_dropout=tune.loguniform(0.01, 0.7),\n",
    "    decoder_hidden_size=tune.randint(100, 300),  # size of hidden layer for the MLP decoder.\n",
    "    decoder_layers=tune.randint(2, 5),   # number of layers for the MLP decoder\n",
    "    learning_rate=tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n",
    "    scaler_type=tune.choice(['robust', 'standard']),                   # Scaler type\n",
    "    max_steps=tune.randint(20, 500),                    # Max number of training iterations\n",
    "    batch_size=tune.choice([16, 32, 64, 128]),                        # Number of series in batch\n",
    "    num_lr_decays=tune.loguniform(0.01, .2),\n",
    "    # random_seed= tune.randint(1, 20),                       # Random seed\n",
    "    hist_exog_list=['f2f_calls', 'remote_calls','ae_sent','evnt_invited'],\n",
    "    futr_exog_list = ['f2f_calls', 'remote_calls','ae_sent','evnt_invited'],\n",
    "    stat_exog_list = ['total_hcp_cnt']\n",
    ")\n",
    "\n",
    "model = [AutoLSTM(h=horizon,\n",
    "                  config=config_lstm, \n",
    "                  #futr_exog_list = ['gen_forecast', 'week_day'], # <- Future exogenous variables\n",
    "                  #hist_exog_list = ['total_calls'], # <- Historical exogenous variables\n",
    "                  #stat_exog_list = ['market_0', 'market_1'], # <- Static exogenous variables\n",
    "                  loss=MQLoss(), #RMSE(),\n",
    "                  num_samples=50, # number of configurations explored --> ideally above 25,\n",
    "                  search_alg=HyperOptSearch())\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1bbae910-0132-48fa-ae22-d75df2f98e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 15.43it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.428, train_loss_epoch=0.428]         \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.347, train_loss_epoch=0.347]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.348, train_loss_epoch=0.348]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371]         \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.261, train_loss_epoch=0.261]         \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 101.13it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252]\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]         \n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 100.16it/s, v_num=0, train_loss_step=0.298, train_loss_epoch=0.298]\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.298, train_loss_epoch=0.298]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227]         \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]         \n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00, 99.87it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154] \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]        \n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 73.76it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.147]\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 75.70it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.121]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 118.84it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 37.59it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.121, valid_loss=4.45e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=4.45e+3]       \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 68.74it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=4.45e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=4.45e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=4.45e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=4.45e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=4.45e+3]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=4.45e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100, valid_loss=4.45e+3]           \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0998, train_loss_epoch=0.0998, valid_loss=4.45e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0987, train_loss_epoch=0.0987, valid_loss=4.45e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0935, train_loss_epoch=0.0935, valid_loss=4.45e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0924, train_loss_epoch=0.0924, valid_loss=4.45e+3]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0898, train_loss_epoch=0.0898, valid_loss=4.45e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0863, train_loss_epoch=0.0863, valid_loss=4.45e+3]         \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0869, train_loss_epoch=0.0869, valid_loss=4.45e+3]        \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 70.25it/s, v_num=0, train_loss_step=0.0918, train_loss_epoch=0.0869, valid_loss=4.45e+3]\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0918, train_loss_epoch=0.0918, valid_loss=4.45e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0905, train_loss_epoch=0.0905, valid_loss=4.45e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0778, train_loss_epoch=0.0778, valid_loss=4.45e+3]         \n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00, 70.84it/s, v_num=0, train_loss_step=0.0778, train_loss_epoch=0.0778, valid_loss=4.45e+3]\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0847, valid_loss=4.45e+3]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0804, train_loss_epoch=0.0804, valid_loss=4.45e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0777, train_loss_epoch=0.0777, valid_loss=4.45e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0726, train_loss_epoch=0.0726, valid_loss=4.45e+3]        \n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00, 83.33it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069, valid_loss=4.45e+3]   \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0742, train_loss_epoch=0.0742, valid_loss=4.45e+3]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0685, train_loss_epoch=0.0685, valid_loss=4.45e+3]        \n",
      "Epoch 196: 100%|██████████| 1/1 [00:00<00:00, 82.23it/s, v_num=0, train_loss_step=0.0605, train_loss_epoch=0.0635, valid_loss=4.45e+3] \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0605, train_loss_epoch=0.0605, valid_loss=4.45e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0598, train_loss_epoch=0.0598, valid_loss=4.45e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 105.99it/s, v_num=0, train_loss_step=0.0661, train_loss_epoch=0.0603, valid_loss=4.45e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 200.56it/s]\u001b[A\n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0579, train_loss_epoch=0.0579, valid_loss=5.38e+3]         \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0567, train_loss_epoch=0.0567, valid_loss=5.38e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558, valid_loss=5.38e+3]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0549, train_loss_epoch=0.0549, valid_loss=5.38e+3]         \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0519, train_loss_epoch=0.0519, valid_loss=5.38e+3]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=5.38e+3]         \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502, valid_loss=5.38e+3]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=5.38e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=5.38e+3]         \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=5.38e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:10,700\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=5.38e+3]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.38e+3]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0374, train_loss_epoch=0.0374, valid_loss=5.38e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00, 83.13it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042, valid_loss=5.38e+3]  \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.452, train_loss_epoch=0.452]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202]       \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]          \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0901, train_loss_epoch=0.0901]        \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 82.83it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0901]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0557]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023]          \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0277, train_loss_epoch=0.0277]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 85.59it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0227]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 144.24it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 43.05it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0227, valid_loss=6.18e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=6.18e+3]       \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.18e+3]        \n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 70.30it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.18e+3]\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.18e+3]        \n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00, 74.51it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.18e+3]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.18e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 69.98it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=6.18e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=6.18e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169, valid_loss=6.18e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261, valid_loss=6.18e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.18e+3]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0114, train_loss_epoch=0.0114, valid_loss=6.18e+3]        \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 63.34it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=6.18e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=6.18e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.18e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00657, train_loss_epoch=0.00657, valid_loss=6.18e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00919, train_loss_epoch=0.00919, valid_loss=6.18e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.18e+3]          \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.18e+3]          \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.18e+3]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 65.41it/s, v_num=0, train_loss_step=0.0101, train_loss_epoch=0.0101, valid_loss=6.18e+3]\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0101, train_loss_epoch=0.0101, valid_loss=6.18e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.18e+3]          \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.18e+3]          \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.18e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 67.44it/s, v_num=0, train_loss_step=0.00963, train_loss_epoch=0.0115, valid_loss=6.18e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 167.83it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00963, train_loss_epoch=0.00963, valid_loss=6.14e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=6.14e+3]          \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0171, train_loss_epoch=0.0171, valid_loss=6.14e+3]          \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=6.14e+3]        \n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 83.91it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0267, valid_loss=6.14e+3]\n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=6.14e+3]        \n",
      "Epoch 224: 100%|██████████| 1/1 [00:00<00:00, 81.13it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.14e+3]\n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.14e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00885, train_loss_epoch=0.00885, valid_loss=6.14e+3]        \n",
      "Epoch 232: 100%|██████████| 1/1 [00:00<00:00, 58.07it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.00885, valid_loss=6.14e+3] \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.14e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00718, train_loss_epoch=0.00718, valid_loss=6.14e+3]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00447, train_loss_epoch=0.00447, valid_loss=6.14e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00617, train_loss_epoch=0.00617, valid_loss=6.14e+3]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00935, train_loss_epoch=0.00935, valid_loss=6.14e+3]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0085, train_loss_epoch=0.0085, valid_loss=6.14e+3]          \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.14e+3]            \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0141, train_loss_epoch=0.0141, valid_loss=6.14e+3]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=6.14e+3]            \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.14e+3]        \n",
      "Epoch 288: 100%|██████████| 1/1 [00:00<00:00, 47.58it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=6.14e+3]\n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=6.14e+3]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=6.14e+3]        \n",
      "Epoch 297: 100%|██████████| 1/1 [00:00<00:00, 80.07it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.14e+3]\n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.14e+3]        \n",
      "Epoch 298: 100%|██████████| 1/1 [00:00<00:00, 62.81it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.14e+3]\n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.14e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 61.44it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0126, valid_loss=6.14e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.03it/s]\u001b[A\n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=6.08e+3]        \n",
      "Epoch 306: 100%|██████████| 1/1 [00:00<00:00, 83.97it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=6.08e+3]\n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=6.08e+3]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=6.08e+3]          \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.08e+3]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=6.08e+3]            \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=6.08e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=6.08e+3]        \n",
      "Epoch 333: 100%|██████████| 1/1 [00:00<00:00, 78.00it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0212, valid_loss=6.08e+3]\n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.08e+3]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=6.08e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.08e+3]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00991, train_loss_epoch=0.00991, valid_loss=6.08e+3]        \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.08e+3]          \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00714, train_loss_epoch=0.00714, valid_loss=6.08e+3]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=6.08e+3]            \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=6.08e+3]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.08e+3]        \n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=6.08e+3]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=6.08e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 86.04it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0177, valid_loss=6.08e+3]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.56it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.19e+3]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.19e+3]        \n",
      "Epoch 408: 100%|██████████| 1/1 [00:00<00:00, 84.39it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.19e+3]\n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.19e+3]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=6.19e+3]        \n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.19e+3]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0392, train_loss_epoch=0.0392, valid_loss=6.19e+3]          \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=6.19e+3]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.19e+3]        \n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.19e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:17,538\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00946, train_loss_epoch=0.00946, valid_loss=6.19e+3]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.19e+3]          \n",
      "Epoch 442: 100%|██████████| 1/1 [00:00<00:00, 79.38it/s, v_num=0, train_loss_step=0.00988, train_loss_epoch=0.00988, valid_loss=6.19e+3]\n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00988, train_loss_epoch=0.00988, valid_loss=6.19e+3]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00922, train_loss_epoch=0.00922, valid_loss=6.19e+3]        \n",
      "Epoch 452:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00702, train_loss_epoch=0.00702, valid_loss=6.19e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452: 100%|██████████| 1/1 [00:00<00:00, 59.23it/s, v_num=0, train_loss_step=0.008, train_loss_epoch=0.008, valid_loss=6.19e+3]    \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.438, train_loss_epoch=0.438]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]       \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]          \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 77.55it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103]  \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0862, train_loss_epoch=0.0862]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0675, train_loss_epoch=0.0675]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0752, train_loss_epoch=0.0752]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0511, train_loss_epoch=0.0511]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0404, train_loss_epoch=0.0404]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:18,797\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 59.70it/s, v_num=0, train_loss_step=0.0471, train_loss_epoch=0.0465]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 150.17it/s]\u001b[A\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 29.81it/s, v_num=0, train_loss_step=0.0471, train_loss_epoch=0.0471, valid_loss=5.52e+3]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]          \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0623, train_loss_epoch=0.0623]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0552, train_loss_epoch=0.0552]         \n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 115.36it/s, v_num=0, train_loss_step=0.0552, train_loss_epoch=0.0552]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0615, train_loss_epoch=0.0615]         \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 105.00it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0406, train_loss_epoch=0.0406]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0641, train_loss_epoch=0.0641]         \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.048, train_loss_epoch=0.048]           \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0515, train_loss_epoch=0.0515]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035]           \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272]         \n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 116.88it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0195]\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032]           \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 98.98it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282]         \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 99.25it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169] \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261]         \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 114.79it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0167]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 182.27it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 55.32it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0167, valid_loss=5.74e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=5.74e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=5.74e+3]         \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0246, train_loss_epoch=0.0246, valid_loss=5.74e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=5.74e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=5.74e+3]         \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 108.83it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=5.74e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=5.74e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332, valid_loss=5.74e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:20,154\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=5.74e+3]           \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 115.72it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.035, valid_loss=5.74e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=5.74e+3]         \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=5.74e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488, valid_loss=5.74e+3]         \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 99.65it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=5.74e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 67.01it/s, v_num=0, train_loss_step=0.514, train_loss_epoch=0.530]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.514, train_loss_epoch=0.514]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.491, train_loss_epoch=0.491]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.446]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.375, train_loss_epoch=0.375]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211]        \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 61.37it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.211]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241]        \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 70.74it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.383, train_loss_epoch=0.383]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]        \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 69.41it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.352, train_loss_epoch=0.352]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 63.29it/s, v_num=0, train_loss_step=0.302, train_loss_epoch=0.302]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.302, train_loss_epoch=0.302]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]        \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 73.30it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 72.49it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.139]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 75.99it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.151]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 138.85it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 40.51it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.151, valid_loss=4.87e+3]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 32.46it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123, valid_loss=4.87e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123, valid_loss=4.87e+3]       \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 69.17it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134, valid_loss=4.87e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134, valid_loss=4.87e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148, valid_loss=4.87e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=4.87e+3]        \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=4.87e+3]        \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 59.25it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=4.87e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=4.87e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0998, train_loss_epoch=0.0998, valid_loss=4.87e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=4.87e+3]          \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 56.51it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=4.87e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=4.87e+3]        \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 69.56it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=4.87e+3]\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=4.87e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0984, train_loss_epoch=0.0984, valid_loss=4.87e+3]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0912, train_loss_epoch=0.0912, valid_loss=4.87e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0889, train_loss_epoch=0.0889, valid_loss=4.87e+3]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=4.87e+3]          \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=4.87e+3]          \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 71.23it/s, v_num=0, train_loss_step=0.0858, train_loss_epoch=0.103, valid_loss=4.87e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0858, train_loss_epoch=0.0858, valid_loss=4.87e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0845, train_loss_epoch=0.0845, valid_loss=4.87e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0854, train_loss_epoch=0.0854, valid_loss=4.87e+3]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=4.87e+3]          \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=4.87e+3]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0819, train_loss_epoch=0.0819, valid_loss=4.87e+3]        \n",
      "Epoch 174: 100%|██████████| 1/1 [00:00<00:00, 69.84it/s, v_num=0, train_loss_step=0.0915, train_loss_epoch=0.0915, valid_loss=4.87e+3]\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0915, train_loss_epoch=0.0915, valid_loss=4.87e+3]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 69.07it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092, valid_loss=4.87e+3]  \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092, valid_loss=4.87e+3]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=4.87e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0833, train_loss_epoch=0.0833, valid_loss=4.87e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0941, train_loss_epoch=0.0941, valid_loss=4.87e+3]        \n",
      "Epoch 191: 100%|██████████| 1/1 [00:00<00:00, 72.06it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.0941, valid_loss=4.87e+3] \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=4.87e+3]         \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 69.47it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=4.87e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=4.87e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.0802, valid_loss=4.87e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 59.55it/s, v_num=0, train_loss_step=0.0821, train_loss_epoch=0.107, valid_loss=4.87e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 156.52it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 30.76it/s, v_num=0, train_loss_step=0.0821, train_loss_epoch=0.0821, valid_loss=4.95e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0821, train_loss_epoch=0.0821, valid_loss=4.95e+3]        \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0734, train_loss_epoch=0.0734, valid_loss=4.95e+3]        \n",
      "Epoch 207: 100%|██████████| 1/1 [00:00<00:00, 56.90it/s, v_num=0, train_loss_step=0.075, train_loss_epoch=0.075, valid_loss=4.95e+3]  \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.075, train_loss_epoch=0.075, valid_loss=4.95e+3]        \n",
      "Epoch 208: 100%|██████████| 1/1 [00:00<00:00, 69.37it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865, valid_loss=4.95e+3]\n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865, valid_loss=4.95e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0782, train_loss_epoch=0.0782, valid_loss=4.95e+3]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=4.95e+3]          \n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00, 71.27it/s, v_num=0, train_loss_step=0.077, train_loss_epoch=0.077, valid_loss=4.95e+3]\n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.077, train_loss_epoch=0.077, valid_loss=4.95e+3]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0757, train_loss_epoch=0.0757, valid_loss=4.95e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0739, train_loss_epoch=0.0739, valid_loss=4.95e+3]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0852, train_loss_epoch=0.0852, valid_loss=4.95e+3]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0714, valid_loss=4.95e+3]        \n",
      "Epoch 246: 100%|██████████| 1/1 [00:00<00:00, 69.81it/s, v_num=0, train_loss_step=0.0932, train_loss_epoch=0.0932, valid_loss=4.95e+3]\n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0932, train_loss_epoch=0.0932, valid_loss=4.95e+3]        \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 69.00it/s, v_num=0, train_loss_step=0.0713, train_loss_epoch=0.0713, valid_loss=4.95e+3]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0713, train_loss_epoch=0.0713, valid_loss=4.95e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0834, train_loss_epoch=0.0834, valid_loss=4.95e+3]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.085, train_loss_epoch=0.085, valid_loss=4.95e+3]          \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0749, train_loss_epoch=0.0749, valid_loss=4.95e+3]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0884, train_loss_epoch=0.0884, valid_loss=4.95e+3]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0937, train_loss_epoch=0.0937, valid_loss=4.95e+3]        \n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 59.48it/s, v_num=0, train_loss_step=0.0654, train_loss_epoch=0.0654, valid_loss=4.95e+3]\n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0654, train_loss_epoch=0.0654, valid_loss=4.95e+3]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0704, train_loss_epoch=0.0704, valid_loss=4.95e+3]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681, valid_loss=4.95e+3]        \n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00, 61.48it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0681, valid_loss=4.95e+3]\n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738, valid_loss=4.95e+3]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0718, train_loss_epoch=0.0718, valid_loss=4.95e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.081, train_loss_epoch=0.081, valid_loss=4.95e+3]          \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 49.34it/s, v_num=0, train_loss_step=0.0799, train_loss_epoch=0.0825, valid_loss=4.95e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 132.22it/s]\u001b[A\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767, valid_loss=5.07e+3]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.060, train_loss_epoch=0.060, valid_loss=5.07e+3]          \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0782, train_loss_epoch=0.0782, valid_loss=5.07e+3]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0582, train_loss_epoch=0.0582, valid_loss=5.07e+3]        \n",
      "Epoch 323: 100%|██████████| 1/1 [00:00<00:00, 61.44it/s, v_num=0, train_loss_step=0.0554, train_loss_epoch=0.0554, valid_loss=5.07e+3]\n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0554, train_loss_epoch=0.0554, valid_loss=5.07e+3]        \n",
      "Epoch 331: 100%|██████████| 1/1 [00:00<00:00, 69.73it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=5.07e+3]\n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=5.07e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0484, train_loss_epoch=0.0484, valid_loss=5.07e+3]        \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644, valid_loss=5.07e+3]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0757, train_loss_epoch=0.0757, valid_loss=5.07e+3]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602, valid_loss=5.07e+3]        \n",
      "Epoch 348: 100%|██████████| 1/1 [00:00<00:00, 69.04it/s, v_num=0, train_loss_step=0.0671, train_loss_epoch=0.0671, valid_loss=5.07e+3]\n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0671, train_loss_epoch=0.0671, valid_loss=5.07e+3]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0731, train_loss_epoch=0.0731, valid_loss=5.07e+3]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0731, train_loss_epoch=0.0731, valid_loss=5.07e+3]\n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0662, train_loss_epoch=0.0662, valid_loss=5.07e+3]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0522, valid_loss=5.07e+3]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0678, train_loss_epoch=0.0678, valid_loss=5.07e+3]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0546, train_loss_epoch=0.0546, valid_loss=5.07e+3]        \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0498, train_loss_epoch=0.0498, valid_loss=5.07e+3]        \n",
      "Epoch 373: 100%|██████████| 1/1 [00:00<00:00, 71.11it/s, v_num=0, train_loss_step=0.0643, train_loss_epoch=0.0643, valid_loss=5.07e+3]\n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0643, train_loss_epoch=0.0643, valid_loss=5.07e+3]        \n",
      "Epoch 374: 100%|██████████| 1/1 [00:00<00:00, 70.02it/s, v_num=0, train_loss_step=0.060, train_loss_epoch=0.060, valid_loss=5.07e+3]  \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.060, train_loss_epoch=0.060, valid_loss=5.07e+3]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0615, train_loss_epoch=0.0615, valid_loss=5.07e+3]        \n",
      "Epoch 382: 100%|██████████| 1/1 [00:00<00:00, 70.92it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=5.07e+3]  \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=5.07e+3]        \n",
      "Epoch 383: 100%|██████████| 1/1 [00:00<00:00, 70.09it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642, valid_loss=5.07e+3]\n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642, valid_loss=5.07e+3]        \n",
      "Epoch 384: 100%|██████████| 1/1 [00:00<00:00, 67.93it/s, v_num=0, train_loss_step=0.0503, train_loss_epoch=0.0503, valid_loss=5.07e+3]\n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0503, train_loss_epoch=0.0503, valid_loss=5.07e+3]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0503, train_loss_epoch=0.0503, valid_loss=5.07e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:26,998\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0617, train_loss_epoch=0.0617, valid_loss=5.07e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 73.71it/s, v_num=0, train_loss_step=0.0637, train_loss_epoch=0.0617, valid_loss=5.07e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 155.60it/s]\u001b[A\n",
      "Epoch 405: 100%|██████████| 1/1 [00:00<00:00, 75.69it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=5.23e+3]\n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0434, valid_loss=5.23e+3]        \n",
      "Epoch 406: 100%|██████████| 1/1 [00:00<00:00, 71.63it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599, valid_loss=5.23e+3]\n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599, valid_loss=5.23e+3]        \n",
      "Epoch 407: 100%|██████████| 1/1 [00:00<00:00, 70.97it/s, v_num=0, train_loss_step=0.0632, train_loss_epoch=0.0632, valid_loss=5.23e+3]\n",
      "Epoch 407: 100%|██████████| 1/1 [00:00<00:00, 65.58it/s, v_num=0, train_loss_step=0.0632, train_loss_epoch=0.0632, valid_loss=5.23e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.905, train_loss_epoch=0.905]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.519, train_loss_epoch=0.519]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.348, train_loss_epoch=0.348]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.193]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 101.32it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.182]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]         \n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00, 97.01it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186] \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 104.60it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.186]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149]         \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 106.50it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.149]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 101.59it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.113]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.68it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=5.19e+3]         \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0965, train_loss_epoch=0.0965, valid_loss=5.19e+3]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0867, train_loss_epoch=0.0867, valid_loss=5.19e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0763, train_loss_epoch=0.0763, valid_loss=5.19e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0763, train_loss_epoch=0.0763, valid_loss=5.19e+3]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0694, train_loss_epoch=0.0694, valid_loss=5.19e+3]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.067, train_loss_epoch=0.067, valid_loss=5.19e+3]          \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0653, train_loss_epoch=0.0653, valid_loss=5.19e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.061, train_loss_epoch=0.061, valid_loss=5.19e+3]           \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646, valid_loss=5.19e+3]         \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 101.75it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584, valid_loss=5.19e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584, valid_loss=5.19e+3]         \n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 96.93it/s, v_num=0, train_loss_step=0.0547, train_loss_epoch=0.0547, valid_loss=5.19e+3] \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0547, train_loss_epoch=0.0547, valid_loss=5.19e+3]        \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 103.52it/s, v_num=0, train_loss_step=0.0547, train_loss_epoch=0.0547, valid_loss=5.19e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571, valid_loss=5.19e+3]         \n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 107.42it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571, valid_loss=5.19e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0598, train_loss_epoch=0.0598, valid_loss=5.19e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506, valid_loss=5.19e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669, valid_loss=5.19e+3]         \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 98.54it/s, v_num=0, train_loss_step=0.0454, train_loss_epoch=0.0454, valid_loss=5.19e+3] \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0454, train_loss_epoch=0.0454, valid_loss=5.19e+3]        \n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 97.76it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479, valid_loss=5.19e+3] \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479, valid_loss=5.19e+3]        \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 101.23it/s, v_num=0, train_loss_step=0.045, train_loss_epoch=0.0479, valid_loss=5.19e+3] \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.045, train_loss_epoch=0.045, valid_loss=5.19e+3]          \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0455, train_loss_epoch=0.0455, valid_loss=5.19e+3]         \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338, valid_loss=5.19e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 101.00it/s, v_num=0, train_loss_step=0.0583, train_loss_epoch=0.0363, valid_loss=5.19e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 182.80it/s]\u001b[A\n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=5.98e+3]         \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0546, train_loss_epoch=0.0546, valid_loss=5.98e+3]         \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 93.34it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=5.98e+3] \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=5.98e+3]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=5.98e+3]           \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=5.98e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185, valid_loss=5.98e+3]         \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=5.98e+3]         \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=5.98e+3]         \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=5.98e+3]         \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=5.98e+3]         \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=5.98e+3]           \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=5.98e+3]\n",
      "Epoch 289: 100%|██████████| 1/1 [00:00<00:00, 95.73it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=5.98e+3]   \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=5.98e+3]        \n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00, 95.43it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=5.98e+3]\n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=5.98e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 98.93it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0163, valid_loss=5.98e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 176.03it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=6.01e+3]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=6.01e+3]         \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.01e+3]           \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.01e+3]         \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=6.01e+3]           \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00915, train_loss_epoch=0.00915, valid_loss=6.01e+3]         \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00967, train_loss_epoch=0.00967, valid_loss=6.01e+3]         \n",
      "Epoch 369: 100%|██████████| 1/1 [00:00<00:00, 96.94it/s, v_num=0, train_loss_step=0.00782, train_loss_epoch=0.00782, valid_loss=6.01e+3] \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00782, train_loss_epoch=0.00782, valid_loss=6.01e+3]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00782, train_loss_epoch=0.00782, valid_loss=6.01e+3]\n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00869, train_loss_epoch=0.00869, valid_loss=6.01e+3]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=6.01e+3]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:31,601\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=6.01e+3]           \n",
      "Epoch 393: 100%|██████████| 1/1 [00:00<00:00, 88.98it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=6.01e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.369, train_loss_epoch=0.369]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]        \n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 78.87it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0978, train_loss_epoch=0.0978]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0895, train_loss_epoch=0.0895]        \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 79.25it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0895]\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0847]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0711, train_loss_epoch=0.0711]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0387, train_loss_epoch=0.0387]        \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 67.25it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316]        \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 77.54it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0397]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279]        \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 78.12it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 67.03it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0201]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.01it/s]\u001b[A\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=6.09e+3]          \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197, valid_loss=6.09e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=6.09e+3]        \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 77.29it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=6.09e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=6.09e+3]        \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 62.67it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0188, valid_loss=6.09e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=6.09e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=6.09e+3]          \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 72.33it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0121, valid_loss=6.09e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0146, valid_loss=6.09e+3]        \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 76.16it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.09e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.09e+3]        \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 63.65it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.010, valid_loss=6.09e+3] \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=6.09e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.09e+3]        \n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 79.25it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=6.09e+3]\n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=6.09e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=6.09e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=6.09e+3]        \n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00, 75.35it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=6.09e+3]\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=6.09e+3]        \n",
      "Epoch 162: 100%|██████████| 1/1 [00:00<00:00, 75.81it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=6.09e+3]\n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=6.09e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 70.67it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.09e+3]  \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.09e+3]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=6.09e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=6.09e+3]\n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=6.09e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=6.09e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.09e+3]        \n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 73.04it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=6.09e+3]\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=6.09e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=6.09e+3]          \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 82.64it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0137, valid_loss=6.09e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.04it/s]\u001b[A\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=6.34e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.34e+3]          \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=6.34e+3]          \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 77.82it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.018, valid_loss=6.34e+3]\n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=6.34e+3]        \n",
      "Epoch 212: 100%|██████████| 1/1 [00:00<00:00, 75.42it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=6.34e+3]  \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=6.34e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0174, valid_loss=6.34e+3]          \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=6.34e+3]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=6.34e+3]        \n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00, 64.39it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=6.34e+3]  \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.34e+3]        \n",
      "Epoch 237: 100%|██████████| 1/1 [00:00<00:00, 73.11it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=6.34e+3]\n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=6.34e+3]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=6.34e+3]            \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=6.34e+3]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=6.34e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.34e+3]        \n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00, 80.99it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0125, valid_loss=6.34e+3]\n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.34e+3]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00964, train_loss_epoch=0.00964, valid_loss=6.34e+3]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=6.34e+3]          \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=6.34e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.34e+3]          \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00822, train_loss_epoch=0.00822, valid_loss=6.34e+3]        \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 76.43it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.34e+3]    \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.34e+3]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.34e+3]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.34e+3]\n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.34e+3]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=6.34e+3]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.34e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:36,567\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 81.66it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0131, valid_loss=6.34e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.33it/s]\u001b[A\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=6.41e+3]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.41e+3]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.41e+3]        \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.41e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315: 100%|██████████| 1/1 [00:00<00:00, 71.33it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=6.41e+3]  \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.590, train_loss_epoch=0.590]         \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.583, train_loss_epoch=0.583]         \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.583, train_loss_epoch=0.583]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.406, train_loss_epoch=0.406]         \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 102.89it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.341]        \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 102.19it/s, v_num=0, train_loss_step=0.390, train_loss_epoch=0.390]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.390, train_loss_epoch=0.390]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.385]         \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 103.99it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297]         \n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 99.98it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317] \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353]         \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 105.83it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.353]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.360, train_loss_epoch=0.360]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 107.16it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.291]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.314]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 109.38it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]\n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 106.65it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.295, train_loss_epoch=0.295]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 109.63it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 107.48it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.267]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 103.87it/s, v_num=0, train_loss_step=0.212, train_loss_epoch=0.212]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.212, train_loss_epoch=0.212]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.205, train_loss_epoch=0.205]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.199, train_loss_epoch=0.199]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.199, train_loss_epoch=0.199]\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.217, train_loss_epoch=0.217]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.179, train_loss_epoch=0.179]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]         \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 108.46it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.130]\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 104.05it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0881, train_loss_epoch=0.0881]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 106.35it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0881]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0784]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0711, train_loss_epoch=0.0711]         \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 102.81it/s, v_num=0, train_loss_step=0.0708, train_loss_epoch=0.0708]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0708, train_loss_epoch=0.0708]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0686, train_loss_epoch=0.0686]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0654, train_loss_epoch=0.0654]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0608, train_loss_epoch=0.0608]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 109.37it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0445]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575]         \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 110.01it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381]         \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 108.06it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0381]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0369]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0374, train_loss_epoch=0.0374]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 103.63it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.038, train_loss_epoch=0.038]          \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0402, train_loss_epoch=0.0402]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 112.17it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0322]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.53it/s]\u001b[A\n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 111.08it/s, v_num=0, train_loss_step=0.0339, train_loss_epoch=0.0339, valid_loss=5.75e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=5.75e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=5.75e+3]           \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 103.54it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=5.75e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=5.75e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=5.75e+3]         \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=5.75e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0203, train_loss_epoch=0.0203, valid_loss=5.75e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=5.75e+3]         \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=5.75e+3]         \n",
      "Epoch 132: 100%|██████████| 1/1 [00:00<00:00, 107.89it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0186, valid_loss=5.75e+3]\n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=5.75e+3]         \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=5.75e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=5.75e+3]         \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 110.50it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=5.75e+3]\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=5.75e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=5.75e+3]         \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 102.75it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=5.75e+3]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=5.75e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=5.75e+3]         \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=5.75e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=5.75e+3]           \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=5.75e+3]         \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=5.75e+3]         \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 110.45it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=5.75e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=5.75e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=5.75e+3]         \n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00, 103.16it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185, valid_loss=5.75e+3]\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185, valid_loss=5.75e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=5.75e+3]          \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0233, train_loss_epoch=0.0233, valid_loss=5.75e+3]         \n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00, 107.06it/s, v_num=0, train_loss_step=0.0229, train_loss_epoch=0.0233, valid_loss=5.75e+3]\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0229, train_loss_epoch=0.0229, valid_loss=5.75e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331, valid_loss=5.75e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=5.75e+3]         \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=5.75e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 96.96it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0193, valid_loss=5.75e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.77it/s]\u001b[A\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=5.61e+3]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=5.61e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=5.61e+3]           \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=5.61e+3]         \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=5.61e+3]           \n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 104.14it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=5.61e+3]\n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=5.61e+3]         \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=5.61e+3]         \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=5.61e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=5.61e+3]           \n",
      "Epoch 241: 100%|██████████| 1/1 [00:00<00:00, 103.97it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=5.61e+3]\n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=5.61e+3]         \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=5.61e+3]          \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=5.61e+3]         \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=5.61e+3]           \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=5.61e+3]           \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=5.61e+3]           \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=5.61e+3]         \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=5.61e+3]           \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=5.61e+3]         \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=5.61e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0138, train_loss_epoch=0.0138, valid_loss=5.61e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 95.43it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0113, valid_loss=5.61e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 178.41it/s]\u001b[A\n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00967, train_loss_epoch=0.00967, valid_loss=5.9e+3]        \n",
      "Epoch 306: 100%|██████████| 1/1 [00:00<00:00, 91.16it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=5.9e+3]    \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=5.9e+3]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=5.9e+3]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=5.9e+3]           \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00, 104.03it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.9e+3]\n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.9e+3]         \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.0112, valid_loss=5.9e+3]         \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=5.9e+3]         \n",
      "Epoch 332: 100%|██████████| 1/1 [00:00<00:00, 94.69it/s, v_num=0, train_loss_step=0.00786, train_loss_epoch=0.00892, valid_loss=5.9e+3] \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00786, train_loss_epoch=0.00786, valid_loss=5.9e+3]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=5.9e+3]          \n",
      "Epoch 343: 100%|██████████| 1/1 [00:00<00:00, 95.66it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.00821, valid_loss=5.9e+3] \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=5.9e+3]         \n",
      "Epoch 344: 100%|██████████| 1/1 [00:00<00:00, 94.75it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0119, valid_loss=5.9e+3]\n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=5.9e+3]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=5.9e+3]          \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=5.9e+3]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=5.9e+3]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.9e+3]        \n",
      "Epoch 375: 100%|██████████| 1/1 [00:00<00:00, 83.04it/s, v_num=0, train_loss_step=0.00944, train_loss_epoch=0.00944, valid_loss=5.9e+3] \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00944, train_loss_epoch=0.00944, valid_loss=5.9e+3]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00944, train_loss_epoch=0.00944, valid_loss=5.9e+3]\n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00895, train_loss_epoch=0.00895, valid_loss=5.9e+3]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=5.9e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:41,046\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=5.9e+3]         \n",
      "Epoch 394: 100%|██████████| 1/1 [00:00<00:00, 95.95it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=5.9e+3]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]        \n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 75.39it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 80.59it/s, v_num=0, train_loss_step=0.0752, train_loss_epoch=0.0752]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0752, train_loss_epoch=0.0752]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.073, train_loss_epoch=0.073]          \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553]        \n",
      "Epoch 31: 100%|██████████| 1/1 [00:00<00:00, 66.50it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.0553] \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053]         \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 76.83it/s, v_num=0, train_loss_step=0.0491, train_loss_epoch=0.0491]\n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0491, train_loss_epoch=0.0491]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0491, train_loss_epoch=0.0491]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0359, train_loss_epoch=0.0359]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0334]        \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 66.06it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211]\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029]          \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0181]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142]        \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 73.09it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014]  \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105]        \n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 81.31it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132]\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135]          \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 65.04it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 88.74it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0113]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.35it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=5.96e+3]          \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 84.32it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0151, valid_loss=5.96e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=5.96e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=5.96e+3]          \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=5.96e+3]          \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00965, train_loss_epoch=0.00965, valid_loss=5.96e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0092, train_loss_epoch=0.0092, valid_loss=5.96e+3]          \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0092, train_loss_epoch=0.0092, valid_loss=5.96e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00798, train_loss_epoch=0.00798, valid_loss=5.96e+3]        \n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 78.47it/s, v_num=0, train_loss_step=0.00798, train_loss_epoch=0.00798, valid_loss=5.96e+3]\n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0074, train_loss_epoch=0.0074, valid_loss=5.96e+3]          \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=5.96e+3]            \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00897, train_loss_epoch=0.00897, valid_loss=5.96e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00702, train_loss_epoch=0.00702, valid_loss=5.96e+3]        \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 81.03it/s, v_num=0, train_loss_step=0.00896, train_loss_epoch=0.00896, valid_loss=5.96e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00896, train_loss_epoch=0.00896, valid_loss=5.96e+3]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=5.96e+3]          \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00953, train_loss_epoch=0.00953, valid_loss=5.96e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 69.22it/s, v_num=0, train_loss_step=0.00899, train_loss_epoch=0.00899, valid_loss=5.96e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00899, train_loss_epoch=0.00899, valid_loss=5.96e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00829, train_loss_epoch=0.00829, valid_loss=5.96e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:43,924\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00657, train_loss_epoch=0.00657, valid_loss=5.96e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0092, train_loss_epoch=0.0092, valid_loss=5.96e+3]          \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00858, train_loss_epoch=0.00858, valid_loss=5.96e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 62.07it/s, v_num=0, train_loss_step=0.00696, train_loss_epoch=0.00696, valid_loss=5.96e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 104.38it/s, v_num=0, train_loss_step=0.568, train_loss_epoch=0.568]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.568, train_loss_epoch=0.568]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.909, train_loss_epoch=0.909]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:44,362\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.635, train_loss_epoch=0.635]         \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 110.43it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.733]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.86it/s]\u001b[A\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 40.72it/s, v_num=0, train_loss_step=0.899, train_loss_epoch=0.899, valid_loss=5.91e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.398, train_loss_epoch=0.398]        \n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 76.49it/s, v_num=0, train_loss_step=0.485, train_loss_epoch=0.398]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.485, train_loss_epoch=0.485]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237]        \n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 77.06it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265]\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265]        \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 77.63it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]          \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0975, train_loss_epoch=0.0975]        \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 88.66it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]  \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0812, train_loss_epoch=0.0812]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0844, train_loss_epoch=0.0844]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793]        \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 81.44it/s, v_num=0, train_loss_step=0.0768, train_loss_epoch=0.0793]\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0768, train_loss_epoch=0.0768]        \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 82.61it/s, v_num=0, train_loss_step=0.0768, train_loss_epoch=0.0768]\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0951, train_loss_epoch=0.0951]        \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 78.75it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0726, train_loss_epoch=0.0726]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0829, train_loss_epoch=0.0829]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0814, train_loss_epoch=0.0814]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0633, train_loss_epoch=0.0633]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 88.06it/s, v_num=0, train_loss_step=0.071, train_loss_epoch=0.071]  \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.071, train_loss_epoch=0.071]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0634, train_loss_epoch=0.0634]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0565, train_loss_epoch=0.0565]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 94.68it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0567]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.11it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 48.76it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0567, valid_loss=5.52e+3]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 37.95it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548, valid_loss=5.52e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548, valid_loss=5.52e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0535, valid_loss=5.52e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=5.52e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=5.52e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=5.52e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=5.52e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 91.42it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0264, valid_loss=5.52e+3]\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=5.52e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0346, train_loss_epoch=0.0346, valid_loss=5.52e+3]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=5.52e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0274, valid_loss=5.52e+3]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033, valid_loss=5.52e+3]          \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=5.52e+3]          \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356, valid_loss=5.52e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0258, train_loss_epoch=0.0258, valid_loss=5.52e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=5.52e+3]        \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 91.14it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0281, valid_loss=5.52e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=5.52e+3]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306, valid_loss=5.52e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=5.52e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=5.52e+3]          \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 86.16it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=5.52e+3]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=5.52e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=5.52e+3]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=5.52e+3]          \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=5.52e+3]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=5.52e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.016, train_loss_epoch=0.016, valid_loss=5.52e+3]          \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=5.52e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0171, train_loss_epoch=0.0171, valid_loss=5.52e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 94.52it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0143, valid_loss=5.52e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 185.11it/s]\u001b[A\n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=6.05e+3]          \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=6.05e+3]        \n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00, 88.35it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.05e+3]\n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.05e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.05e+3]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=6.05e+3]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=6.05e+3]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 88.64it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=6.05e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=6.05e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=6.05e+3]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.05e+3]          \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=6.05e+3]        \n",
      "Epoch 230: 100%|██████████| 1/1 [00:00<00:00, 92.70it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.0105, valid_loss=6.05e+3] \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.05e+3]         \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00907, train_loss_epoch=0.00907, valid_loss=6.05e+3]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00905, train_loss_epoch=0.00905, valid_loss=6.05e+3]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.05e+3]            \n",
      "Epoch 242: 100%|██████████| 1/1 [00:00<00:00, 88.49it/s, v_num=0, train_loss_step=0.00946, train_loss_epoch=0.00946, valid_loss=6.05e+3]\n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00946, train_loss_epoch=0.00946, valid_loss=6.05e+3]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=6.05e+3]          \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00932, train_loss_epoch=0.00932, valid_loss=6.05e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00889, train_loss_epoch=0.00889, valid_loss=6.05e+3]        \n",
      "Epoch 254: 100%|██████████| 1/1 [00:00<00:00, 89.65it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.05e+3]  \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.05e+3]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6.05e+3]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00663, train_loss_epoch=0.00663, valid_loss=6.05e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00665, train_loss_epoch=0.00665, valid_loss=6.05e+3]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 86.63it/s, v_num=0, train_loss_step=0.00918, train_loss_epoch=0.00918, valid_loss=6.05e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00918, train_loss_epoch=0.00918, valid_loss=6.05e+3]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.00645, valid_loss=6.05e+3]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0146, valid_loss=6.05e+3]          \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.05e+3]        \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00652, train_loss_epoch=0.00652, valid_loss=6.05e+3]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.05e+3]          \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00648, train_loss_epoch=0.00648, valid_loss=6.05e+3]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.05e+3]          \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00695, train_loss_epoch=0.00695, valid_loss=6.05e+3]        \n",
      "Epoch 298: 100%|██████████| 1/1 [00:00<00:00, 88.86it/s, v_num=0, train_loss_step=0.00666, train_loss_epoch=0.00666, valid_loss=6.05e+3]\n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00666, train_loss_epoch=0.00666, valid_loss=6.05e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 89.61it/s, v_num=0, train_loss_step=0.00643, train_loss_epoch=0.00666, valid_loss=6.05e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 182.62it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 49.01it/s, v_num=0, train_loss_step=0.00643, train_loss_epoch=0.00666, valid_loss=6.17e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00643, train_loss_epoch=0.00643, valid_loss=6.17e+3]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.17e+3]          \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00661, train_loss_epoch=0.00661, valid_loss=6.17e+3]        \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00, 93.26it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.00797, valid_loss=6.17e+3] \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=6.17e+3]         \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.17e+3]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00698, train_loss_epoch=0.00698, valid_loss=6.17e+3]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.17e+3]          \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=6.17e+3]          \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00, 82.58it/s, v_num=0, train_loss_step=0.00927, train_loss_epoch=0.00927, valid_loss=6.17e+3]\n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00927, train_loss_epoch=0.00927, valid_loss=6.17e+3]        \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.009, train_loss_epoch=0.009, valid_loss=6.17e+3]            \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00636, train_loss_epoch=0.00636, valid_loss=6.17e+3]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00813, train_loss_epoch=0.00813, valid_loss=6.17e+3]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00571, train_loss_epoch=0.00571, valid_loss=6.17e+3]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00607, train_loss_epoch=0.00607, valid_loss=6.17e+3]        \n",
      "Epoch 362: 100%|██████████| 1/1 [00:00<00:00, 91.47it/s, v_num=0, train_loss_step=0.00607, train_loss_epoch=0.00607, valid_loss=6.17e+3]\n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.00645, valid_loss=6.17e+3]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00646, train_loss_epoch=0.00646, valid_loss=6.17e+3]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0138, train_loss_epoch=0.0138, valid_loss=6.17e+3]          \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=6.17e+3]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00694, train_loss_epoch=0.00694, valid_loss=6.17e+3]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0063, train_loss_epoch=0.0063, valid_loss=6.17e+3]          \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00594, train_loss_epoch=0.00594, valid_loss=6.17e+3]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00991, train_loss_epoch=0.00991, valid_loss=6.17e+3]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00525, train_loss_epoch=0.00525, valid_loss=6.17e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 93.20it/s, v_num=0, train_loss_step=0.0061, train_loss_epoch=0.00918, valid_loss=6.17e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 183.74it/s]\u001b[A\n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=6.15e+3]          \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=6.15e+3]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00786, train_loss_epoch=0.00786, valid_loss=6.15e+3]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00781, train_loss_epoch=0.00781, valid_loss=6.15e+3]        \n",
      "Epoch 414: 100%|██████████| 1/1 [00:00<00:00, 88.39it/s, v_num=0, train_loss_step=0.00662, train_loss_epoch=0.00662, valid_loss=6.15e+3]\n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00662, train_loss_epoch=0.00662, valid_loss=6.15e+3]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00796, train_loss_epoch=0.00796, valid_loss=6.15e+3]        \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231, valid_loss=6.15e+3]          \n",
      "Epoch 426:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00703, train_loss_epoch=0.00703, valid_loss=6.15e+3]        \n",
      "Epoch 426: 100%|██████████| 1/1 [00:00<00:00, 88.45it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.15e+3]  \n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.15e+3]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=6.15e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:50,020\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00527, train_loss_epoch=0.00527, valid_loss=6.15e+3]        \n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00844, train_loss_epoch=0.00844, valid_loss=6.15e+3]        \n",
      "Epoch 438: 100%|██████████| 1/1 [00:00<00:00, 87.28it/s, v_num=0, train_loss_step=0.00476, train_loss_epoch=0.00476, valid_loss=6.15e+3]\n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00476, train_loss_epoch=0.00476, valid_loss=6.15e+3]        \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.15e+3]          \n",
      "Epoch 448: 100%|██████████| 1/1 [00:00<00:00, 84.16it/s, v_num=0, train_loss_step=0.00952, train_loss_epoch=0.00952, valid_loss=6.15e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.397, train_loss_epoch=0.397]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.508, train_loss_epoch=0.508]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.446]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.439, train_loss_epoch=0.439]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]        \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 76.37it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 74.81it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.363]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278]        \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 59.87it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]        \n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 82.57it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.098, train_loss_epoch=0.098]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0991, train_loss_epoch=0.0991]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]          \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 84.80it/s, v_num=0, train_loss_step=0.0999, train_loss_epoch=0.124]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 150.64it/s]\u001b[A\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=4.31e+3]          \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=4.31e+3]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=4.31e+3]        \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 82.40it/s, v_num=0, train_loss_step=0.0923, train_loss_epoch=0.0923, valid_loss=4.31e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0923, train_loss_epoch=0.0923, valid_loss=4.31e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842, valid_loss=4.31e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0847, valid_loss=4.31e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0914, train_loss_epoch=0.0914, valid_loss=4.31e+3]        \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 81.70it/s, v_num=0, train_loss_step=0.0653, train_loss_epoch=0.0653, valid_loss=4.31e+3]\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0653, train_loss_epoch=0.0653, valid_loss=4.31e+3]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0839, train_loss_epoch=0.0839, valid_loss=4.31e+3]        \n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 81.27it/s, v_num=0, train_loss_step=0.0682, train_loss_epoch=0.0682, valid_loss=4.31e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0682, train_loss_epoch=0.0682, valid_loss=4.31e+3]        \n",
      "Epoch 155: 100%|██████████| 1/1 [00:00<00:00, 70.25it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793, valid_loss=4.31e+3]\n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793, valid_loss=4.31e+3]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 70.58it/s, v_num=0, train_loss_step=0.074, train_loss_epoch=0.074, valid_loss=4.31e+3]  \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.074, train_loss_epoch=0.074, valid_loss=4.31e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0639, train_loss_epoch=0.0639, valid_loss=4.31e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0755, train_loss_epoch=0.0755, valid_loss=4.31e+3]        \n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 87.35it/s, v_num=0, train_loss_step=0.0656, train_loss_epoch=0.0656, valid_loss=4.31e+3]\n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 85.49it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0656, valid_loss=4.31e+3]\n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=4.31e+3]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652, valid_loss=4.31e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.054, train_loss_epoch=0.054, valid_loss=4.31e+3]          \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 80.96it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=4.31e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=4.31e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0514, train_loss_epoch=0.0514, valid_loss=4.31e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 86.49it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0449, valid_loss=4.31e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.59it/s]\u001b[A\n",
      "Epoch 201: 100%|██████████| 1/1 [00:00<00:00, 74.83it/s, v_num=0, train_loss_step=0.0494, train_loss_epoch=0.0551, valid_loss=5.04e+3]\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0494, train_loss_epoch=0.0494, valid_loss=5.04e+3]        \n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 72.25it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.04e+3]\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.04e+3]        \n",
      "Epoch 203: 100%|██████████| 1/1 [00:00<00:00, 74.80it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0603, valid_loss=5.04e+3]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=5.04e+3]        \n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00, 74.60it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0415, valid_loss=5.04e+3]\n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502, valid_loss=5.04e+3]        \n",
      "Epoch 205: 100%|██████████| 1/1 [00:00<00:00, 76.57it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502, valid_loss=5.04e+3]\n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0555, train_loss_epoch=0.0555, valid_loss=5.04e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=5.04e+3]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529, valid_loss=5.04e+3]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=5.04e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:53,437\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0448, train_loss_epoch=0.0448, valid_loss=5.04e+3]        \n",
      "Epoch 242: 100%|██████████| 1/1 [00:00<00:00, 76.14it/s, v_num=0, train_loss_step=0.045, train_loss_epoch=0.045, valid_loss=5.04e+3]  \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268]         \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 98.54it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268] \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.414, train_loss_epoch=0.414]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0987, train_loss_epoch=0.0987]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0917, train_loss_epoch=0.0917]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]          \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0954, train_loss_epoch=0.0954]         \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 97.64it/s, v_num=0, train_loss_step=0.0862, train_loss_epoch=0.0862] \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0862, train_loss_epoch=0.0862]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0816, train_loss_epoch=0.0816]         \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 96.98it/s, v_num=0, train_loss_step=0.0911, train_loss_epoch=0.0911] \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0911, train_loss_epoch=0.0911]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0753, train_loss_epoch=0.0753]         \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 101.39it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0753]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0796]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0752, train_loss_epoch=0.0752]         \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0821, train_loss_epoch=0.0821]         \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 97.57it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627] \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627]        \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 102.63it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0627]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 101.90it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0501]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 155.59it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=5.87e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=5.87e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0393, train_loss_epoch=0.0393, valid_loss=5.87e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0463, train_loss_epoch=0.0463, valid_loss=5.87e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0492, train_loss_epoch=0.0492, valid_loss=5.87e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=5.87e+3]         \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=5.87e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462, valid_loss=5.87e+3]         \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0386, valid_loss=5.87e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=5.87e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=5.87e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:55,381\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166: 100%|██████████| 1/1 [00:00<00:00, 99.60it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436, valid_loss=5.87e+3] \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.837, train_loss_epoch=0.837]         \n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 97.80it/s, v_num=0, train_loss_step=0.695, train_loss_epoch=0.837] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.695, train_loss_epoch=0.695]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.734, train_loss_epoch=0.734]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.754, train_loss_epoch=0.754]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.767, train_loss_epoch=0.767]         \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 105.10it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683]\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 111.90it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.644, train_loss_epoch=0.644]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.782, train_loss_epoch=0.782]         \n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 105.55it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749]\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.710, train_loss_epoch=0.710]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.673, train_loss_epoch=0.673]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.673, train_loss_epoch=0.673]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.732, train_loss_epoch=0.732]         \n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00, 112.86it/s, v_num=0, train_loss_step=0.732, train_loss_epoch=0.732]\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.683, train_loss_epoch=0.683]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.727, train_loss_epoch=0.727]         \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 106.18it/s, v_num=0, train_loss_step=0.609, train_loss_epoch=0.609]\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.609, train_loss_epoch=0.609]         \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.743, train_loss_epoch=0.743]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.709, train_loss_epoch=0.709]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672]         \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 106.37it/s, v_num=0, train_loss_step=0.702, train_loss_epoch=0.702]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.702, train_loss_epoch=0.702]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.676, train_loss_epoch=0.676]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.660, train_loss_epoch=0.660]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.565, train_loss_epoch=0.565]         \n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 105.78it/s, v_num=0, train_loss_step=0.494, train_loss_epoch=0.494]\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.494, train_loss_epoch=0.494]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.488, train_loss_epoch=0.488]         \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.514, train_loss_epoch=0.514]         \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 93.16it/s, v_num=0, train_loss_step=0.480, train_loss_epoch=0.480] \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.480, train_loss_epoch=0.480]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.527, train_loss_epoch=0.527]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 96.14it/s, v_num=0, train_loss_step=0.503, train_loss_epoch=0.527]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 152.57it/s]\u001b[A\n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 105.47it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472, valid_loss=4.92e+3]\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472, valid_loss=4.92e+3]         \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.467, train_loss_epoch=0.467, valid_loss=4.92e+3]         \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.483, train_loss_epoch=0.483, valid_loss=4.92e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.343, train_loss_epoch=0.343, valid_loss=4.92e+3]         \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 110.25it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.343, valid_loss=4.92e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323, valid_loss=4.92e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353, valid_loss=4.92e+3]         \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 106.09it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=4.92e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=4.92e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.382, train_loss_epoch=0.382, valid_loss=4.92e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310, valid_loss=4.92e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346, valid_loss=4.92e+3]         \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 110.56it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.346, valid_loss=4.92e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308, valid_loss=4.92e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.306, train_loss_epoch=0.306, valid_loss=4.92e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278, valid_loss=4.92e+3]         \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332, valid_loss=4.92e+3]         \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 109.74it/s, v_num=0, train_loss_step=0.266, train_loss_epoch=0.332, valid_loss=4.92e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.266, train_loss_epoch=0.266, valid_loss=4.92e+3]         \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276, valid_loss=4.92e+3]         \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 105.28it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262, valid_loss=4.92e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262, valid_loss=4.92e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322, valid_loss=4.92e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300, valid_loss=4.92e+3]         \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=4.92e+3]         \n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00, 104.95it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289, valid_loss=4.92e+3]\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289, valid_loss=4.92e+3]         \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.270, train_loss_epoch=0.270, valid_loss=4.92e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265, valid_loss=4.92e+3]         \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263, valid_loss=4.92e+3]         \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244, valid_loss=4.92e+3]         \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.236, train_loss_epoch=0.236, valid_loss=4.92e+3]         \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 104.35it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208, valid_loss=4.92e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208, valid_loss=4.92e+3]         \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218, valid_loss=4.92e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 98.27it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201, valid_loss=4.92e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 175.08it/s]\u001b[A\n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 94.25it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201, valid_loss=4.92e+3] \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201, valid_loss=4.92e+3]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=4.92e+3]         \n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00, 101.37it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=4.92e+3]\n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209, valid_loss=4.92e+3]         \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=4.92e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:57,827\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186, valid_loss=4.92e+3]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182, valid_loss=4.92e+3]         \n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 86.61it/s, v_num=0, train_loss_step=0.189, train_loss_epoch=0.189, valid_loss=4.92e+3] \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296]        \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 96.79it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.409]        \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 96.41it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.390, train_loss_epoch=0.390]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.374, train_loss_epoch=0.374]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.369, train_loss_epoch=0.369]        \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 90.57it/s, v_num=0, train_loss_step=0.450, train_loss_epoch=0.450]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.450, train_loss_epoch=0.450]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]        \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 91.73it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.261, train_loss_epoch=0.261]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.313, train_loss_epoch=0.313]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.385]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]        \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 97.28it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.313, train_loss_epoch=0.313]        \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 97.54it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 95.76it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.223]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.16it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.223, valid_loss=5.56e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=5.56e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190, valid_loss=5.56e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264, valid_loss=5.56e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144, valid_loss=5.56e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.143, train_loss_epoch=0.143, valid_loss=5.56e+3]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=5.56e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=5.56e+3]        \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 95.22it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.103, valid_loss=5.56e+3]\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169, valid_loss=5.56e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=5.56e+3]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=5.56e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:45:59,696\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=5.56e+3]        \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 84.45it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=5.56e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 116.72it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.691]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.691]         \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.521, train_loss_epoch=0.521]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.494, train_loss_epoch=0.494]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]        \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 82.78it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.339, train_loss_epoch=0.339]        \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 125.07it/s, v_num=0, train_loss_step=0.339, train_loss_epoch=0.339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:00,091\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 121.85it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.311]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.90it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 45.33it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=4.34e+3]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 43.39it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256, valid_loss=4.34e+3]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 69.53it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]        \n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 68.84it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]        \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 69.54it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 68.75it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.186]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.193]       \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0976, train_loss_epoch=0.0976]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]          \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.0802]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0707, train_loss_epoch=0.0707]        \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0643, train_loss_epoch=0.0643]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506]        \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 56.18it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0452, train_loss_epoch=0.0452]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0388, train_loss_epoch=0.0388]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0367, train_loss_epoch=0.0367]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283]\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0291]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 63.65it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196]        \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 64.14it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 61.90it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0167]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 131.21it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=6.11e+3]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=6.11e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.11e+3]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00893, train_loss_epoch=0.00893, valid_loss=6.11e+3]        \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 56.30it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.11e+3]  \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.11e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00824, train_loss_epoch=0.00824, valid_loss=6.11e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00945, train_loss_epoch=0.00945, valid_loss=6.11e+3]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=6.11e+3]          \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.11e+3]          \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00976, train_loss_epoch=0.00976, valid_loss=6.11e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00967, train_loss_epoch=0.00967, valid_loss=6.11e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00553, train_loss_epoch=0.00553, valid_loss=6.11e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00541, train_loss_epoch=0.00541, valid_loss=6.11e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00808, train_loss_epoch=0.00808, valid_loss=6.11e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00465, train_loss_epoch=0.00465, valid_loss=6.11e+3]        \n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00, 69.87it/s, v_num=0, train_loss_step=0.00623, train_loss_epoch=0.00465, valid_loss=6.11e+3]\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00623, train_loss_epoch=0.00623, valid_loss=6.11e+3]        \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 68.32it/s, v_num=0, train_loss_step=0.00596, train_loss_epoch=0.00596, valid_loss=6.11e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00596, train_loss_epoch=0.00596, valid_loss=6.11e+3]        \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 67.68it/s, v_num=0, train_loss_step=0.00677, train_loss_epoch=0.00677, valid_loss=6.11e+3]\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00677, train_loss_epoch=0.00677, valid_loss=6.11e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 67.78it/s, v_num=0, train_loss_step=0.00676, train_loss_epoch=0.00676, valid_loss=6.11e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00676, train_loss_epoch=0.00676, valid_loss=6.11e+3]        \n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00, 68.89it/s, v_num=0, train_loss_step=0.00659, train_loss_epoch=0.00659, valid_loss=6.11e+3]\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00659, train_loss_epoch=0.00659, valid_loss=6.11e+3]        \n",
      "Epoch 178: 100%|██████████| 1/1 [00:00<00:00, 61.00it/s, v_num=0, train_loss_step=0.00664, train_loss_epoch=0.00664, valid_loss=6.11e+3]\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00664, train_loss_epoch=0.00664, valid_loss=6.11e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00886, train_loss_epoch=0.00886, valid_loss=6.11e+3]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00691, train_loss_epoch=0.00691, valid_loss=6.11e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00823, train_loss_epoch=0.00823, valid_loss=6.11e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 73.26it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.00395, valid_loss=6.11e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.07it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 41.72it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.00395, valid_loss=6.23e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.0078, valid_loss=6.23e+3]         \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00417, train_loss_epoch=0.00417, valid_loss=6.23e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0036, train_loss_epoch=0.0036, valid_loss=6.23e+3]          \n",
      "Epoch 208: 100%|██████████| 1/1 [00:00<00:00, 70.61it/s, v_num=0, train_loss_step=0.0036, train_loss_epoch=0.0036, valid_loss=6.23e+3]\n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00405, train_loss_epoch=0.00405, valid_loss=6.23e+3]        \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 69.97it/s, v_num=0, train_loss_step=0.00682, train_loss_epoch=0.00405, valid_loss=6.23e+3]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00682, train_loss_epoch=0.00682, valid_loss=6.23e+3]        \n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 69.12it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.00682, valid_loss=6.23e+3]\n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.00645, valid_loss=6.23e+3]        \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 69.73it/s, v_num=0, train_loss_step=0.00479, train_loss_epoch=0.00645, valid_loss=6.23e+3]\n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00479, train_loss_epoch=0.00479, valid_loss=6.23e+3]        \n",
      "Epoch 212: 100%|██████████| 1/1 [00:00<00:00, 69.01it/s, v_num=0, train_loss_step=0.00709, train_loss_epoch=0.00479, valid_loss=6.23e+3]\n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00709, train_loss_epoch=0.00709, valid_loss=6.23e+3]        \n",
      "Epoch 213: 100%|██████████| 1/1 [00:00<00:00, 67.77it/s, v_num=0, train_loss_step=0.00523, train_loss_epoch=0.00523, valid_loss=6.23e+3]\n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00523, train_loss_epoch=0.00523, valid_loss=6.23e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00733, train_loss_epoch=0.00733, valid_loss=6.23e+3]        \n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 71.32it/s, v_num=0, train_loss_step=0.00733, train_loss_epoch=0.00733, valid_loss=6.23e+3]\n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00678, train_loss_epoch=0.00678, valid_loss=6.23e+3]        \n",
      "Epoch 222: 100%|██████████| 1/1 [00:00<00:00, 69.81it/s, v_num=0, train_loss_step=0.00592, train_loss_epoch=0.00678, valid_loss=6.23e+3]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00592, train_loss_epoch=0.00592, valid_loss=6.23e+3]        \n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 69.80it/s, v_num=0, train_loss_step=0.00508, train_loss_epoch=0.00592, valid_loss=6.23e+3]\n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00508, train_loss_epoch=0.00508, valid_loss=6.23e+3]        \n",
      "Epoch 224: 100%|██████████| 1/1 [00:00<00:00, 69.51it/s, v_num=0, train_loss_step=0.00702, train_loss_epoch=0.00508, valid_loss=6.23e+3]\n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00702, train_loss_epoch=0.00702, valid_loss=6.23e+3]        \n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00, 67.49it/s, v_num=0, train_loss_step=0.00806, train_loss_epoch=0.00806, valid_loss=6.23e+3]\n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00806, train_loss_epoch=0.00806, valid_loss=6.23e+3]        \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 67.91it/s, v_num=0, train_loss_step=0.00562, train_loss_epoch=0.00562, valid_loss=6.23e+3]\n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00562, train_loss_epoch=0.00562, valid_loss=6.23e+3]        \n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 67.91it/s, v_num=0, train_loss_step=0.00443, train_loss_epoch=0.00443, valid_loss=6.23e+3]\n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00443, train_loss_epoch=0.00443, valid_loss=6.23e+3]        \n",
      "Epoch 228: 100%|██████████| 1/1 [00:00<00:00, 67.94it/s, v_num=0, train_loss_step=0.00711, train_loss_epoch=0.00711, valid_loss=6.23e+3]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00711, train_loss_epoch=0.00711, valid_loss=6.23e+3]        \n",
      "Epoch 229: 100%|██████████| 1/1 [00:00<00:00, 68.46it/s, v_num=0, train_loss_step=0.00555, train_loss_epoch=0.00555, valid_loss=6.23e+3]\n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00555, train_loss_epoch=0.00555, valid_loss=6.23e+3]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00509, train_loss_epoch=0.00509, valid_loss=6.23e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00417, train_loss_epoch=0.00417, valid_loss=6.23e+3]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0047, train_loss_epoch=0.0047, valid_loss=6.23e+3]          \n",
      "Epoch 246: 100%|██████████| 1/1 [00:00<00:00, 68.59it/s, v_num=0, train_loss_step=0.00778, train_loss_epoch=0.00778, valid_loss=6.23e+3]\n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00778, train_loss_epoch=0.00778, valid_loss=6.23e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00571, train_loss_epoch=0.00571, valid_loss=6.23e+3]        \n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00, 71.00it/s, v_num=0, train_loss_step=0.0039, train_loss_epoch=0.0039, valid_loss=6.23e+3]  \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00547, train_loss_epoch=0.00547, valid_loss=6.23e+3]        \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 68.25it/s, v_num=0, train_loss_step=0.00619, train_loss_epoch=0.00547, valid_loss=6.23e+3]\n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00619, train_loss_epoch=0.00619, valid_loss=6.23e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:04,814\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269: 100%|██████████| 1/1 [00:00<00:00, 67.03it/s, v_num=0, train_loss_step=0.0029, train_loss_epoch=0.0029, valid_loss=6.23e+3]  \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0029, train_loss_epoch=0.0029, valid_loss=6.23e+3]        \n",
      "Epoch 270: 100%|██████████| 1/1 [00:00<00:00, 66.41it/s, v_num=0, train_loss_step=0.00314, train_loss_epoch=0.00314, valid_loss=6.23e+3]\n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00314, train_loss_epoch=0.00314, valid_loss=6.23e+3]        \n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00, 66.56it/s, v_num=0, train_loss_step=0.00327, train_loss_epoch=0.00327, valid_loss=6.23e+3]\n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00327, train_loss_epoch=0.00327, valid_loss=6.23e+3]        \n",
      "Epoch 272: 100%|██████████| 1/1 [00:00<00:00, 66.75it/s, v_num=0, train_loss_step=0.00392, train_loss_epoch=0.00392, valid_loss=6.23e+3]\n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00392, train_loss_epoch=0.00392, valid_loss=6.23e+3]        \n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 65.96it/s, v_num=0, train_loss_step=0.00523, train_loss_epoch=0.00523, valid_loss=6.23e+3]\n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00523, train_loss_epoch=0.00523, valid_loss=6.23e+3]        \n",
      "Epoch 274: 100%|██████████| 1/1 [00:00<00:00, 66.71it/s, v_num=0, train_loss_step=0.00492, train_loss_epoch=0.00492, valid_loss=6.23e+3]\n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00492, train_loss_epoch=0.00492, valid_loss=6.23e+3]        \n",
      "Epoch 275: 100%|██████████| 1/1 [00:00<00:00, 66.67it/s, v_num=0, train_loss_step=0.00355, train_loss_epoch=0.00355, valid_loss=6.23e+3]\n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00355, train_loss_epoch=0.00355, valid_loss=6.23e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 55.79it/s, v_num=0, train_loss_step=0.00403, train_loss_epoch=0.00403, valid_loss=6.23e+3]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                              \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184]        \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 93.65it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.096, train_loss_epoch=0.096]           \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 96.49it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.096]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0825, train_loss_epoch=0.0825]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0886, train_loss_epoch=0.0886]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 96.18it/s, v_num=0, train_loss_step=0.0733, train_loss_epoch=0.0767]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0733, train_loss_epoch=0.0733]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0667, train_loss_epoch=0.0667]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0648, train_loss_epoch=0.0648]         \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 97.83it/s, v_num=0, train_loss_step=0.0648, train_loss_epoch=0.0648]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 97.70it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0652]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 165.61it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 39.44it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607, valid_loss=5.68e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607, valid_loss=5.68e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=5.68e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 81.62it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536, valid_loss=5.68e+3] \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536, valid_loss=5.68e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529, valid_loss=5.68e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 97.30it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0371, valid_loss=5.68e+3]\n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 93.71it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=5.68e+3]\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=5.68e+3]        \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 80.28it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=5.68e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=5.68e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0492, train_loss_epoch=0.0492, valid_loss=5.68e+3]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0426, train_loss_epoch=0.0426, valid_loss=5.68e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0336, valid_loss=5.68e+3]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=5.68e+3]        \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 82.57it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=5.68e+3]   \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=5.68e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=5.68e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0308, train_loss_epoch=0.0308, valid_loss=5.68e+3]         \n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00, 96.38it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0308, valid_loss=5.68e+3]\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0291, valid_loss=5.68e+3]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=5.68e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=5.68e+3]         \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197, valid_loss=5.68e+3]         \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=5.68e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 85.46it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0151, valid_loss=5.68e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.79it/s]\u001b[A\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=6.09e+3]        \n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 96.52it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0153, valid_loss=6.09e+3]\n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 92.95it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=6.09e+3]\n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=6.09e+3]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=6.09e+3]        \n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 92.99it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=6.09e+3]\n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=6.09e+3]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0171, train_loss_epoch=0.0171, valid_loss=6.09e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.09e+3]          \n",
      "Epoch 240: 100%|██████████| 1/1 [00:00<00:00, 86.27it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0124, valid_loss=6.09e+3]\n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.09e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 78.84it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.09e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.09e+3]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=6.09e+3]         \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=6.09e+3]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=6.09e+3]            \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=6.09e+3]          \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00771, train_loss_epoch=0.00771, valid_loss=6.09e+3]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.09e+3]          \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 80.68it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.09e+3]\n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.09e+3]        \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 74.39it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.09e+3]     \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.09e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 85.71it/s, v_num=0, train_loss_step=0.00835, train_loss_epoch=0.00799, valid_loss=6.09e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 163.23it/s]\u001b[A\n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00652, train_loss_epoch=0.00652, valid_loss=6.27e+3]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0082, train_loss_epoch=0.0082, valid_loss=6.27e+3]           \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.27e+3]           \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00665, train_loss_epoch=0.00665, valid_loss=6.27e+3]         \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00547, train_loss_epoch=0.00547, valid_loss=6.27e+3]         \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00646, train_loss_epoch=0.00646, valid_loss=6.27e+3]        \n",
      "Epoch 345: 100%|██████████| 1/1 [00:00<00:00, 63.57it/s, v_num=0, train_loss_step=0.00725, train_loss_epoch=0.00725, valid_loss=6.27e+3]\n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00725, train_loss_epoch=0.00725, valid_loss=6.27e+3]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00628, train_loss_epoch=0.00628, valid_loss=6.27e+3]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00604, train_loss_epoch=0.00604, valid_loss=6.27e+3]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0051, train_loss_epoch=0.0051, valid_loss=6.27e+3]          \n",
      "Epoch 366: 100%|██████████| 1/1 [00:00<00:00, 99.35it/s, v_num=0, train_loss_step=0.00637, train_loss_epoch=0.00637, valid_loss=6.27e+3]\n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00606, train_loss_epoch=0.00606, valid_loss=6.27e+3]        \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00593, train_loss_epoch=0.00593, valid_loss=6.27e+3]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00772, train_loss_epoch=0.00772, valid_loss=6.27e+3]         \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=6.27e+3]           \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00911, train_loss_epoch=0.00911, valid_loss=6.27e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:10,015\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 97.40it/s, v_num=0, train_loss_step=0.00847, train_loss_epoch=0.0136, valid_loss=6.27e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 186.58it/s]\u001b[A\n",
      "Epoch 406: 100%|██████████| 1/1 [00:00<00:00, 97.60it/s, v_num=0, train_loss_step=0.00864, train_loss_epoch=0.00882, valid_loss=6.18e+3]\n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00864, train_loss_epoch=0.00864, valid_loss=6.18e+3]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00958, train_loss_epoch=0.00958, valid_loss=6.18e+3]        \n",
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 85.75it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.0112, valid_loss=6.18e+3]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.839, train_loss_epoch=0.839]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.997, train_loss_epoch=0.997]         \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.808, train_loss_epoch=0.808]         \n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 120.18it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.808]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.833, train_loss_epoch=0.833]         \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.783, train_loss_epoch=0.783]         \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.571, train_loss_epoch=0.571]         \n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 120.07it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.571]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322]         \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213]         \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 114.40it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 103.89it/s, v_num=0, train_loss_step=0.392, train_loss_epoch=0.392]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.392, train_loss_epoch=0.392]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 104.15it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171]         \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 103.96it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168]         \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 110.97it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100]         \n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00, 114.50it/s, v_num=0, train_loss_step=0.0943, train_loss_epoch=0.0943]\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0943, train_loss_epoch=0.0943]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]           \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0822, train_loss_epoch=0.0822]         \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 121.24it/s, v_num=0, train_loss_step=0.0795, train_loss_epoch=0.0822]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0795, train_loss_epoch=0.0795]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0998, train_loss_epoch=0.0998]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0831, train_loss_epoch=0.0831]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069]           \n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 115.09it/s, v_num=0, train_loss_step=0.0658, train_loss_epoch=0.0658]\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0658, train_loss_epoch=0.0658]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0914, train_loss_epoch=0.0914]         \n",
      "Epoch 66: 100%|██████████| 1/1 [00:00<00:00, 115.62it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722]\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722]         \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0828, train_loss_epoch=0.0828]         \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 115.36it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301]         \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0566, train_loss_epoch=0.0566]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325]         \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 113.63it/s, v_num=0, train_loss_step=0.0326, train_loss_epoch=0.0326]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0326, train_loss_epoch=0.0326]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0396, train_loss_epoch=0.0396]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288]         \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 118.23it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0288]\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331]         \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0539, train_loss_epoch=0.0539]         \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 124.47it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0278]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 183.14it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=6e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=6e+3]         \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=6e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=6e+3]         \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 114.23it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=6e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=6e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328, valid_loss=6e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0497, train_loss_epoch=0.0497, valid_loss=6e+3]         \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 119.88it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0497, valid_loss=6e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=6e+3]         \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0458, train_loss_epoch=0.0458, valid_loss=6e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0533, train_loss_epoch=0.0533, valid_loss=6e+3]         \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185, valid_loss=6e+3]         \n",
      "Epoch 132: 100%|██████████| 1/1 [00:00<00:00, 113.92it/s, v_num=0, train_loss_step=0.0172, train_loss_epoch=0.0172, valid_loss=6e+3]\n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0172, train_loss_epoch=0.0172, valid_loss=6e+3]         \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=6e+3]         \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=6e+3]         \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 110.35it/s, v_num=0, train_loss_step=0.0499, train_loss_epoch=0.0499, valid_loss=6e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0499, train_loss_epoch=0.0499, valid_loss=6e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=6e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0577, train_loss_epoch=0.0577, valid_loss=6e+3]         \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.021, train_loss_epoch=0.021, valid_loss=6e+3]           \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 111.48it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=6e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=6e+3]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0556, train_loss_epoch=0.0556, valid_loss=6e+3]         \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=6e+3]         \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 120.90it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0215, valid_loss=6e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=6e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0345, train_loss_epoch=0.0345, valid_loss=6e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=6e+3]         \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 121.22it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0297, valid_loss=6e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=6e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=6e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=6e+3]         \n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00, 123.62it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=6e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=6e+3]         \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=6e+3]         \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0146, valid_loss=6e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:12,004\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=6e+3]         \n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00, 114.87it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6e+3]\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6e+3]         \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=6e+3]         \n",
      "Epoch 189: 100%|██████████| 1/1 [00:00<00:00, 105.39it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.0149, valid_loss=6e+3] \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042, valid_loss=6e+3]          \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=6e+3]         \n",
      "Epoch 191: 100%|██████████| 1/1 [00:00<00:00, 107.62it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0184, valid_loss=6e+3]\n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=6e+3]         \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0519, train_loss_epoch=0.0519, valid_loss=6e+3]         \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 108.15it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0519, valid_loss=6e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=6e+3]         \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 94.30it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6e+3] \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.406, train_loss_epoch=0.406]        \n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 116.77it/s, v_num=0, train_loss_step=0.461, train_loss_epoch=0.461]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.461, train_loss_epoch=0.461]         \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127]         \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0979, train_loss_epoch=0.0979]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]           \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 121.64it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0977, train_loss_epoch=0.0977]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0626, train_loss_epoch=0.0626]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0645, train_loss_epoch=0.0645]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0606, train_loss_epoch=0.0606]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0443, train_loss_epoch=0.0443]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0636, train_loss_epoch=0.0636]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0379, train_loss_epoch=0.0379]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0369]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017]          \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 90.97it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.017]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032]          \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 89.85it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.032]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168]        \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 91.13it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0168]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018]          \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 89.49it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.018]\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0334]        \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 90.93it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0334]\n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142]        \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 86.39it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 85.94it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 90.36it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0156]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 132.41it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 42.91it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0156, valid_loss=5.51e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=5.51e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=5.51e+3]        \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 92.99it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=5.51e+3]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=5.51e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=5.51e+3]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=5.51e+3]          \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 92.89it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=5.51e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.51e+3]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0187, train_loss_epoch=0.0187, valid_loss=5.51e+3]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=5.51e+3]        \n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 92.52it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=5.51e+3]\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0181, valid_loss=5.51e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=5.51e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=5.51e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=5.51e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=5.51e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=5.51e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=5.51e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=5.51e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=5.51e+3]         \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=5.51e+3]         \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=5.51e+3]         \n",
      "Epoch 128: 100%|██████████| 1/1 [00:00<00:00, 128.38it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0183, valid_loss=5.51e+3]\n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=5.51e+3]         \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=5.51e+3]         \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=5.51e+3]         \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 122.67it/s, v_num=0, train_loss_step=0.00807, train_loss_epoch=0.00807, valid_loss=5.51e+3]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00807, train_loss_epoch=0.00807, valid_loss=5.51e+3]         \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00747, train_loss_epoch=0.00747, valid_loss=5.51e+3]         \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0081, train_loss_epoch=0.0081, valid_loss=5.51e+3]           \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00638, train_loss_epoch=0.00638, valid_loss=5.51e+3]         \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 133.11it/s, v_num=0, train_loss_step=0.00638, train_loss_epoch=0.00638, valid_loss=5.51e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=5.51e+3]           \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0065, train_loss_epoch=0.0065, valid_loss=5.51e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00912, train_loss_epoch=0.00912, valid_loss=5.51e+3]         \n",
      "Epoch 138: 100%|██████████| 1/1 [00:00<00:00, 123.42it/s, v_num=0, train_loss_step=0.00766, train_loss_epoch=0.00766, valid_loss=5.51e+3]\n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00766, train_loss_epoch=0.00766, valid_loss=5.51e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=5.51e+3]           \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=5.51e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00777, train_loss_epoch=0.00777, valid_loss=5.51e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00741, train_loss_epoch=0.00741, valid_loss=5.51e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.51e+3]           \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00543, train_loss_epoch=0.00543, valid_loss=5.51e+3]         \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 122.68it/s, v_num=0, train_loss_step=0.00515, train_loss_epoch=0.00515, valid_loss=5.51e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00515, train_loss_epoch=0.00515, valid_loss=5.51e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00529, train_loss_epoch=0.00529, valid_loss=5.51e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0053, train_loss_epoch=0.0053, valid_loss=5.51e+3]           \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00529, train_loss_epoch=0.00529, valid_loss=5.51e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00576, train_loss_epoch=0.00576, valid_loss=5.51e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00775, train_loss_epoch=0.00775, valid_loss=5.51e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00562, train_loss_epoch=0.00562, valid_loss=5.51e+3]         \n",
      "Epoch 174: 100%|██████████| 1/1 [00:00<00:00, 120.85it/s, v_num=0, train_loss_step=0.00551, train_loss_epoch=0.00551, valid_loss=5.51e+3]\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00551, train_loss_epoch=0.00551, valid_loss=5.51e+3]         \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00451, train_loss_epoch=0.00451, valid_loss=5.51e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:13,971\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=5.51e+3]           \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 121.18it/s, v_num=0, train_loss_step=0.00641, train_loss_epoch=0.00641, valid_loss=5.51e+3]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00641, train_loss_epoch=0.00641, valid_loss=5.51e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=5.51e+3]           \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=5.51e+3]         \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0085, train_loss_epoch=0.0085, valid_loss=5.51e+3]         \n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 110.00it/s, v_num=0, train_loss_step=0.00609, train_loss_epoch=0.00609, valid_loss=5.51e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.769, train_loss_epoch=0.769]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.477, train_loss_epoch=0.477]         \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.418, train_loss_epoch=0.418]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.308, train_loss_epoch=0.308]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]         \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 98.05it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182] \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]         \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 99.48it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162] \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]         \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 103.79it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.167]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.158, train_loss_epoch=0.158]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 99.17it/s, v_num=0, train_loss_step=0.0975, train_loss_epoch=0.0975]\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0975, train_loss_epoch=0.0975]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0881, train_loss_epoch=0.0881]         \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 100.35it/s, v_num=0, train_loss_step=0.0859, train_loss_epoch=0.0859]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0859, train_loss_epoch=0.0859]         \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0869, train_loss_epoch=0.0869]         \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 102.45it/s, v_num=0, train_loss_step=0.0823, train_loss_epoch=0.0869]\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0823, train_loss_epoch=0.0823]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0982, train_loss_epoch=0.0982]         \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0596, train_loss_epoch=0.0596]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0744, train_loss_epoch=0.0744]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0411, train_loss_epoch=0.0411]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0392, train_loss_epoch=0.0392]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:15,279\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479]         \n",
      "Epoch 92: 100%|██████████| 1/1 [00:00<00:00, 100.17it/s, v_num=0, train_loss_step=0.0467, train_loss_epoch=0.0467]\n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0467, train_loss_epoch=0.0467]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0366, train_loss_epoch=0.0366]         \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 103.55it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0366]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 170.26it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 53.39it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0366, valid_loss=6.12e+3]\n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 39.72it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=6.12e+3]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.885, train_loss_epoch=0.885]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n",
      "2023-10-31 14:46:15,627\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.687, train_loss_epoch=0.687]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.769, train_loss_epoch=0.769]        \n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 91.30it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786]\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.786, train_loss_epoch=0.786]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.758, train_loss_epoch=0.758]        \n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 106.38it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.713]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 173.75it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 41.31it/s, v_num=0, train_loss_step=0.691, train_loss_epoch=0.691, valid_loss=5.79e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.779, train_loss_epoch=0.779]         \n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 104.11it/s, v_num=0, train_loss_step=0.641, train_loss_epoch=0.779]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.641, train_loss_epoch=0.641]         \n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 104.62it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.400]\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394]         \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.409]         \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 105.20it/s, v_num=0, train_loss_step=0.408, train_loss_epoch=0.409]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.408, train_loss_epoch=0.408]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.330, train_loss_epoch=0.330]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 105.06it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.330]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.341]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 104.91it/s, v_num=0, train_loss_step=0.356, train_loss_epoch=0.341]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.356, train_loss_epoch=0.356]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 104.24it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.309]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.341, train_loss_epoch=0.341]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346]         \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 107.30it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 105.18it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.301]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249]         \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 104.86it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.249]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]         \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 106.96it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]         \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 107.93it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]         \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 107.80it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]\n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 105.11it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.146]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00, 106.60it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]         \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 107.77it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]         \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 107.70it/s, v_num=0, train_loss_step=0.083, train_loss_epoch=0.083]  \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.087, train_loss_epoch=0.087]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0783, train_loss_epoch=0.0783]         \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 105.40it/s, v_num=0, train_loss_step=0.0917, train_loss_epoch=0.0783]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0917, train_loss_epoch=0.0917]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0785, train_loss_epoch=0.0785]         \n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 104.87it/s, v_num=0, train_loss_step=0.0725, train_loss_epoch=0.0785]\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0725, train_loss_epoch=0.0725]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 104.34it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0539]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0606, train_loss_epoch=0.0606]         \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 101.29it/s, v_num=0, train_loss_step=0.0579, train_loss_epoch=0.0579]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0579, train_loss_epoch=0.0579]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0425, train_loss_epoch=0.0425]         \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 100.41it/s, v_num=0, train_loss_step=0.0364, train_loss_epoch=0.0364]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0364, train_loss_epoch=0.0364]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0387, train_loss_epoch=0.0387]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 104.00it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0387]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0324]         \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 105.52it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0324]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459]         \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 107.96it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447]         \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 104.71it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0239]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.41it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 40.81it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=6.67e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=6.67e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195, valid_loss=6.67e+3]         \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.67e+3]         \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=6.67e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=6.67e+3]           \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129, valid_loss=6.67e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.67e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.67e+3]         \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.67e+3]         \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.67e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.67e+3]         \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.67e+3]           \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00865, train_loss_epoch=0.00865, valid_loss=6.67e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0181, valid_loss=6.67e+3]           \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.67e+3]           \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=6.67e+3]         \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00983, train_loss_epoch=0.00983, valid_loss=6.67e+3]         \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00705, train_loss_epoch=0.00705, valid_loss=6.67e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00875, train_loss_epoch=0.00875, valid_loss=6.67e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0146, valid_loss=6.67e+3]           \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=6.67e+3]             \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=6.67e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=6.67e+3]        \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 89.86it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=6.67e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=6.67e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=6.67e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 94.71it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.00934, valid_loss=6.67e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 194.66it/s]\u001b[A\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.56e+3]         \n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00, 108.03it/s, v_num=0, train_loss_step=0.00767, train_loss_epoch=0.00767, valid_loss=6.56e+3]\n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00914, train_loss_epoch=0.00914, valid_loss=6.56e+3]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0067, train_loss_epoch=0.0067, valid_loss=6.56e+3]           \n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 104.61it/s, v_num=0, train_loss_step=0.0067, train_loss_epoch=0.0067, valid_loss=6.56e+3]\n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=6.56e+3]         \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00877, train_loss_epoch=0.00877, valid_loss=6.56e+3]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00677, train_loss_epoch=0.00677, valid_loss=6.56e+3]         \n",
      "Epoch 228: 100%|██████████| 1/1 [00:00<00:00, 105.18it/s, v_num=0, train_loss_step=0.00677, train_loss_epoch=0.00677, valid_loss=6.56e+3]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=6.56e+3]           \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00749, train_loss_epoch=0.00749, valid_loss=6.56e+3]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00923, train_loss_epoch=0.00923, valid_loss=6.56e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00843, train_loss_epoch=0.00843, valid_loss=6.56e+3]         \n",
      "Epoch 241: 100%|██████████| 1/1 [00:00<00:00, 100.06it/s, v_num=0, train_loss_step=0.00817, train_loss_epoch=0.00817, valid_loss=6.56e+3]\n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00817, train_loss_epoch=0.00817, valid_loss=6.56e+3]         \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00767, train_loss_epoch=0.00767, valid_loss=6.56e+3]         \n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 100.89it/s, v_num=0, train_loss_step=0.00694, train_loss_epoch=0.00694, valid_loss=6.56e+3]\n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00694, train_loss_epoch=0.00694, valid_loss=6.56e+3]         \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00817, train_loss_epoch=0.00817, valid_loss=6.56e+3]         \n",
      "Epoch 245: 100%|██████████| 1/1 [00:00<00:00, 99.45it/s, v_num=0, train_loss_step=0.00696, train_loss_epoch=0.00696, valid_loss=6.56e+3] \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00696, train_loss_epoch=0.00696, valid_loss=6.56e+3]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=6.56e+3]           \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 104.15it/s, v_num=0, train_loss_step=0.00885, train_loss_epoch=0.0119, valid_loss=6.56e+3]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00885, train_loss_epoch=0.00885, valid_loss=6.56e+3]         \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00979, train_loss_epoch=0.00979, valid_loss=6.56e+3]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00555, train_loss_epoch=0.00555, valid_loss=6.56e+3]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.56e+3]          \n",
      "Epoch 259: 100%|██████████| 1/1 [00:00<00:00, 90.75it/s, v_num=0, train_loss_step=0.00593, train_loss_epoch=0.00593, valid_loss=6.56e+3]\n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00593, train_loss_epoch=0.00593, valid_loss=6.56e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00559, train_loss_epoch=0.00559, valid_loss=6.56e+3]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00508, train_loss_epoch=0.00508, valid_loss=6.56e+3]         \n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00, 106.68it/s, v_num=0, train_loss_step=0.00487, train_loss_epoch=0.00508, valid_loss=6.56e+3]\n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00487, train_loss_epoch=0.00487, valid_loss=6.56e+3]         \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00554, train_loss_epoch=0.00554, valid_loss=6.56e+3]         \n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 106.69it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.00554, valid_loss=6.56e+3] \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.56e+3]          \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.56e+3]         \n",
      "Epoch 275: 100%|██████████| 1/1 [00:00<00:00, 101.89it/s, v_num=0, train_loss_step=0.00705, train_loss_epoch=0.00705, valid_loss=6.56e+3]\n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00705, train_loss_epoch=0.00705, valid_loss=6.56e+3]         \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00745, train_loss_epoch=0.00745, valid_loss=6.56e+3]        \n",
      "Epoch 287: 100%|██████████| 1/1 [00:00<00:00, 102.56it/s, v_num=0, train_loss_step=0.00595, train_loss_epoch=0.00595, valid_loss=6.56e+3]\n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00595, train_loss_epoch=0.00595, valid_loss=6.56e+3]         \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.56e+3]           \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00934, train_loss_epoch=0.00934, valid_loss=6.56e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 106.53it/s, v_num=0, train_loss_step=0.00816, train_loss_epoch=0.0102, valid_loss=6.56e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 196.38it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 55.06it/s, v_num=0, train_loss_step=0.00816, train_loss_epoch=0.0102, valid_loss=6.67e+3] \n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00816, train_loss_epoch=0.00816, valid_loss=6.67e+3]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.67e+3]           \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00845, train_loss_epoch=0.00845, valid_loss=6.67e+3]         \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00643, train_loss_epoch=0.00643, valid_loss=6.67e+3]         \n",
      "Epoch 312: 100%|██████████| 1/1 [00:00<00:00, 106.79it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.00643, valid_loss=6.67e+3] \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.0078, valid_loss=6.67e+3]          \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00857, train_loss_epoch=0.00857, valid_loss=6.67e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:19,316\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00634, train_loss_epoch=0.00634, valid_loss=6.67e+3]        \n",
      "Epoch 323: 100%|██████████| 1/1 [00:00<00:00, 92.21it/s, v_num=0, train_loss_step=0.00784, train_loss_epoch=0.00784, valid_loss=6.67e+3] \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.854, train_loss_epoch=0.854]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.806, train_loss_epoch=0.806]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.768, train_loss_epoch=0.768]         \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.783, train_loss_epoch=0.783]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.783, train_loss_epoch=0.783]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.757, train_loss_epoch=0.757]         \n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 99.18it/s, v_num=0, train_loss_step=0.668, train_loss_epoch=0.668] \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.668, train_loss_epoch=0.668]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.696, train_loss_epoch=0.696]         \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 99.72it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740] \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.753, train_loss_epoch=0.753]         \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 102.09it/s, v_num=0, train_loss_step=0.676, train_loss_epoch=0.753]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.676, train_loss_epoch=0.676]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.718, train_loss_epoch=0.718]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.509, train_loss_epoch=0.509]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.618, train_loss_epoch=0.618]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.508, train_loss_epoch=0.508]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.436, train_loss_epoch=0.436]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.368, train_loss_epoch=0.368]         \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 98.10it/s, v_num=0, train_loss_step=0.327, train_loss_epoch=0.327] \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.327, train_loss_epoch=0.327]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300]         \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 104.51it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:20,186\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.310, train_loss_epoch=0.310]         \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 98.15it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247] \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 105.80it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.277, train_loss_epoch=0.277]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242]         \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 104.19it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.227]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 167.00it/s]\u001b[A\n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 40.14it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=4.39e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.954, train_loss_epoch=0.954]         \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.685, train_loss_epoch=0.685]         \n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 107.51it/s, v_num=0, train_loss_step=0.717, train_loss_epoch=0.717]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.717, train_loss_epoch=0.717]         \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.798, train_loss_epoch=0.798]         \n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 108.20it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.834, train_loss_epoch=0.834]         \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.607, train_loss_epoch=0.607]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.590, train_loss_epoch=0.590]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.536, train_loss_epoch=0.536]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.543]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]         \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.404, train_loss_epoch=0.404]         \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 109.29it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.298, train_loss_epoch=0.298]         \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 109.07it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.344, train_loss_epoch=0.344]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.281, train_loss_epoch=0.281]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.236, train_loss_epoch=0.236]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.203, train_loss_epoch=0.203]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]         \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 108.80it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]         \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]         \n",
      "Epoch 89: 100%|██████████| 1/1 [00:00<00:00, 101.98it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]         \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 113.86it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.151]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 173.55it/s]\u001b[A\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=5.48e+3]        \n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=5.48e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=5.48e+3]         \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=5.48e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=5.48e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127, valid_loss=5.48e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115, valid_loss=5.48e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.132, valid_loss=5.48e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=5.48e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=5.48e+3]         \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=5.48e+3]           \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=5.48e+3]         \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897, valid_loss=5.48e+3]         \n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00, 107.39it/s, v_num=0, train_loss_step=0.0928, train_loss_epoch=0.0928, valid_loss=5.48e+3]\n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0928, train_loss_epoch=0.0928, valid_loss=5.48e+3]         \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=5.48e+3]           \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.088, train_loss_epoch=0.088, valid_loss=5.48e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0963, train_loss_epoch=0.0963, valid_loss=5.48e+3]         \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0766, train_loss_epoch=0.0766, valid_loss=5.48e+3]         \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0676, train_loss_epoch=0.0676, valid_loss=5.48e+3]         \n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00, 107.22it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716, valid_loss=5.48e+3]\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716, valid_loss=5.48e+3]         \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0709, train_loss_epoch=0.0709, valid_loss=5.48e+3]         \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0703, train_loss_epoch=0.0703, valid_loss=5.48e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 113.23it/s, v_num=0, train_loss_step=0.0786, train_loss_epoch=0.0611, valid_loss=5.48e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 199.24it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0786, train_loss_epoch=0.0786, valid_loss=6431.5]          \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0737, train_loss_epoch=0.0737, valid_loss=6431.5]         \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 107.12it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563, valid_loss=6431.5]\n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563, valid_loss=6431.5]         \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0539, train_loss_epoch=0.0539, valid_loss=6431.5]         \n",
      "Epoch 213: 100%|██████████| 1/1 [00:00<00:00, 107.62it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=6431.5]  \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=6431.5]         \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0518, train_loss_epoch=0.0518, valid_loss=6431.5]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0517, train_loss_epoch=0.0517, valid_loss=6431.5]         \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=6431.5]         \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=6431.5]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:22,729\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238: 100%|██████████| 1/1 [00:00<00:00, 98.99it/s, v_num=0, train_loss_step=0.0387, train_loss_epoch=0.0387, valid_loss=6431.5] \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 71.75it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.847, train_loss_epoch=0.847]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.759, train_loss_epoch=0.759]         \n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 80.49it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672]\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.672, train_loss_epoch=0.672]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.704, train_loss_epoch=0.704]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.747, train_loss_epoch=0.747]        \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 86.57it/s, v_num=0, train_loss_step=0.591, train_loss_epoch=0.591] \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.591, train_loss_epoch=0.591]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.552, train_loss_epoch=0.552]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.670, train_loss_epoch=0.670]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:23,399\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 86.59it/s, v_num=0, train_loss_step=0.476, train_loss_epoch=0.476] \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.476, train_loss_epoch=0.476]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.429, train_loss_epoch=0.429]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386]        \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 121.14it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.383]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 183.82it/s]\u001b[A\n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 43.43it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299, valid_loss=4.2e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.754, train_loss_epoch=0.754]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.429, train_loss_epoch=0.429]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.314]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 93.79it/s, v_num=0, train_loss_step=0.313, train_loss_epoch=0.313] \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.313, train_loss_epoch=0.313]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253]         \n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 102.33it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.253]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 100.36it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 100.09it/s, v_num=0, train_loss_step=0.0976, train_loss_epoch=0.0976]\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0976, train_loss_epoch=0.0976]         \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0885, train_loss_epoch=0.0885]         \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 105.32it/s, v_num=0, train_loss_step=0.0885, train_loss_epoch=0.0885]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0878, train_loss_epoch=0.0878]         \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0864, train_loss_epoch=0.0864]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0519, train_loss_epoch=0.0519]         \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 99.14it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551] \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]         \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 95.04it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0512, train_loss_epoch=0.0512]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383]\n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 96.31it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0369]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:24,627\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 88.62it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0279] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.92it/s]\u001b[A\n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 35.81it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=5.73e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.992, train_loss_epoch=0.992]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.804, train_loss_epoch=0.804]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:25,568\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.689, train_loss_epoch=0.689]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 119.04it/s, v_num=0, train_loss_step=0.689, train_loss_epoch=0.689]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.706, train_loss_epoch=0.706]         \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 109.95it/s, v_num=0, train_loss_step=0.618, train_loss_epoch=0.618]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.618, train_loss_epoch=0.618]         \n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 110.83it/s, v_num=0, train_loss_step=0.667, train_loss_epoch=0.667]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.667, train_loss_epoch=0.667]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.567, train_loss_epoch=0.567]         \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 115.03it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.567]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 181.67it/s]\u001b[A\n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 42.06it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.559, valid_loss=6.03e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.603, train_loss_epoch=0.603]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.603, train_loss_epoch=0.603]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.572, train_loss_epoch=0.572]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.570, train_loss_epoch=0.570]         \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 116.85it/s, v_num=0, train_loss_step=0.402, train_loss_epoch=0.402]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.402, train_loss_epoch=0.402]         \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 102.85it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309]         \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 104.11it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.292, train_loss_epoch=0.292]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.277, train_loss_epoch=0.277]         \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 103.31it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.354, train_loss_epoch=0.354]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.312, train_loss_epoch=0.312]         \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 115.87it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]         \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]         \n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00, 115.55it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 115.84it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]         \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 103.11it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]         \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 105.43it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.139]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]         \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 107.83it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.138]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 107.04it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.36it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 51.26it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=5.24e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=5.24e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=5.24e+3]         \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 107.73it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.107, valid_loss=5.24e+3]\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122, valid_loss=5.24e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:27,391\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=5.24e+3]         \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 93.92it/s, v_num=0, train_loss_step=0.0999, train_loss_epoch=0.0999, valid_loss=5.24e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 115.19it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.832, train_loss_epoch=0.832]         \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.626, train_loss_epoch=0.626]         \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.626, train_loss_epoch=0.626]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.636, train_loss_epoch=0.636]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.542, train_loss_epoch=0.542]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 111.24it/s, v_num=0, train_loss_step=0.586, train_loss_epoch=0.586]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.586, train_loss_epoch=0.586]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.559]         \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 118.77it/s, v_num=0, train_loss_step=0.559, train_loss_epoch=0.559]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.488, train_loss_epoch=0.488]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.532, train_loss_epoch=0.532]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.544, train_loss_epoch=0.544]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 117.83it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:28,123\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]         \n",
      "Epoch 38: 100%|██████████| 1/1 [00:00<00:00, 107.24it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.280]\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262]         \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.261, train_loss_epoch=0.261]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.287, train_loss_epoch=0.287]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 108.59it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.287]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 188.47it/s]\u001b[A\n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 55.77it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.287, valid_loss=4.41e+3]\n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 41.17it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232, valid_loss=4.41e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.643, train_loss_epoch=0.643]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.449, train_loss_epoch=0.449]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.346, train_loss_epoch=0.346]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 95.12it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.303]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247]        \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 91.30it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184]        \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 94.28it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.145]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0964, train_loss_epoch=0.0964]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0952, train_loss_epoch=0.0952]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]          \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0641, train_loss_epoch=0.0641]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0615, train_loss_epoch=0.0615]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0458, train_loss_epoch=0.0458]        \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 89.95it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0269]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024]          \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 90.98it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:30,098\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 93.92it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0189]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 155.18it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=6.52e+3]          \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=6.52e+3]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195, valid_loss=6.52e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=6.52e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.52e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.52e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=6.52e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 82.73it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=6.52e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.642, train_loss_epoch=0.642]         \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 101.87it/s, v_num=0, train_loss_step=0.673, train_loss_epoch=0.673]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.673, train_loss_epoch=0.673]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.749, train_loss_epoch=0.749]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:30,469\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 97.94it/s, v_num=0, train_loss_step=0.681, train_loss_epoch=0.700] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 181.20it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 52.42it/s, v_num=0, train_loss_step=0.681, train_loss_epoch=0.700, valid_loss=6.19e+3]\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 38.19it/s, v_num=0, train_loss_step=0.681, train_loss_epoch=0.681, valid_loss=6.19e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.611, train_loss_epoch=0.611]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.503, train_loss_epoch=0.503]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.538, train_loss_epoch=0.538]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.361, train_loss_epoch=0.361]         \n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00, 104.67it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284]\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 108.95it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.0982] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 176.58it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0895, train_loss_epoch=0.0895, valid_loss=5.78e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842, valid_loss=5.78e+3]         \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0835, train_loss_epoch=0.0835, valid_loss=5.78e+3]         \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 101.42it/s, v_num=0, train_loss_step=0.076, train_loss_epoch=0.076, valid_loss=5.78e+3]  \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0827, train_loss_epoch=0.0827, valid_loss=5.78e+3]        \n",
      "Epoch 115: 100%|██████████| 1/1 [00:00<00:00, 86.60it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=5.78e+3]  \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=5.78e+3]        \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 91.30it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805, valid_loss=5.78e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805, valid_loss=5.78e+3]        \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 99.16it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805, valid_loss=5.78e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787, valid_loss=5.78e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0713, train_loss_epoch=0.0713, valid_loss=5.78e+3]         \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 102.32it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=5.78e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0549, train_loss_epoch=0.0549, valid_loss=5.78e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0469, train_loss_epoch=0.0469, valid_loss=5.78e+3]         \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 111.59it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=5.78e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0434, valid_loss=5.78e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0632, train_loss_epoch=0.0632, valid_loss=5.78e+3]         \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 110.22it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0403, valid_loss=5.78e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0363, valid_loss=5.78e+3]         \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.034, train_loss_epoch=0.034, valid_loss=5.78e+3]           \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 103.82it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354, valid_loss=5.78e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354, valid_loss=5.78e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0363, valid_loss=5.78e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571, valid_loss=5.78e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0379, train_loss_epoch=0.0379, valid_loss=5.78e+3]         \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 113.37it/s, v_num=0, train_loss_step=0.0379, train_loss_epoch=0.0379, valid_loss=5.78e+3]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365, valid_loss=5.78e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=5.78e+3]           \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306, valid_loss=5.78e+3]         \n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00, 110.00it/s, v_num=0, train_loss_step=0.0448, train_loss_epoch=0.0306, valid_loss=5.78e+3]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0448, train_loss_epoch=0.0448, valid_loss=5.78e+3]         \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033, valid_loss=5.78e+3]           \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 104.32it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=5.78e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=5.78e+3]         \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=5.78e+3]           \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=5.78e+3]         \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=5.78e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 114.30it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=5.78e+3]\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0257, valid_loss=5.78e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 191.11it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0257, valid_loss=6.21e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0391, valid_loss=6.21e+3]        \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=6.21e+3]           \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=6.21e+3]\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231, valid_loss=6.21e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=6.21e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=6.21e+3]         \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=6.21e+3]         \n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00, 77.39it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=6.21e+3]\n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=6.21e+3]          \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=6.21e+3]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=6.21e+3]          \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.21e+3]         \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0138, train_loss_epoch=0.0138, valid_loss=6.21e+3]         \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=6.21e+3]         \n",
      "Epoch 237: 100%|██████████| 1/1 [00:00<00:00, 105.25it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.21e+3]  \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.21e+3]         \n",
      "Epoch 239: 100%|██████████| 1/1 [00:00<00:00, 111.94it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=6.21e+3]\n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.21e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=6.21e+3]         \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6.21e+3]         \n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00, 111.94it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.21e+3]\n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=6.21e+3]         \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=6.21e+3]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=6.21e+3]         \n",
      "Epoch 279: 100%|██████████| 1/1 [00:00<00:00, 110.55it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.0195, valid_loss=6.21e+3] \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=6.21e+3]          \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=6.21e+3]         \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.21e+3]           \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 108.92it/s, v_num=0, train_loss_step=0.00951, train_loss_epoch=0.0116, valid_loss=6.21e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s, v_num=0, train_loss_step=0.00951, train_loss_epoch=0.0116, valid_loss=6.18e+3] \n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00951, train_loss_epoch=0.00951, valid_loss=6.18e+3]        \n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=6.18e+3]           \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00861, train_loss_epoch=0.00861, valid_loss=6.18e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:38,062\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317: 100%|██████████| 1/1 [00:00<00:00, 113.05it/s, v_num=0, train_loss_step=0.00889, train_loss_epoch=0.00889, valid_loss=6.18e+3]\n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0078, train_loss_epoch=0.0078, valid_loss=6.18e+3]           \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.18e+3]         \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00723, train_loss_epoch=0.00723, valid_loss=6.18e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332: 100%|██████████| 1/1 [00:00<00:00, 96.49it/s, v_num=0, train_loss_step=0.00706, train_loss_epoch=0.00706, valid_loss=6.18e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 121.63it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 97.76it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194] \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 118.12it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.107]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]         \n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 113.17it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:40,016\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0853, train_loss_epoch=0.0853]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776]         \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 118.27it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0645]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 194.84it/s]\u001b[A\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 43.39it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638, valid_loss=6.64e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.609, train_loss_epoch=0.609]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.491, train_loss_epoch=0.491]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 100.72it/s, v_num=0, train_loss_step=0.522, train_loss_epoch=0.491]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.522, train_loss_epoch=0.522]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.482, train_loss_epoch=0.482]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.369, train_loss_epoch=0.369]         \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 97.00it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286] \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283]         \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 101.21it/s, v_num=0, train_loss_step=0.354, train_loss_epoch=0.283]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.354, train_loss_epoch=0.354]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.227, train_loss_epoch=0.227]         \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 95.27it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213] \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240]         \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]         \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 96.72it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180] \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 103.32it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]         \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 97.49it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113] \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00, 101.16it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.136]\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:41,329\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 98.44it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.123] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 159.00it/s]\u001b[A\n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 37.91it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=5.54e+3]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 75.81it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.473, train_loss_epoch=0.473]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.532, train_loss_epoch=0.532]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300]         \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 106.09it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 109.08it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]         \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 106.31it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 106.63it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]           \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0884, train_loss_epoch=0.0884]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0633, train_loss_epoch=0.0633]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0656, train_loss_epoch=0.0656]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 112.18it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0414]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 202.06it/s]\u001b[A\n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 84.11it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.0373, valid_loss=6.14e+3]  \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=6.14e+3]         \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=6.14e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=6.14e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=6.14e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=6.14e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0463, train_loss_epoch=0.0463, valid_loss=6.14e+3]         \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=6.14e+3]         \n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00, 106.16it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=6.14e+3]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=6.14e+3]         \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=6.14e+3]           \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=6.14e+3]         \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=6.14e+3]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.14e+3]         \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=6.14e+3]         \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=6.14e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=6.14e+3]\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=6.14e+3]         \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=6.14e+3]         \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=6.14e+3]         \n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00, 104.97it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=6.14e+3]\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=6.14e+3]         \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=6.14e+3]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=6.14e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 108.23it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0161, valid_loss=6.14e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 200.81it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 55.85it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0161, valid_loss=6.01e+3] \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=6.01e+3]        \n",
      "Epoch 200: 100%|██████████| 1/1 [00:00<00:00, 101.84it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=6.01e+3]\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=6.01e+3]         \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=6.01e+3]         \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.01e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.01e+3]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.01e+3]         \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.01e+3]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00838, train_loss_epoch=0.00838, valid_loss=6.01e+3]         \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.01e+3]           \n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 102.07it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.01e+3]\n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=6.01e+3]         \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=6.01e+3]         \n",
      "Epoch 229: 100%|██████████| 1/1 [00:00<00:00, 103.18it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.01e+3]\n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.01e+3]         \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.01e+3]         \n",
      "Epoch 231: 100%|██████████| 1/1 [00:00<00:00, 102.30it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=6.01e+3]\n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=6.01e+3]         \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=6.01e+3]         \n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00, 98.69it/s, v_num=0, train_loss_step=0.00924, train_loss_epoch=0.00924, valid_loss=6.01e+3]\n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00924, train_loss_epoch=0.00924, valid_loss=6.01e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00943, train_loss_epoch=0.00943, valid_loss=6.01e+3]         \n",
      "Epoch 235: 100%|██████████| 1/1 [00:00<00:00, 105.20it/s, v_num=0, train_loss_step=0.00932, train_loss_epoch=0.00932, valid_loss=6.01e+3]\n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00932, train_loss_epoch=0.00932, valid_loss=6.01e+3]         \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00896, train_loss_epoch=0.00896, valid_loss=6.01e+3]         \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00871, train_loss_epoch=0.00871, valid_loss=6.01e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00992, train_loss_epoch=0.00992, valid_loss=6.01e+3]         \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00889, train_loss_epoch=0.00889, valid_loss=6.01e+3]         \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=6.01e+3]           \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00694, train_loss_epoch=0.00694, valid_loss=6.01e+3]        \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 78.63it/s, v_num=0, train_loss_step=0.00706, train_loss_epoch=0.00694, valid_loss=6.01e+3]\n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00706, train_loss_epoch=0.00706, valid_loss=6.01e+3]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00711, train_loss_epoch=0.00711, valid_loss=6.01e+3]        \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 78.74it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.00711, valid_loss=6.01e+3]  \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=6.01e+3]          \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00618, train_loss_epoch=0.00618, valid_loss=6.01e+3]        \n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 77.18it/s, v_num=0, train_loss_step=0.00602, train_loss_epoch=0.00618, valid_loss=6.01e+3]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00602, train_loss_epoch=0.00602, valid_loss=6.01e+3]        \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00907, train_loss_epoch=0.00907, valid_loss=6.01e+3]        \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 104.87it/s, v_num=0, train_loss_step=0.00896, train_loss_epoch=0.00896, valid_loss=6.01e+3]\n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 112.39it/s, v_num=0, train_loss_step=0.00896, train_loss_epoch=0.00896, valid_loss=6.01e+3]\n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00786, train_loss_epoch=0.00786, valid_loss=6.01e+3]         \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.01e+3]           \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.01e+3]         \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00706, train_loss_epoch=0.00706, valid_loss=6.01e+3]         \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0066, train_loss_epoch=0.0066, valid_loss=6.01e+3]           \n",
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00, 109.79it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0066, valid_loss=6.01e+3]\n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=6.01e+3]         \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00738, train_loss_epoch=0.00738, valid_loss=6.01e+3]         \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0065, train_loss_epoch=0.0065, valid_loss=6.01e+3]           \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00549, train_loss_epoch=0.00549, valid_loss=6.01e+3]         \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00692, train_loss_epoch=0.00692, valid_loss=6.01e+3]         \n",
      "Epoch 287: 100%|██████████| 1/1 [00:00<00:00, 106.01it/s, v_num=0, train_loss_step=0.00838, train_loss_epoch=0.00838, valid_loss=6.01e+3]\n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00838, train_loss_epoch=0.00838, valid_loss=6.01e+3]         \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00882, train_loss_epoch=0.00882, valid_loss=6.01e+3]         \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00801, train_loss_epoch=0.00801, valid_loss=6.01e+3]         \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00626, train_loss_epoch=0.00626, valid_loss=6.01e+3]         \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.01e+3]           \n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00, 106.06it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.01e+3]\n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=6.01e+3]         \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=6.01e+3]           \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00837, train_loss_epoch=0.00837, valid_loss=6.01e+3]         \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0159, train_loss_epoch=0.0159, valid_loss=6.01e+3]           \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0083, train_loss_epoch=0.0083, valid_loss=6.01e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 110.35it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0119, valid_loss=6.01e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 200.82it/s]\u001b[A\n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=6.08e+3]         \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=6.08e+3]         \n",
      "Epoch 307: 100%|██████████| 1/1 [00:00<00:00, 107.76it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.0123, valid_loss=6.08e+3] \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=6.08e+3]          \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=6.08e+3]         \n",
      "Epoch 309: 100%|██████████| 1/1 [00:00<00:00, 103.69it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=6.08e+3]\n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=6.08e+3]         \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.08e+3]         \n",
      "Epoch 311: 100%|██████████| 1/1 [00:00<00:00, 101.90it/s, v_num=0, train_loss_step=0.00968, train_loss_epoch=0.00968, valid_loss=6.08e+3]\n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00968, train_loss_epoch=0.00968, valid_loss=6.08e+3]         \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=6.08e+3]           \n",
      "Epoch 313: 100%|██████████| 1/1 [00:00<00:00, 104.63it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=6.08e+3]  \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=6.08e+3]         \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=6.08e+3]         \n",
      "Epoch 315: 100%|██████████| 1/1 [00:00<00:00, 104.06it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.08e+3]\n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.08e+3]         \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00912, train_loss_epoch=0.00912, valid_loss=6.08e+3]         \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00959, train_loss_epoch=0.00959, valid_loss=6.08e+3]         \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00853, train_loss_epoch=0.00853, valid_loss=6.08e+3]         \n",
      "Epoch 329: 100%|██████████| 1/1 [00:00<00:00, 100.21it/s, v_num=0, train_loss_step=0.00789, train_loss_epoch=0.00789, valid_loss=6.08e+3]\n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00789, train_loss_epoch=0.00789, valid_loss=6.08e+3]         \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00736, train_loss_epoch=0.00736, valid_loss=6.08e+3]         \n",
      "Epoch 331: 100%|██████████| 1/1 [00:00<00:00, 103.88it/s, v_num=0, train_loss_step=0.00743, train_loss_epoch=0.00743, valid_loss=6.08e+3]\n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00743, train_loss_epoch=0.00743, valid_loss=6.08e+3]         \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00705, train_loss_epoch=0.00705, valid_loss=6.08e+3]         \n",
      "Epoch 333: 100%|██████████| 1/1 [00:00<00:00, 104.43it/s, v_num=0, train_loss_step=0.00616, train_loss_epoch=0.00616, valid_loss=6.08e+3]\n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00616, train_loss_epoch=0.00616, valid_loss=6.08e+3]         \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=6.08e+3]           \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00612, train_loss_epoch=0.00612, valid_loss=6.08e+3]         \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00996, train_loss_epoch=0.00996, valid_loss=6.08e+3]         \n",
      "Epoch 347: 100%|██████████| 1/1 [00:00<00:00, 105.70it/s, v_num=0, train_loss_step=0.00937, train_loss_epoch=0.00937, valid_loss=6.08e+3]\n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00937, train_loss_epoch=0.00937, valid_loss=6.08e+3]         \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00803, train_loss_epoch=0.00803, valid_loss=6.08e+3]         \n",
      "Epoch 349: 100%|██████████| 1/1 [00:00<00:00, 109.06it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.00803, valid_loss=6.08e+3] \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.08e+3]          \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00769, train_loss_epoch=0.00769, valid_loss=6.08e+3]         \n",
      "Epoch 351: 100%|██████████| 1/1 [00:00<00:00, 104.24it/s, v_num=0, train_loss_step=0.00859, train_loss_epoch=0.00859, valid_loss=6.08e+3]\n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00859, train_loss_epoch=0.00859, valid_loss=6.08e+3]         \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00895, train_loss_epoch=0.00895, valid_loss=6.08e+3]         \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00776, train_loss_epoch=0.00776, valid_loss=6.08e+3]         \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00781, train_loss_epoch=0.00781, valid_loss=6.08e+3]         \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00646, train_loss_epoch=0.00646, valid_loss=6.08e+3]         \n",
      "Epoch 376: 100%|██████████| 1/1 [00:00<00:00, 106.32it/s, v_num=0, train_loss_step=0.00605, train_loss_epoch=0.00646, valid_loss=6.08e+3]\n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00605, train_loss_epoch=0.00605, valid_loss=6.08e+3]         \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00575, train_loss_epoch=0.00575, valid_loss=6.08e+3]         \n",
      "Epoch 378: 100%|██████████| 1/1 [00:00<00:00, 105.00it/s, v_num=0, train_loss_step=0.00594, train_loss_epoch=0.00594, valid_loss=6.08e+3]\n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00594, train_loss_epoch=0.00594, valid_loss=6.08e+3]         \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00646, train_loss_epoch=0.00646, valid_loss=6.08e+3]         \n",
      "Epoch 380: 100%|██████████| 1/1 [00:00<00:00, 104.71it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.08e+3]  \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.08e+3]         \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00981, train_loss_epoch=0.00981, valid_loss=6.08e+3]         \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.08e+3]           \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00586, train_loss_epoch=0.00586, valid_loss=6.08e+3]         \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 74.05it/s, v_num=0, train_loss_step=0.00578, train_loss_epoch=0.00692, valid_loss=6.08e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 134.74it/s]\u001b[A\n",
      "Epoch 401: 100%|██████████| 1/1 [00:00<00:00, 75.66it/s, v_num=0, train_loss_step=0.00803, train_loss_epoch=0.00566, valid_loss=6.3e+3] \n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00803, train_loss_epoch=0.00803, valid_loss=6.3e+3]        \n",
      "Epoch 402: 100%|██████████| 1/1 [00:00<00:00, 76.65it/s, v_num=0, train_loss_step=0.0061, train_loss_epoch=0.00803, valid_loss=6.3e+3] \n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0061, train_loss_epoch=0.0061, valid_loss=6.3e+3]         \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=6.3e+3]           \n",
      "Epoch 425:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00852, train_loss_epoch=0.00852, valid_loss=6.3e+3]         \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=6.3e+3]         \n",
      "Epoch 436: 100%|██████████| 1/1 [00:00<00:00, 104.86it/s, v_num=0, train_loss_step=0.00715, train_loss_epoch=0.00715, valid_loss=6.3e+3]\n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00715, train_loss_epoch=0.00715, valid_loss=6.3e+3]         \n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00933, train_loss_epoch=0.00933, valid_loss=6.3e+3]         \n",
      "Epoch 438: 100%|██████████| 1/1 [00:00<00:00, 103.34it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.3e+3]  \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.3e+3]         \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00983, train_loss_epoch=0.00983, valid_loss=6.3e+3]         \n",
      "Epoch 440: 100%|██████████| 1/1 [00:00<00:00, 95.44it/s, v_num=0, train_loss_step=0.00822, train_loss_epoch=0.00822, valid_loss=6.3e+3] \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00822, train_loss_epoch=0.00822, valid_loss=6.3e+3]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=6.3e+3]           \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00884, train_loss_epoch=0.00884, valid_loss=6.3e+3]         \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00884, train_loss_epoch=0.00884, valid_loss=6.3e+3]\n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00831, train_loss_epoch=0.00831, valid_loss=6.3e+3]         \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0114, train_loss_epoch=0.0114, valid_loss=6.3e+3]           \n",
      "Epoch 455: 100%|██████████| 1/1 [00:00<00:00, 109.20it/s, v_num=0, train_loss_step=0.0071, train_loss_epoch=0.00639, valid_loss=6.3e+3] \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0071, train_loss_epoch=0.0071, valid_loss=6.3e+3]          \n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00746, train_loss_epoch=0.00746, valid_loss=6.3e+3]         \n",
      "Epoch 457: 100%|██████████| 1/1 [00:00<00:00, 103.63it/s, v_num=0, train_loss_step=0.00556, train_loss_epoch=0.00556, valid_loss=6.3e+3]\n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00556, train_loss_epoch=0.00556, valid_loss=6.3e+3]         \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0069, train_loss_epoch=0.0069, valid_loss=6.3e+3]           \n",
      "Epoch 459: 100%|██████████| 1/1 [00:00<00:00, 99.41it/s, v_num=0, train_loss_step=0.00652, train_loss_epoch=0.00652, valid_loss=6.3e+3]\n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00652, train_loss_epoch=0.00652, valid_loss=6.3e+3]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00745, train_loss_epoch=0.00745, valid_loss=6.3e+3]         \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=6.3e+3]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:46,984\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=6.3e+3]           \n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.3e+3]         \n",
      "Epoch 473: 100%|██████████| 1/1 [00:00<00:00, 103.22it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.3e+3]\n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=6.3e+3]         \n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00903, train_loss_epoch=0.00903, valid_loss=6.3e+3]         \n",
      "Epoch 475: 100%|██████████| 1/1 [00:00<00:00, 104.58it/s, v_num=0, train_loss_step=0.0095, train_loss_epoch=0.0095, valid_loss=6.3e+3]  \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0095, train_loss_epoch=0.0095, valid_loss=6.3e+3]         \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00707, train_loss_epoch=0.00707, valid_loss=6.3e+3]         \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00822, train_loss_epoch=0.00822, valid_loss=6.3e+3]         \n",
      "Epoch 487: 100%|██████████| 1/1 [00:00<00:00, 98.36it/s, v_num=0, train_loss_step=0.0087, train_loss_epoch=0.0087, valid_loss=6.3e+3]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246]         \n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 101.91it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.231, train_loss_epoch=0.231]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 104.33it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]         \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 99.90it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198] \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]        \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 99.16it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209] \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 75.10it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]         \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 91.67it/s, v_num=0, train_loss_step=0.0809, train_loss_epoch=0.0809] \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0845, train_loss_epoch=0.0845]        \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 93.78it/s, v_num=0, train_loss_step=0.0791, train_loss_epoch=0.0791] \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0791, train_loss_epoch=0.0791]        \n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00, 102.18it/s, v_num=0, train_loss_step=0.0791, train_loss_epoch=0.0791]\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0457, train_loss_epoch=0.0457]         \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 104.82it/s, v_num=0, train_loss_step=0.0543, train_loss_epoch=0.0543]\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0543, train_loss_epoch=0.0543]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 104.67it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 103.22it/s, v_num=0, train_loss_step=0.0349, train_loss_epoch=0.0349]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0349, train_loss_epoch=0.0349]         \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 103.84it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]         \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 104.86it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0499, train_loss_epoch=0.0499]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214]         \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 99.69it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.0214]  \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0159, train_loss_epoch=0.0159]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 105.02it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.019] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.59it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245, valid_loss=6.51e+3]         \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 102.18it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0245, valid_loss=6.51e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0174, valid_loss=6.51e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=6.51e+3]           \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=6.51e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0254, train_loss_epoch=0.0254, valid_loss=6.51e+3]         \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.51e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=6.51e+3]         \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=6.51e+3]           \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 108.32it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=6.51e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=6.51e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:49,313\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 94.55it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=6.51e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.646, train_loss_epoch=0.646]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.639, train_loss_epoch=0.639]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.661, train_loss_epoch=0.661]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.632, train_loss_epoch=0.632]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 106.44it/s, v_num=0, train_loss_step=0.633, train_loss_epoch=0.633]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.633, train_loss_epoch=0.633]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.566, train_loss_epoch=0.566]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 116.79it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.465, train_loss_epoch=0.465]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.449, train_loss_epoch=0.449]         \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 112.27it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.449]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.409, train_loss_epoch=0.409]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.510, train_loss_epoch=0.510]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]\n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.384, train_loss_epoch=0.384]         \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.460, train_loss_epoch=0.460]         \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 114.66it/s, v_num=0, train_loss_step=0.460, train_loss_epoch=0.460]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.338, train_loss_epoch=0.338]         \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 107.94it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.374, train_loss_epoch=0.374]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297]         \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 113.65it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263]\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.287, train_loss_epoch=0.287]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.234, train_loss_epoch=0.234]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176]         \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 100.29it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.176]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]         \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 96.15it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124] \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]        \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 95.95it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113] \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 99.82it/s, v_num=0, train_loss_step=0.0982, train_loss_epoch=0.101]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 157.47it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916, valid_loss=5.15e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=5.15e+3]           \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 99.76it/s, v_num=0, train_loss_step=0.0951, train_loss_epoch=0.113, valid_loss=5.15e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0951, train_loss_epoch=0.0951, valid_loss=5.15e+3]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0908, train_loss_epoch=0.0908, valid_loss=5.15e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0931, train_loss_epoch=0.0931, valid_loss=5.15e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0875, train_loss_epoch=0.0875, valid_loss=5.15e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0712, train_loss_epoch=0.0712, valid_loss=5.15e+3]         \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=5.15e+3]         \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638, valid_loss=5.15e+3]         \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0516, train_loss_epoch=0.0516, valid_loss=5.15e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.049, train_loss_epoch=0.049, valid_loss=5.15e+3]           \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383, valid_loss=5.15e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=5.15e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0334, valid_loss=5.15e+3]         \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354, valid_loss=5.15e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=5.15e+3]         \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=5.15e+3]         \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=5.15e+3]         \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=5.15e+3]         \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=5.15e+3]         \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=5.15e+3]\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 76.60it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0182, valid_loss=5.15e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.17it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 39.20it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0182, valid_loss=5.83e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=5.83e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=5.83e+3]         \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=5.83e+3]           \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 114.98it/s, v_num=0, train_loss_step=0.013, train_loss_epoch=0.013, valid_loss=5.83e+3]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352, valid_loss=5.83e+3]         \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=5.83e+3]         \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 107.21it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=5.83e+3]\n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=5.83e+3]         \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=5.83e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=5.83e+3]         \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=5.83e+3]         \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=5.83e+3]         \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 115.78it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=5.83e+3]\n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=5.83e+3]         \n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00, 96.45it/s, v_num=0, train_loss_step=0.00987, train_loss_epoch=0.00987, valid_loss=5.83e+3] \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00987, train_loss_epoch=0.00987, valid_loss=5.83e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00934, train_loss_epoch=0.00934, valid_loss=5.83e+3]         \n",
      "Epoch 238: 100%|██████████| 1/1 [00:00<00:00, 101.94it/s, v_num=0, train_loss_step=0.00921, train_loss_epoch=0.00934, valid_loss=5.83e+3]\n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00921, train_loss_epoch=0.00921, valid_loss=5.83e+3]         \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00856, train_loss_epoch=0.00856, valid_loss=5.83e+3]         \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245, valid_loss=5.83e+3]           \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 97.62it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0103, valid_loss=5.83e+3]   \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=5.83e+3]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00965, train_loss_epoch=0.00965, valid_loss=5.83e+3]         \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0132, train_loss_epoch=0.0132, valid_loss=5.83e+3]           \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00947, train_loss_epoch=0.00947, valid_loss=5.83e+3]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=5.83e+3]           \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=5.83e+3]         \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=5.83e+3]\n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00842, train_loss_epoch=0.00842, valid_loss=5.83e+3]         \n",
      "Epoch 285: 100%|██████████| 1/1 [00:00<00:00, 97.23it/s, v_num=0, train_loss_step=0.00751, train_loss_epoch=0.00751, valid_loss=5.83e+3] \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00751, train_loss_epoch=0.00751, valid_loss=5.83e+3]        \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 101.46it/s, v_num=0, train_loss_step=0.00682, train_loss_epoch=0.00751, valid_loss=5.83e+3]\n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00682, train_loss_epoch=0.00682, valid_loss=5.83e+3]         \n",
      "Epoch 287: 100%|██████████| 1/1 [00:00<00:00, 101.20it/s, v_num=0, train_loss_step=0.00818, train_loss_epoch=0.00682, valid_loss=5.83e+3]\n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00818, train_loss_epoch=0.00818, valid_loss=5.83e+3]         \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=5.83e+3]           \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 102.50it/s, v_num=0, train_loss_step=0.00952, train_loss_epoch=0.0125, valid_loss=5.83e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 183.67it/s]\u001b[A\n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.007, train_loss_epoch=0.007, valid_loss=6.02e+3]             \n",
      "Epoch 317: 100%|██████████| 1/1 [00:00<00:00, 85.82it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.0056, valid_loss=6.02e+3]  \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00645, train_loss_epoch=0.00645, valid_loss=6.02e+3]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=6.02e+3]          \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00, 80.10it/s, v_num=0, train_loss_step=0.00724, train_loss_epoch=0.0149, valid_loss=6.02e+3]\n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00724, train_loss_epoch=0.00724, valid_loss=6.02e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00795, train_loss_epoch=0.00795, valid_loss=6.02e+3]        \n",
      "Epoch 321: 100%|██████████| 1/1 [00:00<00:00, 76.71it/s, v_num=0, train_loss_step=0.00721, train_loss_epoch=0.00721, valid_loss=6.02e+3]\n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00721, train_loss_epoch=0.00721, valid_loss=6.02e+3]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00742, train_loss_epoch=0.00742, valid_loss=6.02e+3]        \n",
      "Epoch 323: 100%|██████████| 1/1 [00:00<00:00, 76.27it/s, v_num=0, train_loss_step=0.0084, train_loss_epoch=0.0084, valid_loss=6.02e+3]  \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0084, train_loss_epoch=0.0084, valid_loss=6.02e+3]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00689, train_loss_epoch=0.00689, valid_loss=6.02e+3]        \n",
      "Epoch 325: 100%|██████████| 1/1 [00:00<00:00, 76.24it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.02e+3]  \n",
      "Epoch 326:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.02e+3]        \n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0083, train_loss_epoch=0.0083, valid_loss=6.02e+3]        \n",
      "Epoch 327: 100%|██████████| 1/1 [00:00<00:00, 77.17it/s, v_num=0, train_loss_step=0.00668, train_loss_epoch=0.00668, valid_loss=6.02e+3]\n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00668, train_loss_epoch=0.00668, valid_loss=6.02e+3]        \n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00673, train_loss_epoch=0.00673, valid_loss=6.02e+3]        \n",
      "Epoch 329: 100%|██████████| 1/1 [00:00<00:00, 76.44it/s, v_num=0, train_loss_step=0.00737, train_loss_epoch=0.00737, valid_loss=6.02e+3]\n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00737, train_loss_epoch=0.00737, valid_loss=6.02e+3]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0069, train_loss_epoch=0.0069, valid_loss=6.02e+3]          \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00906, train_loss_epoch=0.00906, valid_loss=6.02e+3]        \n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 99.49it/s, v_num=0, train_loss_step=0.00603, train_loss_epoch=0.00603, valid_loss=6.02e+3] \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00603, train_loss_epoch=0.00603, valid_loss=6.02e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0063, train_loss_epoch=0.0063, valid_loss=6.02e+3]           \n",
      "Epoch 343: 100%|██████████| 1/1 [00:00<00:00, 98.15it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.02e+3] \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=6.02e+3]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00817, train_loss_epoch=0.00817, valid_loss=6.02e+3]        \n",
      "Epoch 345: 100%|██████████| 1/1 [00:00<00:00, 105.44it/s, v_num=0, train_loss_step=0.00817, train_loss_epoch=0.00817, valid_loss=6.02e+3]\n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0088, train_loss_epoch=0.0088, valid_loss=6.02e+3]           \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00782, train_loss_epoch=0.00782, valid_loss=6.02e+3]        \n",
      "Epoch 357: 100%|██████████| 1/1 [00:00<00:00, 117.22it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=6.02e+3]  \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=6.02e+3]         \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=6.02e+3]         \n",
      "Epoch 359: 100%|██████████| 1/1 [00:00<00:00, 107.73it/s, v_num=0, train_loss_step=0.00982, train_loss_epoch=0.00982, valid_loss=6.02e+3]\n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00982, train_loss_epoch=0.00982, valid_loss=6.02e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:54,179\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00795, train_loss_epoch=0.00795, valid_loss=6.02e+3]         \n",
      "Epoch 362: 100%|██████████| 1/1 [00:00<00:00, 88.12it/s, v_num=0, train_loss_step=0.00796, train_loss_epoch=0.00796, valid_loss=6.02e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.799, train_loss_epoch=0.799]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.701, train_loss_epoch=0.701]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.554, train_loss_epoch=0.554]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.597, train_loss_epoch=0.597]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.419, train_loss_epoch=0.419]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]        \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 92.79it/s, v_num=0, train_loss_step=0.340, train_loss_epoch=0.325]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.340, train_loss_epoch=0.340]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.248, train_loss_epoch=0.248]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.245, train_loss_epoch=0.245]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]        \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 89.66it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:55,574\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 94.32it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.187]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 165.19it/s]\u001b[A\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176, valid_loss=5.27e+3]        \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 79.90it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175, valid_loss=5.27e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.494, train_loss_epoch=0.494]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.549, train_loss_epoch=0.549]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.545, train_loss_epoch=0.545]         \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.489, train_loss_epoch=0.489]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288]         \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:56,035\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 131.65it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.258]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 191.01it/s]\u001b[A\n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 45.94it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228, valid_loss=5.59e+3]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 86.15it/s, v_num=0, train_loss_step=0.391, train_loss_epoch=0.427]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.391, train_loss_epoch=0.391]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263]        \n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 87.61it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.173]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0985, train_loss_epoch=0.0985]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]          \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0778, train_loss_epoch=0.0778]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 90.97it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0616, train_loss_epoch=0.0616]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0537, train_loss_epoch=0.0537]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 84.71it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0433]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 172.43it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 46.29it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0433, valid_loss=5.41e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=5.41e+3]       \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=5.41e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=5.41e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332, valid_loss=5.41e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279, valid_loss=5.41e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=5.41e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=5.41e+3]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=5.41e+3]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=5.41e+3]        \n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00, 86.48it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=5.41e+3]\n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=5.41e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=5.41e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=5.41e+3]        \n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00, 83.41it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=5.41e+3]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=5.41e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 87.14it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0103, valid_loss=5.41e+3]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 185.49it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 48.35it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0103, valid_loss=5.87e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=5.87e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00937, train_loss_epoch=0.00937, valid_loss=5.87e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0091, train_loss_epoch=0.0091, valid_loss=5.87e+3]          \n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00, 84.55it/s, v_num=0, train_loss_step=0.00802, train_loss_epoch=0.00802, valid_loss=5.87e+3]\n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00802, train_loss_epoch=0.00802, valid_loss=5.87e+3]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00795, train_loss_epoch=0.00795, valid_loss=5.87e+3]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0071, train_loss_epoch=0.0071, valid_loss=5.87e+3]          \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00597, train_loss_epoch=0.00597, valid_loss=5.87e+3]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00554, train_loss_epoch=0.00554, valid_loss=5.87e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00644, train_loss_epoch=0.00644, valid_loss=5.87e+3]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=5.87e+3]          \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00528, train_loss_epoch=0.00528, valid_loss=5.87e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:46:59,726\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280: 100%|██████████| 1/1 [00:00<00:00, 87.10it/s, v_num=0, train_loss_step=0.00532, train_loss_epoch=0.00921, valid_loss=5.87e+3]\n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00532, train_loss_epoch=0.00532, valid_loss=5.87e+3]        \n",
      "Epoch 281: 100%|██████████| 1/1 [00:00<00:00, 87.90it/s, v_num=0, train_loss_step=0.0048, train_loss_epoch=0.00532, valid_loss=5.87e+3] \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0048, train_loss_epoch=0.0048, valid_loss=5.87e+3]         \n",
      "Epoch 283: 100%|██████████| 1/1 [00:00<00:00, 79.02it/s, v_num=0, train_loss_step=0.00443, train_loss_epoch=0.00443, valid_loss=5.87e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.678, train_loss_epoch=0.678]         \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 82.09it/s, v_num=0, train_loss_step=0.670, train_loss_epoch=0.670]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.670, train_loss_epoch=0.670]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.721, train_loss_epoch=0.721]       \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 72.73it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.875, train_loss_epoch=0.875]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.900, train_loss_epoch=0.900]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.658, train_loss_epoch=0.658]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.725, train_loss_epoch=0.725]         \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.616, train_loss_epoch=0.616]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.461, train_loss_epoch=0.461]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.424, train_loss_epoch=0.424]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413]        \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 82.68it/s, v_num=0, train_loss_step=0.320, train_loss_epoch=0.320] \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.320, train_loss_epoch=0.320]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316]        \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 73.77it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.278, train_loss_epoch=0.278]        \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 73.96it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]        \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 75.23it/s, v_num=0, train_loss_step=0.351, train_loss_epoch=0.326]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.351, train_loss_epoch=0.351]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 77.97it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.315, train_loss_epoch=0.315]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.340, train_loss_epoch=0.340]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.302, train_loss_epoch=0.302]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 105.09it/s, v_num=0, train_loss_step=0.266, train_loss_epoch=0.276]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.86it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233, valid_loss=5.09e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250, valid_loss=5.09e+3]         \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.236, train_loss_epoch=0.236, valid_loss=5.09e+3]         \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 91.79it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240, valid_loss=5.09e+3] \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239, valid_loss=5.09e+3]        \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 83.85it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=5.09e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=5.09e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:01,394\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 75.11it/s, v_num=0, train_loss_step=0.217, train_loss_epoch=0.217, valid_loss=5.09e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 67.89it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.872, train_loss_epoch=0.872]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.817, train_loss_epoch=0.817]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.778, train_loss_epoch=0.778]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.797, train_loss_epoch=0.797]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.747, train_loss_epoch=0.747]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.697, train_loss_epoch=0.697]        \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 93.16it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.697]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.740, train_loss_epoch=0.740]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.719, train_loss_epoch=0.719]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.594, train_loss_epoch=0.594]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.560, train_loss_epoch=0.560]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.542, train_loss_epoch=0.542]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:02,399\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 110.48it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.460]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 176.96it/s]\u001b[A\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 56.26it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.460, valid_loss=4.96e+3]\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 40.39it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.379, valid_loss=4.96e+3]\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 70.85it/s]                     \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.388, train_loss_epoch=0.388]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 106.16it/s, v_num=0, train_loss_step=0.0948, train_loss_epoch=0.173]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 101.90it/s, v_num=0, train_loss_step=0.0948, train_loss_epoch=0.0948]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0948, train_loss_epoch=0.0948]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0977, train_loss_epoch=0.0977]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0777, train_loss_epoch=0.0777]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 87.66it/s, v_num=0, train_loss_step=0.0734, train_loss_epoch=0.0734]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0734, train_loss_epoch=0.0734]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0407, train_loss_epoch=0.0407]         \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 100.74it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0353, train_loss_epoch=0.0353]         \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 101.04it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342]\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033]           \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 100.54it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033]           \n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 100.28it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0312]\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0312]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283]         \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 101.15it/s, v_num=0, train_loss_step=0.0277, train_loss_epoch=0.0277]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0277, train_loss_epoch=0.0277]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243]         \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 102.04it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318]         \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 101.32it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306]\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221]         \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 105.59it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0221]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0293, train_loss_epoch=0.0293]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 100.80it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271]         \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395]         \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00, 101.57it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191]\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241]         \n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00, 104.33it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0241]\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0334]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.021, train_loss_epoch=0.021]           \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 101.85it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305]\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305]         \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 100.89it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198]         \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 98.51it/s, v_num=0, train_loss_step=0.0392, train_loss_epoch=0.0392] \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0392, train_loss_epoch=0.0392]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239]         \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00, 103.39it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0239]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0269]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281]         \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 103.51it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0281]\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0291]         \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 107.12it/s, v_num=0, train_loss_step=0.0291, train_loss_epoch=0.0291]\n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 104.55it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0291]\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309]         \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 104.18it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0309]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285]         \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 104.29it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0255, train_loss_epoch=0.0255]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0232]         \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 101.60it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017]  \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136]         \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 108.18it/s, v_num=0, train_loss_step=0.0203, train_loss_epoch=0.0203]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231]         \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 100.72it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 105.54it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.0223] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 170.30it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 52.78it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.0223, valid_loss=5.77e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.025, valid_loss=5.77e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=5.77e+3]         \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=5.77e+3]         \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=5.77e+3]         \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 105.02it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0198, valid_loss=5.77e+3]\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=5.77e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=5.77e+3]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=5.77e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=5.77e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=5.77e+3]           \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.0112, valid_loss=5.77e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=5.77e+3]           \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=5.77e+3]           \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00658, train_loss_epoch=0.00658, valid_loss=5.77e+3]         \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=5.77e+3]           \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=5.77e+3]           \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=5.77e+3]         \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=5.77e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.0112, valid_loss=5.77e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=5.77e+3]           \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0185, train_loss_epoch=0.0185, valid_loss=5.77e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:04,983\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 102.22it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.0185, valid_loss=5.77e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 194.99it/s]\u001b[A\n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00933, train_loss_epoch=0.00933, valid_loss=5.68e+3]         \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00894, train_loss_epoch=0.00894, valid_loss=5.68e+3]         \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=5.68e+3]             \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=5.68e+3]         \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=5.68e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 91.96it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=5.68e+3] \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 92.33it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.030, train_loss_epoch=1.030]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.912, train_loss_epoch=0.912]         \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 105.83it/s, v_num=0, train_loss_step=0.627, train_loss_epoch=0.627]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.627, train_loss_epoch=0.627]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.533, train_loss_epoch=0.533]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.365, train_loss_epoch=0.365]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.300, train_loss_epoch=0.300]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 107.32it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 92.09it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146] \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 111.64it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.124]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 210.60it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=5.48e+3]         \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0995, train_loss_epoch=0.0995, valid_loss=5.48e+3]         \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 105.35it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=5.48e+3]  \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=5.48e+3]         \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 101.10it/s, v_num=0, train_loss_step=0.0807, train_loss_epoch=0.0807, valid_loss=5.48e+3]\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0807, train_loss_epoch=0.0807, valid_loss=5.48e+3]         \n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00, 106.58it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871, valid_loss=5.48e+3]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871, valid_loss=5.48e+3]         \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0784, valid_loss=5.48e+3]         \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0784, valid_loss=5.48e+3]\n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0863, train_loss_epoch=0.0863, valid_loss=5.48e+3]         \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0745, train_loss_epoch=0.0745, valid_loss=5.48e+3]         \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 112.92it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0676, valid_loss=5.48e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681, valid_loss=5.48e+3]         \n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 107.41it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.48e+3]\n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.48e+3]         \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 105.24it/s, v_num=0, train_loss_step=0.0676, train_loss_epoch=0.0676, valid_loss=5.48e+3]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0676, train_loss_epoch=0.0676, valid_loss=5.48e+3]         \n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 105.67it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=5.48e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=5.48e+3]         \n",
      "Epoch 155: 100%|██████████| 1/1 [00:00<00:00, 105.09it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=5.48e+3]\n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=5.48e+3]         \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0557, valid_loss=5.48e+3]         \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0549, train_loss_epoch=0.0549, valid_loss=5.48e+3]         \n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00, 110.36it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=5.48e+3]\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=5.48e+3]         \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 106.01it/s, v_num=0, train_loss_step=0.0469, train_loss_epoch=0.0469, valid_loss=5.48e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0469, train_loss_epoch=0.0469, valid_loss=5.48e+3]         \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0458, train_loss_epoch=0.0458, valid_loss=5.48e+3]         \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.48e+3]         \n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 112.57it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0401, valid_loss=5.48e+3]\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=5.48e+3]         \n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00, 107.26it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0394, valid_loss=5.48e+3]\n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642, valid_loss=5.48e+3]         \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 108.44it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=5.48e+3]  \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=5.48e+3]         \n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00, 107.99it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627, valid_loss=5.48e+3]\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627, valid_loss=5.48e+3]         \n",
      "Epoch 185: 100%|██████████| 1/1 [00:00<00:00, 107.20it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.48e+3]\n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.48e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=5.48e+3]         \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=5.48e+3]\n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=5.48e+3]           \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 110.98it/s, v_num=0, train_loss_step=0.054, train_loss_epoch=0.0329, valid_loss=5.48e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 209.41it/s]\u001b[A\n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=6.29e+3]         \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=6.29e+3]           \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=6.29e+3]         \n",
      "Epoch 219: 100%|██████████| 1/1 [00:00<00:00, 106.77it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=6.29e+3]\n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=6.29e+3]         \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0457, train_loss_epoch=0.0457, valid_loss=6.29e+3]         \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=6.29e+3]           \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195, valid_loss=6.29e+3]         \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243, valid_loss=6.29e+3]         \n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 105.54it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=6.29e+3]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=6.29e+3]         \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=6.29e+3]         \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=6.29e+3]           \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197, valid_loss=6.29e+3]         \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=6.29e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 113.14it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.0149, valid_loss=6.29e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 211.88it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 58.39it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.0149, valid_loss=6.54e+3] \n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017, valid_loss=6.54e+3]         \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=6.54e+3]         \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=6.54e+3]         \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=6.54e+3]           \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=6.54e+3]           \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=6.54e+3]         \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=6.54e+3]           \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00811, train_loss_epoch=0.00811, valid_loss=6.54e+3]         \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00927, train_loss_epoch=0.00927, valid_loss=6.54e+3]         \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=6.54e+3]           \n",
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00874, train_loss_epoch=0.00874, valid_loss=6.54e+3]         \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 108.89it/s, v_num=0, train_loss_step=0.0086, train_loss_epoch=0.00874, valid_loss=6.54e+3] \n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.52it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 51.12it/s, v_num=0, train_loss_step=0.0086, train_loss_epoch=0.00874, valid_loss=6.52e+3] \n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0086, train_loss_epoch=0.0086, valid_loss=6.52e+3]         \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=6.52e+3]           \n",
      "Epoch 410: 100%|██████████| 1/1 [00:00<00:00, 106.03it/s, v_num=0, train_loss_step=0.00791, train_loss_epoch=0.00791, valid_loss=6.52e+3]\n",
      "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00791, train_loss_epoch=0.00791, valid_loss=6.52e+3]         \n",
      "Epoch 411: 100%|██████████| 1/1 [00:00<00:00, 105.38it/s, v_num=0, train_loss_step=0.00748, train_loss_epoch=0.00748, valid_loss=6.52e+3]\n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00748, train_loss_epoch=0.00748, valid_loss=6.52e+3]         \n",
      "Epoch 412: 100%|██████████| 1/1 [00:00<00:00, 105.78it/s, v_num=0, train_loss_step=0.00913, train_loss_epoch=0.00913, valid_loss=6.52e+3]\n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00913, train_loss_epoch=0.00913, valid_loss=6.52e+3]         \n",
      "Epoch 413: 100%|██████████| 1/1 [00:00<00:00, 105.55it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.52e+3]  \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.52e+3]         \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=6.52e+3]\n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.52e+3]         \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=6.52e+3]\n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00715, train_loss_epoch=0.00715, valid_loss=6.52e+3]         \n",
      "Epoch 426: 100%|██████████| 1/1 [00:00<00:00, 109.33it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=6.52e+3]    \n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=6.52e+3]         \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00913, train_loss_epoch=0.00913, valid_loss=6.52e+3]         \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=6.52e+3]             \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0074, train_loss_epoch=0.0074, valid_loss=6.52e+3]           \n",
      "Epoch 460: 100%|██████████| 1/1 [00:00<00:00, 106.58it/s, v_num=0, train_loss_step=0.00804, train_loss_epoch=0.00804, valid_loss=6.52e+3]\n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00804, train_loss_epoch=0.00804, valid_loss=6.52e+3]         \n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00995, train_loss_epoch=0.00995, valid_loss=6.52e+3]         \n",
      "Epoch 472: 100%|██████████| 1/1 [00:00<00:00, 92.41it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.014, valid_loss=6.52e+3]    \n",
      "Epoch 472: 100%|██████████| 1/1 [00:00<00:00, 88.57it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.52e+3]\n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=6.52e+3]        \n",
      "Epoch 473: 100%|██████████| 1/1 [00:00<00:00, 106.69it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=6.52e+3]\n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=6.52e+3]         \n",
      "Epoch 474: 100%|██████████| 1/1 [00:00<00:00, 106.23it/s, v_num=0, train_loss_step=0.00826, train_loss_epoch=0.00826, valid_loss=6.52e+3]\n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00826, train_loss_epoch=0.00826, valid_loss=6.52e+3]         \n",
      "Epoch 475: 100%|██████████| 1/1 [00:00<00:00, 106.97it/s, v_num=0, train_loss_step=0.00908, train_loss_epoch=0.00908, valid_loss=6.52e+3]\n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00908, train_loss_epoch=0.00908, valid_loss=6.52e+3]         \n",
      "Epoch 476: 100%|██████████| 1/1 [00:00<00:00, 106.30it/s, v_num=0, train_loss_step=0.00762, train_loss_epoch=0.00762, valid_loss=6.52e+3]\n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00762, train_loss_epoch=0.00762, valid_loss=6.52e+3]         \n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00818, train_loss_epoch=0.00818, valid_loss=6.52e+3]         \n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00748, train_loss_epoch=0.00748, valid_loss=6.52e+3]         \n",
      "Epoch 489: 100%|██████████| 1/1 [00:00<00:00, 107.41it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.52e+3]  \n",
      "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=6.52e+3]         \n",
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=6.52e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:10,213\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497: 100%|██████████| 1/1 [00:00<00:00, 97.06it/s, v_num=0, train_loss_step=0.00517, train_loss_epoch=0.00517, valid_loss=6.52e+3] \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.130, train_loss_epoch=1.130]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.524, train_loss_epoch=0.524]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.522, train_loss_epoch=0.522]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.605, train_loss_epoch=0.605]        \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 53.27it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.888, train_loss_epoch=0.888]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.543, train_loss_epoch=0.543]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.734, train_loss_epoch=0.734]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.763, train_loss_epoch=0.763]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.654, train_loss_epoch=0.654]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.419, train_loss_epoch=0.419]        \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 55.33it/s, v_num=0, train_loss_step=0.479, train_loss_epoch=0.419]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.479, train_loss_epoch=0.479]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.579, train_loss_epoch=0.579]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.434, train_loss_epoch=0.434]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.292, train_loss_epoch=0.292]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.368, train_loss_epoch=0.368]        \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 53.35it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.374, train_loss_epoch=0.374]        \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 47.99it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.290, train_loss_epoch=0.290]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 56.55it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.280]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 138.74it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349, valid_loss=4.78e+3]        \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 54.63it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=4.78e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=4.78e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=4.78e+3]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.204, train_loss_epoch=0.204, valid_loss=4.78e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215, valid_loss=4.78e+3]        \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 54.24it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.215, valid_loss=4.78e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269, valid_loss=4.78e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260, valid_loss=4.78e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198, valid_loss=4.78e+3]        \n",
      "Epoch 141: 100%|██████████| 1/1 [00:00<00:00, 55.48it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=4.78e+3]\n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=4.78e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=4.78e+3]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164, valid_loss=4.78e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=4.78e+3]        \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 54.11it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=4.78e+3]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=4.78e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.132, valid_loss=4.78e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:13,796\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 52.05it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138, valid_loss=4.78e+3]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.345, train_loss_epoch=0.345]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]         \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 115.22it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.132]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0746, train_loss_epoch=0.0746]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0493, train_loss_epoch=0.0493]         \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486]         \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 122.79it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0486]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0504, train_loss_epoch=0.0504]         \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 116.87it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0396, train_loss_epoch=0.0396]         \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 115.28it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0391]\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0391]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0375, train_loss_epoch=0.0375]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 120.96it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0318]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381]         \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315]         \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281]         \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 117.36it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259]         \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0299]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 123.17it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0299]\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.021, train_loss_epoch=0.021]           \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 118.00it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186]         \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 122.57it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0186]\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0165, train_loss_epoch=0.0165]         \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00994, train_loss_epoch=0.00994]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00928, train_loss_epoch=0.00928]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 117.37it/s, v_num=0, train_loss_step=0.00845, train_loss_epoch=0.00845]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00845, train_loss_epoch=0.00845]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00813, train_loss_epoch=0.00813]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00877, train_loss_epoch=0.00877]         \n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 110.70it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129]  \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123]         \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00755, train_loss_epoch=0.00755]         \n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 114.93it/s, v_num=0, train_loss_step=0.00727, train_loss_epoch=0.00727]\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00727, train_loss_epoch=0.00727]         \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00813, train_loss_epoch=0.00813]         \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00775, train_loss_epoch=0.00775]         \n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 117.06it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116]  \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00781, train_loss_epoch=0.00781]         \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 116.42it/s, v_num=0, train_loss_step=0.00736, train_loss_epoch=0.00736]\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00736, train_loss_epoch=0.00736]         \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00766, train_loss_epoch=0.00766]         \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0064, train_loss_epoch=0.0064]           \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 117.61it/s, v_num=0, train_loss_step=0.00785, train_loss_epoch=0.00785]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00785, train_loss_epoch=0.00785]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00885, train_loss_epoch=0.00885]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 121.57it/s, v_num=0, train_loss_step=0.00656, train_loss_epoch=0.00885]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 189.29it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 58.02it/s, v_num=0, train_loss_step=0.00656, train_loss_epoch=0.00885, valid_loss=5.51e+3]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 43.69it/s, v_num=0, train_loss_step=0.00656, train_loss_epoch=0.00656, valid_loss=5.51e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00656, train_loss_epoch=0.00656, valid_loss=5.51e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.007, train_loss_epoch=0.007, valid_loss=5.51e+3]             \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00663, train_loss_epoch=0.00663, valid_loss=5.51e+3]         \n",
      "Epoch 102: 100%|██████████| 1/1 [00:00<00:00, 116.57it/s, v_num=0, train_loss_step=0.00753, train_loss_epoch=0.00753, valid_loss=5.51e+3]\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00753, train_loss_epoch=0.00753, valid_loss=5.51e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=5.51e+3]           \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00699, train_loss_epoch=0.00699, valid_loss=5.51e+3]         \n",
      "Epoch 105: 100%|██████████| 1/1 [00:00<00:00, 115.39it/s, v_num=0, train_loss_step=0.00758, train_loss_epoch=0.00758, valid_loss=5.51e+3]\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00758, train_loss_epoch=0.00758, valid_loss=5.51e+3]         \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00754, train_loss_epoch=0.00754, valid_loss=5.51e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0063, train_loss_epoch=0.0063, valid_loss=5.51e+3]           \n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 116.86it/s, v_num=0, train_loss_step=0.00818, train_loss_epoch=0.00818, valid_loss=5.51e+3]\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00818, train_loss_epoch=0.00818, valid_loss=5.51e+3]         \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00754, train_loss_epoch=0.00754, valid_loss=5.51e+3]         \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00608, train_loss_epoch=0.00608, valid_loss=5.51e+3]         \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 116.04it/s, v_num=0, train_loss_step=0.00769, train_loss_epoch=0.00769, valid_loss=5.51e+3]\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00769, train_loss_epoch=0.00769, valid_loss=5.51e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00972, train_loss_epoch=0.00972, valid_loss=5.51e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00736, train_loss_epoch=0.00736, valid_loss=5.51e+3]         \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 117.05it/s, v_num=0, train_loss_step=0.00558, train_loss_epoch=0.00558, valid_loss=5.51e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00558, train_loss_epoch=0.00558, valid_loss=5.51e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00941, train_loss_epoch=0.00941, valid_loss=5.51e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00635, train_loss_epoch=0.00635, valid_loss=5.51e+3]         \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 115.83it/s, v_num=0, train_loss_step=0.00587, train_loss_epoch=0.00587, valid_loss=5.51e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00587, train_loss_epoch=0.00587, valid_loss=5.51e+3]         \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00712, train_loss_epoch=0.00712, valid_loss=5.51e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00595, train_loss_epoch=0.00595, valid_loss=5.51e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00707, train_loss_epoch=0.00707, valid_loss=5.51e+3]         \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00693, train_loss_epoch=0.00693, valid_loss=5.51e+3]         \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00592, train_loss_epoch=0.00592, valid_loss=5.51e+3]         \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 122.21it/s, v_num=0, train_loss_step=0.00531, train_loss_epoch=0.00592, valid_loss=5.51e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00531, train_loss_epoch=0.00531, valid_loss=5.51e+3]         \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0054, train_loss_epoch=0.0054, valid_loss=5.51e+3]           \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00564, train_loss_epoch=0.00564, valid_loss=5.51e+3]         \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 120.28it/s, v_num=0, train_loss_step=0.00471, train_loss_epoch=0.00564, valid_loss=5.51e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00471, train_loss_epoch=0.00471, valid_loss=5.51e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00379, train_loss_epoch=0.00379, valid_loss=5.51e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00748, train_loss_epoch=0.00748, valid_loss=5.51e+3]         \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 116.16it/s, v_num=0, train_loss_step=0.00382, train_loss_epoch=0.00382, valid_loss=5.51e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00382, train_loss_epoch=0.00382, valid_loss=5.51e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00379, train_loss_epoch=0.00379, valid_loss=5.51e+3]         \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0036, train_loss_epoch=0.0036, valid_loss=5.51e+3]           \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 115.76it/s, v_num=0, train_loss_step=0.00806, train_loss_epoch=0.00806, valid_loss=5.51e+3]\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00806, train_loss_epoch=0.00806, valid_loss=5.51e+3]         \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00432, train_loss_epoch=0.00432, valid_loss=5.51e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00452, train_loss_epoch=0.00452, valid_loss=5.51e+3]         \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 114.13it/s, v_num=0, train_loss_step=0.00478, train_loss_epoch=0.00478, valid_loss=5.51e+3]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00478, train_loss_epoch=0.00478, valid_loss=5.51e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00507, train_loss_epoch=0.00507, valid_loss=5.51e+3]         \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00822, train_loss_epoch=0.00822, valid_loss=5.51e+3]         \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 118.32it/s, v_num=0, train_loss_step=0.00572, train_loss_epoch=0.00822, valid_loss=5.51e+3]\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00572, train_loss_epoch=0.00572, valid_loss=5.51e+3]         \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00695, train_loss_epoch=0.00695, valid_loss=5.51e+3]         \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 112.58it/s, v_num=0, train_loss_step=0.00573, train_loss_epoch=0.00573, valid_loss=5.51e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00573, train_loss_epoch=0.00573, valid_loss=5.51e+3]         \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=5.51e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00623, train_loss_epoch=0.00623, valid_loss=5.51e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00682, train_loss_epoch=0.00682, valid_loss=5.51e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00811, train_loss_epoch=0.00811, valid_loss=5.51e+3]         \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00574, train_loss_epoch=0.00574, valid_loss=5.51e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00487, train_loss_epoch=0.00487, valid_loss=5.51e+3]         \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 123.60it/s, v_num=0, train_loss_step=0.00504, train_loss_epoch=0.00487, valid_loss=5.51e+3]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00504, train_loss_epoch=0.00504, valid_loss=5.51e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00428, train_loss_epoch=0.00428, valid_loss=5.51e+3]         \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00448, train_loss_epoch=0.00448, valid_loss=5.51e+3]         \n",
      "Epoch 180: 100%|██████████| 1/1 [00:00<00:00, 117.02it/s, v_num=0, train_loss_step=0.00713, train_loss_epoch=0.00713, valid_loss=5.51e+3]\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00713, train_loss_epoch=0.00713, valid_loss=5.51e+3]         \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00565, train_loss_epoch=0.00565, valid_loss=5.51e+3]         \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00943, train_loss_epoch=0.00943, valid_loss=5.51e+3]         \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 117.36it/s, v_num=0, train_loss_step=0.00814, train_loss_epoch=0.00814, valid_loss=5.51e+3]\n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00814, train_loss_epoch=0.00814, valid_loss=5.51e+3]         \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00902, train_loss_epoch=0.00902, valid_loss=5.51e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00763, train_loss_epoch=0.00763, valid_loss=5.51e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00691, train_loss_epoch=0.00691, valid_loss=5.51e+3]         \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00691, train_loss_epoch=0.00691, valid_loss=5.51e+3]\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0064, train_loss_epoch=0.0064, valid_loss=5.51e+3]           \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00582, train_loss_epoch=0.00582, valid_loss=5.51e+3]         \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00624, train_loss_epoch=0.00624, valid_loss=5.51e+3]         \n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00, 126.59it/s, v_num=0, train_loss_step=0.00624, train_loss_epoch=0.00624, valid_loss=5.51e+3]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00763, train_loss_epoch=0.00763, valid_loss=5.51e+3]         \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00627, train_loss_epoch=0.00627, valid_loss=5.51e+3]         \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0068, train_loss_epoch=0.0068, valid_loss=5.51e+3]           \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 116.00it/s, v_num=0, train_loss_step=0.00562, train_loss_epoch=0.00562, valid_loss=5.51e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00562, train_loss_epoch=0.00562, valid_loss=5.51e+3]         \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00591, train_loss_epoch=0.00591, valid_loss=5.51e+3]         \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00606, train_loss_epoch=0.00606, valid_loss=5.51e+3]         \n",
      "Epoch 196: 100%|██████████| 1/1 [00:00<00:00, 122.45it/s, v_num=0, train_loss_step=0.00542, train_loss_epoch=0.00606, valid_loss=5.51e+3]\n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00542, train_loss_epoch=0.00542, valid_loss=5.51e+3]         \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00655, train_loss_epoch=0.00655, valid_loss=5.51e+3]         \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00516, train_loss_epoch=0.00516, valid_loss=5.51e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:15,907\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 121.89it/s, v_num=0, train_loss_step=0.00754, train_loss_epoch=0.00516, valid_loss=5.51e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 219.69it/s]\u001b[A\n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00, 91.48it/s, v_num=0, train_loss_step=0.00403, train_loss_epoch=0.00403, valid_loss=5.59e+3] \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.809, train_loss_epoch=0.809]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=1.040, train_loss_epoch=1.040]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.818, train_loss_epoch=0.818]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.970, train_loss_epoch=0.970]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.970, train_loss_epoch=0.970]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 95.02it/s, v_num=0, train_loss_step=0.627, train_loss_epoch=0.970]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.627, train_loss_epoch=0.627]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.770, train_loss_epoch=0.770]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.722, train_loss_epoch=0.722]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.850, train_loss_epoch=0.850]        \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 90.11it/s, v_num=0, train_loss_step=0.518, train_loss_epoch=0.518]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.518, train_loss_epoch=0.518]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.438, train_loss_epoch=0.438]        \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 90.53it/s, v_num=0, train_loss_step=0.577, train_loss_epoch=0.577]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.577, train_loss_epoch=0.577]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.516, train_loss_epoch=0.516]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.358, train_loss_epoch=0.358]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216]        \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 93.01it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.409]\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.446, train_loss_epoch=0.446]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.414, train_loss_epoch=0.414]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.334, train_loss_epoch=0.334]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.328, train_loss_epoch=0.328]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202]        \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 83.19it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.202]\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]        \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 79.43it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 78.66it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 77.64it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 80.85it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.285]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 140.63it/s]\u001b[A\n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 90.32it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=4.87e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=4.87e+3]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279, valid_loss=4.87e+3]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197, valid_loss=4.87e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=4.87e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200, valid_loss=4.87e+3]        \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 81.08it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=4.87e+3]\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=4.87e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=4.87e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:17,919\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153, valid_loss=4.87e+3]        \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 85.81it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187, valid_loss=4.87e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187, valid_loss=4.87e+3]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.193, valid_loss=4.87e+3]        \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 94.12it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=4.87e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=4.87e+3]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152, valid_loss=4.87e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 81.77it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153, valid_loss=4.87e+3]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.292, train_loss_epoch=0.292]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289]         \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 110.82it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]        \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 85.67it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.115]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]        \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 82.70it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]        \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 85.48it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.116]\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.097, train_loss_epoch=0.097]           \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0864, train_loss_epoch=0.0864]         \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 82.30it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0882, train_loss_epoch=0.0882]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897]        \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 109.81it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0897]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842]         \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:19,137\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0756, train_loss_epoch=0.0756]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 119.71it/s, v_num=0, train_loss_step=0.0737, train_loss_epoch=0.0756]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 188.33it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 44.18it/s, v_num=0, train_loss_step=0.0737, train_loss_epoch=0.0737, valid_loss=5.18e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0737, train_loss_epoch=0.0737, valid_loss=5.18e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0757, train_loss_epoch=0.0757, valid_loss=5.18e+3]         \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717, valid_loss=5.18e+3]         \n",
      "Epoch 105: 100%|██████████| 1/1 [00:00<00:00, 105.44it/s, v_num=0, train_loss_step=0.0699, train_loss_epoch=0.0699, valid_loss=5.18e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.721, train_loss_epoch=0.721]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.738, train_loss_epoch=0.738]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.576, train_loss_epoch=0.576]        \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 92.57it/s, v_num=0, train_loss_step=0.579, train_loss_epoch=0.579]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.579, train_loss_epoch=0.579]        \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 94.67it/s, v_num=0, train_loss_step=0.497, train_loss_epoch=0.579]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.497, train_loss_epoch=0.497]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.297, train_loss_epoch=0.297]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.357, train_loss_epoch=0.357]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284]         \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 102.37it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251]\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.229, train_loss_epoch=0.229]         \n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00, 102.70it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]         \n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00, 99.43it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191] \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 100.40it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]         \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 101.36it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 92.15it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127]         \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 109.29it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.112]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 196.77it/s]\u001b[A\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=5.32e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=5.32e+3]        \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 104.81it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.104, valid_loss=5.32e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=5.32e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100, valid_loss=5.32e+3]         \n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 100.71it/s, v_num=0, train_loss_step=0.0974, train_loss_epoch=0.0974, valid_loss=5.32e+3]\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0974, train_loss_epoch=0.0974, valid_loss=5.32e+3]         \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0994, train_loss_epoch=0.0994, valid_loss=5.32e+3]         \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 100.96it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916, valid_loss=5.32e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916, valid_loss=5.32e+3]         \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=5.32e+3]           \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 100.88it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=5.32e+3]\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=5.32e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897, valid_loss=5.32e+3]         \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 102.93it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0897, valid_loss=5.32e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0922, valid_loss=5.32e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=5.32e+3]           \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 105.27it/s, v_num=0, train_loss_step=0.0925, train_loss_epoch=0.109, valid_loss=5.32e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0925, train_loss_epoch=0.0925, valid_loss=5.32e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.088, train_loss_epoch=0.088, valid_loss=5.32e+3]           \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 100.58it/s, v_num=0, train_loss_step=0.0828, train_loss_epoch=0.0828, valid_loss=5.32e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0828, train_loss_epoch=0.0828, valid_loss=5.32e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0892, train_loss_epoch=0.0892, valid_loss=5.32e+3]         \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 99.76it/s, v_num=0, train_loss_step=0.0883, train_loss_epoch=0.0883, valid_loss=5.32e+3] \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0883, train_loss_epoch=0.0883, valid_loss=5.32e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100, valid_loss=5.32e+3]           \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 99.36it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805, valid_loss=5.32e+3]\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805, valid_loss=5.32e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.079, train_loss_epoch=0.079, valid_loss=5.32e+3]           \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 103.87it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.079, valid_loss=5.32e+3]\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767, valid_loss=5.32e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0947, train_loss_epoch=0.0947, valid_loss=5.32e+3]         \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 104.66it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0947, valid_loss=5.32e+3]\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751, valid_loss=5.32e+3]         \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0794, train_loss_epoch=0.0794, valid_loss=5.32e+3]         \n",
      "Epoch 138: 100%|██████████| 1/1 [00:00<00:00, 101.33it/s, v_num=0, train_loss_step=0.0648, train_loss_epoch=0.0648, valid_loss=5.32e+3]\n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0648, train_loss_epoch=0.0648, valid_loss=5.32e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0874, train_loss_epoch=0.0874, valid_loss=5.32e+3]         \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 99.73it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681, valid_loss=5.32e+3] \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681, valid_loss=5.32e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=5.32e+3]         \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 101.32it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.32e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0603, train_loss_epoch=0.0603, valid_loss=5.32e+3]         \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0786, train_loss_epoch=0.0786, valid_loss=5.32e+3]         \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 101.60it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=5.32e+3]\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=5.32e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0618, train_loss_epoch=0.0618, valid_loss=5.32e+3]         \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 102.61it/s, v_num=0, train_loss_step=0.0573, train_loss_epoch=0.0618, valid_loss=5.32e+3]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0573, train_loss_epoch=0.0573, valid_loss=5.32e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0568, train_loss_epoch=0.0568, valid_loss=5.32e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716, valid_loss=5.32e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0524, train_loss_epoch=0.0524, valid_loss=5.32e+3]         \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0697, train_loss_epoch=0.0697, valid_loss=5.32e+3]         \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 101.70it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=5.32e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=5.32e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0453, train_loss_epoch=0.0453, valid_loss=5.32e+3]         \n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00, 100.22it/s, v_num=0, train_loss_step=0.0639, train_loss_epoch=0.0639, valid_loss=5.32e+3]\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0639, train_loss_epoch=0.0639, valid_loss=5.32e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0426, train_loss_epoch=0.0426, valid_loss=5.32e+3]         \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 102.23it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=5.32e+3]  \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=5.32e+3]         \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0573, train_loss_epoch=0.0573, valid_loss=5.32e+3]         \n",
      "Epoch 185: 100%|██████████| 1/1 [00:00<00:00, 84.69it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=5.32e+3]  \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=5.32e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0379, train_loss_epoch=0.0379, valid_loss=5.32e+3]        \n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00, 73.83it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=5.32e+3]\n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=5.32e+3]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=5.32e+3]        \n",
      "Epoch 189: 100%|██████████| 1/1 [00:00<00:00, 73.79it/s, v_num=0, train_loss_step=0.038, train_loss_epoch=0.038, valid_loss=5.32e+3]  \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.038, train_loss_epoch=0.038, valid_loss=5.32e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548, valid_loss=5.32e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548, valid_loss=5.32e+3]\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 94.77it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.037, valid_loss=5.32e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 175.19it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=20096)\u001b[0m \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506, valid_loss=5.62e+3]        \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506, valid_loss=5.62e+3]\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0387, train_loss_epoch=0.0387, valid_loss=5.62e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=5.62e+3]         \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=5.62e+3]         \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=5.62e+3]         \n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 101.27it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.0259, valid_loss=5.62e+3] \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.025, valid_loss=5.62e+3]          \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=5.62e+3]        \n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00, 99.01it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.62e+3] \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=5.62e+3]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=5.62e+3]           \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0229, train_loss_epoch=0.0229, valid_loss=5.62e+3]         \n",
      "Epoch 238: 100%|██████████| 1/1 [00:00<00:00, 101.79it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195, valid_loss=5.62e+3]\n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0195, train_loss_epoch=0.0195, valid_loss=5.62e+3]         \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=5.62e+3]           \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=5.62e+3]         \n",
      "Epoch 251: 100%|██████████| 1/1 [00:00<00:00, 101.57it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=5.62e+3]\n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=5.62e+3]         \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0203, train_loss_epoch=0.0203, valid_loss=5.62e+3]         \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0229, train_loss_epoch=0.0229, valid_loss=5.62e+3]         \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=5.62e+3]         \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 104.48it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0164, valid_loss=5.62e+3]\n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 100.34it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=5.62e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=5.62e+3]         \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=5.62e+3]         \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 100.76it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=5.62e+3]\n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=5.62e+3]         \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=5.62e+3]         \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017, valid_loss=5.62e+3]           \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0135, train_loss_epoch=0.0135, valid_loss=5.62e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:22,544\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=5.62e+3]         \n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00, 105.28it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0259, valid_loss=5.62e+3]\n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=5.62e+3]         \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=5.62e+3]         \n",
      "Epoch 294: 100%|██████████| 1/1 [00:00<00:00, 100.99it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=5.62e+3]\n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=5.62e+3]         \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=5.62e+3]         \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 93.30it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=5.62e+3] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e970ad39f947049bfba261acca13bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad23863cf3c487cbc29f7ccef6576fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce21e04aa08d416d8e4e4d8b6b529110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf9616aaa0f4d63ae4af04ed12021d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nixtla2 = df_nixtla[df_nixtla.ds<='2022-05-01'].copy()\n",
    "\n",
    "nf = NeuralForecast(\n",
    "     models=model,\n",
    "     freq='MS')\n",
    "\n",
    "val_size  = 18 # 3 x 6 months\n",
    "test_size = 6 # 1 x 6 months\n",
    "\n",
    "Y_hat_df = nf.cross_validation(df=df_nixtla2,\n",
    "                               static_df=static_df,\n",
    "                               val_size=val_size,\n",
    "                               test_size=test_size, \n",
    "                               n_windows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "12f11e6d-bf5b-442d-8810-34f2c44e76f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 12,\n",
       " 'encoder_hidden_size': 128,\n",
       " 'encoder_n_layers': 2,\n",
       " 'encoder_dropout': 0.05886265561818178,\n",
       " 'decoder_hidden_size': 126,\n",
       " 'decoder_layers': 4,\n",
       " 'learning_rate': 0.0002828828009198774,\n",
       " 'scaler_type': 'robust',\n",
       " 'max_steps': 52,\n",
       " 'batch_size': 64,\n",
       " 'num_lr_decays': 0.037815201774331834,\n",
       " 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'),\n",
       " 'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'),\n",
       " 'stat_exog_list': ('total_hcp_cnt',),\n",
       " 'h': 6,\n",
       " 'loss': MQLoss(),\n",
       " 'valid_loss': MQLoss()}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf.models[0].results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "db0b2b80-2272-4180-831b-9434c5d96a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 15.00it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.359, train_loss_epoch=0.359]         \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.390, train_loss_epoch=0.390]         \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.428, train_loss_epoch=0.428]         \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 99.48it/s, v_num=0, train_loss_step=0.373, train_loss_epoch=0.373] \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.373, train_loss_epoch=0.373]        \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 101.08it/s, v_num=0, train_loss_step=0.382, train_loss_epoch=0.382]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.382, train_loss_epoch=0.382]         \n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 104.43it/s, v_num=0, train_loss_step=0.420, train_loss_epoch=0.382]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.420, train_loss_epoch=0.420]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 105.72it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.420]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]         \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 104.89it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.311]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 107.65it/s, v_num=0, train_loss_step=0.323, train_loss_epoch=0.323]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 106.50it/s, v_num=0, train_loss_step=0.305, train_loss_epoch=0.305]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.365, train_loss_epoch=0.365]         \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 103.19it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.267]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259]         \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 90.14it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.217, train_loss_epoch=0.217]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214]        \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 91.14it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242]\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00, 98.58it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194] \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.204, train_loss_epoch=0.204]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.199, train_loss_epoch=0.199]         \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]         \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 103.03it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.189]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]         \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 99.11it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137] \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 105.66it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.204]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 146.31it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150, valid_loss=2.85e+3]         \n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 98.62it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=2.85e+3] \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=2.85e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=2.85e+3]         \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129, valid_loss=2.85e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163, valid_loss=2.85e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.85e+3]         \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.85e+3]         \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=2.85e+3]         \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.179, train_loss_epoch=0.179, valid_loss=2.85e+3]         \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 102.65it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=2.85e+3]\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=2.85e+3]         \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129, valid_loss=2.85e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134, valid_loss=2.85e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=2.85e+3]         \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=2.85e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=2.85e+3]         \n",
      "Epoch 158: 100%|██████████| 1/1 [00:00<00:00, 106.12it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.131, valid_loss=2.85e+3]\n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138, valid_loss=2.85e+3]         \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.85e+3]         \n",
      "Epoch 160: 100%|██████████| 1/1 [00:00<00:00, 101.29it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127, valid_loss=2.85e+3]\n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127, valid_loss=2.85e+3]         \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154, valid_loss=2.85e+3]         \n",
      "Epoch 162: 100%|██████████| 1/1 [00:00<00:00, 102.83it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.85e+3]\n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.85e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0995, train_loss_epoch=0.0995, valid_loss=2.85e+3]         \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 101.84it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0992, valid_loss=2.85e+3]\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0992, valid_loss=2.85e+3]         \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.210, valid_loss=2.85e+3]           \n",
      "Epoch 166: 100%|██████████| 1/1 [00:00<00:00, 101.95it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.85e+3]\n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.85e+3]         \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.85e+3]         \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=2.85e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.85e+3]           \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.85e+3]         \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.85e+3]         \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=2.85e+3]           \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140, valid_loss=2.85e+3]         \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.85e+3]         \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 106.00it/s, v_num=0, train_loss_step=0.0961, train_loss_epoch=0.106, valid_loss=2.85e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0961, train_loss_epoch=0.0961, valid_loss=2.85e+3]         \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114, valid_loss=2.85e+3]           \n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 102.22it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=2.85e+3]\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=2.85e+3]         \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0967, train_loss_epoch=0.0967, valid_loss=2.85e+3]         \n",
      "Epoch 197: 100%|██████████| 1/1 [00:00<00:00, 102.95it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916, valid_loss=2.85e+3]\n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916, valid_loss=2.85e+3]         \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.85e+3]           \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 105.90it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=0.107, valid_loss=2.85e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 195.78it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 43.95it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=0.0906, valid_loss=2.62e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=0.0906, valid_loss=2.62e+3]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089, valid_loss=2.62e+3]           \n",
      "Epoch 201: 100%|██████████| 1/1 [00:00<00:00, 101.18it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123, valid_loss=2.62e+3]\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123, valid_loss=2.62e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089, valid_loss=2.62e+3]         \n",
      "Epoch 203: 100%|██████████| 1/1 [00:00<00:00, 102.41it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.62e+3]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.62e+3]         \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0864, train_loss_epoch=0.0864, valid_loss=2.62e+3]         \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160, valid_loss=2.62e+3]           \n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 103.25it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.62e+3]  \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.62e+3]         \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121, valid_loss=2.62e+3]         \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 102.73it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.62e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.62e+3]         \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=2.62e+3]         \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=2.62e+3]         \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851, valid_loss=2.62e+3]         \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0969, train_loss_epoch=0.0969, valid_loss=2.62e+3]         \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0857, train_loss_epoch=0.0857, valid_loss=2.62e+3]         \n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 102.29it/s, v_num=0, train_loss_step=0.090, train_loss_epoch=0.090, valid_loss=2.62e+3]  \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.090, train_loss_epoch=0.090, valid_loss=2.62e+3]         \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0872, train_loss_epoch=0.0872, valid_loss=2.62e+3]         \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0734, train_loss_epoch=0.0734, valid_loss=2.62e+3]         \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.62e+3]           \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.62e+3]         \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091, valid_loss=2.62e+3]         \n",
      "Epoch 258: 100%|██████████| 1/1 [00:00<00:00, 106.89it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=0.091, valid_loss=2.62e+3]\n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0906, train_loss_epoch=0.0906, valid_loss=2.62e+3]         \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.62e+3]           \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 102.40it/s, v_num=0, train_loss_step=0.0818, train_loss_epoch=0.0818, valid_loss=2.62e+3]\n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0818, train_loss_epoch=0.0818, valid_loss=2.62e+3]         \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0814, train_loss_epoch=0.0814, valid_loss=2.62e+3]         \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 101.02it/s, v_num=0, train_loss_step=0.0919, train_loss_epoch=0.0919, valid_loss=2.62e+3]\n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0919, train_loss_epoch=0.0919, valid_loss=2.62e+3]         \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.080, train_loss_epoch=0.080, valid_loss=2.62e+3]           \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.0802, valid_loss=2.62e+3]         \n",
      "Epoch 275: 100%|██████████| 1/1 [00:00<00:00, 103.10it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.62e+3]  \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.62e+3]         \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0856, train_loss_epoch=0.0856, valid_loss=2.62e+3]        \n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 101.53it/s, v_num=0, train_loss_step=0.0702, train_loss_epoch=0.0702, valid_loss=2.62e+3]\n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0702, train_loss_epoch=0.0702, valid_loss=2.62e+3]         \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0797, train_loss_epoch=0.0797, valid_loss=2.62e+3]         \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0735, train_loss_epoch=0.0735, valid_loss=2.62e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:32,380\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00, 94.57it/s, v_num=0, train_loss_step=0.0866, train_loss_epoch=0.0866, valid_loss=2.62e+3] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]          \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.257, train_loss_epoch=0.257]          \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 80.71it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]  \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]        \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 62.33it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0932, train_loss_epoch=0.0932]        \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 68.31it/s, v_num=0, train_loss_step=0.0913, train_loss_epoch=0.0913]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0913, train_loss_epoch=0.0913]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0835, train_loss_epoch=0.0835]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0498, train_loss_epoch=0.0498]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 70.48it/s, v_num=0, train_loss_step=0.0946, train_loss_epoch=0.0946]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0946, train_loss_epoch=0.0946]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0388, train_loss_epoch=0.0388]        \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 65.03it/s, v_num=0, train_loss_step=0.058, train_loss_epoch=0.058]  \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0577, train_loss_epoch=0.0577]        \n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 71.01it/s, v_num=0, train_loss_step=0.0746, train_loss_epoch=0.0746]\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0746, train_loss_epoch=0.0746]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0624]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]          \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0626, train_loss_epoch=0.0626]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 85.72it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.0566] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 144.22it/s]\u001b[A\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0697, train_loss_epoch=0.0697, valid_loss=1.22e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.062, train_loss_epoch=0.062, valid_loss=1.22e+3]          \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=1.22e+3]        \n",
      "Epoch 128: 100%|██████████| 1/1 [00:00<00:00, 79.82it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352, valid_loss=1.22e+3]\n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352, valid_loss=1.22e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=1.22e+3]        \n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 74.04it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0355, valid_loss=1.22e+3]\n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=1.22e+3]        \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 70.96it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.22e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.22e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=1.22e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=1.22e+3]        \n",
      "Epoch 160: 100%|██████████| 1/1 [00:00<00:00, 80.71it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.22e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.22e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.22e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=1.22e+3]        \n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 62.72it/s, v_num=0, train_loss_step=0.0422, train_loss_epoch=0.0422, valid_loss=1.22e+3]\n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0422, train_loss_epoch=0.0422, valid_loss=1.22e+3]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=1.22e+3]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0367, train_loss_epoch=0.0367, valid_loss=1.22e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=1.22e+3]          \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=1.22e+3]        \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 81.39it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.0265, valid_loss=1.22e+3] \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=1.22e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 85.36it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0197, valid_loss=1.22e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.72it/s]\u001b[A\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0229, train_loss_epoch=0.0229, valid_loss=1.74e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=1.74e+3]        \n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 83.89it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=1.74e+3]\n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0417, train_loss_epoch=0.0417, valid_loss=1.74e+3]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0535, valid_loss=1.74e+3]        \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309, valid_loss=1.74e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272, valid_loss=1.74e+3]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0656, train_loss_epoch=0.0656, valid_loss=1.74e+3]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=1.74e+3]        \n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 81.22it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865, valid_loss=1.74e+3]\n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865, valid_loss=1.74e+3]        \n",
      "Epoch 251: 100%|██████████| 1/1 [00:00<00:00, 60.75it/s, v_num=0, train_loss_step=0.0403, train_loss_epoch=0.0403, valid_loss=1.74e+3]\n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0403, train_loss_epoch=0.0403, valid_loss=1.74e+3]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=1.74e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.74e+3]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.74e+3]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=1.74e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=1.74e+3]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0171, train_loss_epoch=0.0171, valid_loss=1.74e+3]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.74e+3]        \n",
      "Epoch 293: 100%|██████████| 1/1 [00:00<00:00, 83.42it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.74e+3]\n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0108, train_loss_epoch=0.0108, valid_loss=1.74e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 66.45it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0191, valid_loss=1.74e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.98it/s]\u001b[A\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243, valid_loss=2.02e+3]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00866, train_loss_epoch=0.00866, valid_loss=2.02e+3]        \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=2.02e+3]          \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0203, train_loss_epoch=0.0203, valid_loss=2.02e+3]        \n",
      "Epoch 317: 100%|██████████| 1/1 [00:00<00:00, 80.29it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=2.02e+3]\n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=2.02e+3]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=2.02e+3]        \n",
      "Epoch 326: 100%|██████████| 1/1 [00:00<00:00, 65.35it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=2.02e+3]\n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=2.02e+3]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=2.02e+3]        \n",
      "Epoch 335: 100%|██████████| 1/1 [00:00<00:00, 64.37it/s, v_num=0, train_loss_step=0.00964, train_loss_epoch=0.00964, valid_loss=2.02e+3]\n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00964, train_loss_epoch=0.00964, valid_loss=2.02e+3]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00964, train_loss_epoch=0.00964, valid_loss=2.02e+3]\n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.02e+3]          \n",
      "Epoch 344: 100%|██████████| 1/1 [00:00<00:00, 67.70it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0274, valid_loss=2.02e+3]\n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0274, valid_loss=2.02e+3]        \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.021, train_loss_epoch=0.021, valid_loss=2.02e+3]          \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0611, train_loss_epoch=0.0611, valid_loss=2.02e+3]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0377, train_loss_epoch=0.0377, valid_loss=2.02e+3]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=2.02e+3]        \n",
      "Epoch 378:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=2.02e+3]          \n",
      "Epoch 378: 100%|██████████| 1/1 [00:00<00:00, 83.70it/s, v_num=0, train_loss_step=0.00985, train_loss_epoch=0.0119, valid_loss=2.02e+3]\n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00985, train_loss_epoch=0.00985, valid_loss=2.02e+3]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389, valid_loss=2.02e+3]          \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=2.02e+3]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017, valid_loss=2.02e+3]          \n",
      "Epoch 388: 100%|██████████| 1/1 [00:00<00:00, 79.16it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=2.02e+3]\n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=2.02e+3]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=2.02e+3]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=2.02e+3]        \n",
      "Epoch 398: 100%|██████████| 1/1 [00:00<00:00, 80.20it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=2.02e+3]  \n",
      "Epoch 399:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=2.02e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 62.54it/s, v_num=0, train_loss_step=0.0165, train_loss_epoch=0.019, valid_loss=2.02e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.58it/s]\u001b[A\n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=1.66e+3]          \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=1.66e+3]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243, valid_loss=1.66e+3]        \n",
      "Epoch 414: 100%|██████████| 1/1 [00:00<00:00, 79.46it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=1.66e+3]\n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=1.66e+3]        \n",
      "Epoch 422: 100%|██████████| 1/1 [00:00<00:00, 76.59it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.66e+3]\n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.66e+3]        \n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.66e+3]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=1.66e+3]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=1.66e+3]          \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0115, train_loss_epoch=0.0115, valid_loss=1.66e+3]        \n",
      "Epoch 440: 100%|██████████| 1/1 [00:00<00:00, 81.20it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0115, valid_loss=1.66e+3]\n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=1.66e+3]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=1.66e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:39,244\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=1.66e+3]        \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=1.66e+3]        \n",
      "Epoch 450: 100%|██████████| 1/1 [00:00<00:00, 77.76it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=1.66e+3]\n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=1.66e+3]        \n",
      "Epoch 452: 100%|██████████| 1/1 [00:00<00:00, 74.73it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.66e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.326, train_loss_epoch=0.326]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.179, train_loss_epoch=0.179]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150]        \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 75.99it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0983, train_loss_epoch=0.0983]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:40,355\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0998, train_loss_epoch=0.0998]        \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00, 57.97it/s, v_num=0, train_loss_step=0.0986, train_loss_epoch=0.0986]\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0986, train_loss_epoch=0.0986]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]          \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]          \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 84.44it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.112]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.24it/s]\u001b[A\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 33.95it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.45e+3]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150]         \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 107.72it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.205, train_loss_epoch=0.205]         \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]         \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 114.20it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.134]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 110.24it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.095, train_loss_epoch=0.095]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0878, train_loss_epoch=0.0878]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0996, train_loss_epoch=0.0996]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119]           \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0564]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0957, train_loss_epoch=0.0957]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]           \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.093, train_loss_epoch=0.093]         \n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 107.23it/s, v_num=0, train_loss_step=0.0657, train_loss_epoch=0.0657]\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0657, train_loss_epoch=0.0657]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]           \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 114.22it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0668]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0504, train_loss_epoch=0.0504]         \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 101.98it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040]           \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0739, train_loss_epoch=0.0739]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0779, train_loss_epoch=0.0779]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0769, train_loss_epoch=0.0769]         \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059]          \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:41,757\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 115.63it/s, v_num=0, train_loss_step=0.0693, train_loss_epoch=0.0371]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.02it/s]\u001b[A\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=3.03e+3]           \n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 81.70it/s, v_num=0, train_loss_step=0.0783, train_loss_epoch=0.119, valid_loss=3.03e+3]\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0783, train_loss_epoch=0.0783, valid_loss=3.03e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0995, train_loss_epoch=0.0995, valid_loss=3.03e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 78.40it/s, v_num=0, train_loss_step=0.0898, train_loss_epoch=0.0898, valid_loss=3.03e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0898, train_loss_epoch=0.0898, valid_loss=3.03e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=3.03e+3]          \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=3.03e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 100.45it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717, valid_loss=3.03e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.431, train_loss_epoch=0.431]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.450, train_loss_epoch=0.450]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.395, train_loss_epoch=0.395]        \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 65.49it/s, v_num=0, train_loss_step=0.329, train_loss_epoch=0.329]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.329, train_loss_epoch=0.329]       \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 65.87it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.379]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.379, train_loss_epoch=0.379]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 64.79it/s, v_num=0, train_loss_step=0.422, train_loss_epoch=0.422]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.422, train_loss_epoch=0.422]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.350, train_loss_epoch=0.350]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.350, train_loss_epoch=0.350]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.406, train_loss_epoch=0.406]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.327, train_loss_epoch=0.327]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.306, train_loss_epoch=0.306]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.176, train_loss_epoch=0.176]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.214, train_loss_epoch=0.214]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 74.57it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.224]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 137.39it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.213, train_loss_epoch=0.213, valid_loss=2.46e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.199, train_loss_epoch=0.199, valid_loss=2.46e+3]        \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 70.27it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211, valid_loss=2.46e+3]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211, valid_loss=2.46e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177, valid_loss=2.46e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198, valid_loss=2.46e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=2.46e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=2.46e+3]        \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 68.54it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180, valid_loss=2.46e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142, valid_loss=2.46e+3]        \n",
      "Epoch 141: 100%|██████████| 1/1 [00:00<00:00, 53.06it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160, valid_loss=2.46e+3]\n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160, valid_loss=2.46e+3]        \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 63.20it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=2.46e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=2.46e+3]        \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 64.57it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.156, valid_loss=2.46e+3]\n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.46e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152, valid_loss=2.46e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152, valid_loss=2.46e+3]\n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00, 69.75it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.152, valid_loss=2.46e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221, valid_loss=2.46e+3]        \n",
      "Epoch 160: 100%|██████████| 1/1 [00:00<00:00, 69.53it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=2.46e+3]\n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=2.46e+3]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=2.46e+3]        \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 71.45it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=2.46e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=2.46e+3]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173, valid_loss=2.46e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164, valid_loss=2.46e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157, valid_loss=2.46e+3]        \n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 71.57it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.46e+3]\n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.46e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=2.46e+3]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=2.46e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 77.56it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.115, valid_loss=2.46e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.05it/s]\u001b[A\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=2.93e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167, valid_loss=2.93e+3]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 65.29it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.117, valid_loss=2.93e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.93e+3]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=2.93e+3]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163, valid_loss=2.93e+3]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163, valid_loss=2.93e+3]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.93e+3]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.93e+3]        \n",
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00, 72.70it/s, v_num=0, train_loss_step=0.0949, train_loss_epoch=0.0949, valid_loss=2.93e+3]\n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0949, train_loss_epoch=0.0949, valid_loss=2.93e+3]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.158, train_loss_epoch=0.158, valid_loss=2.93e+3]          \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.93e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.93e+3]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.93e+3]          \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.93e+3]          \n",
      "Epoch 288: 100%|██████████| 1/1 [00:00<00:00, 72.69it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.93e+3]  \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.93e+3]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0968, train_loss_epoch=0.0968, valid_loss=2.93e+3]        \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0992, valid_loss=2.93e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0982, train_loss_epoch=0.0982, valid_loss=2.93e+3]        \n",
      "Epoch 298: 100%|██████████| 1/1 [00:00<00:00, 73.71it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.0982, valid_loss=2.93e+3] \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150, valid_loss=2.93e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 74.07it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.150, valid_loss=2.93e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 131.71it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 38.96it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.150, valid_loss=2.49e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137, valid_loss=2.49e+3]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0899, train_loss_epoch=0.0899, valid_loss=2.49e+3]        \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0888, train_loss_epoch=0.0888, valid_loss=2.49e+3]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167, valid_loss=2.49e+3]          \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971, valid_loss=2.49e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=2.49e+3]          \n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0968, train_loss_epoch=0.0968, valid_loss=2.49e+3]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0961, train_loss_epoch=0.0961, valid_loss=2.49e+3]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=2.49e+3]          \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=2.49e+3]          \n",
      "Epoch 348: 100%|██████████| 1/1 [00:00<00:00, 65.32it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128, valid_loss=2.49e+3]  \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128, valid_loss=2.49e+3]        \n",
      "Epoch 349: 100%|██████████| 1/1 [00:00<00:00, 65.53it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.128, valid_loss=2.49e+3]\n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121, valid_loss=2.49e+3]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=2.49e+3]          \n",
      "Epoch 363: 100%|██████████| 1/1 [00:00<00:00, 65.36it/s, v_num=0, train_loss_step=0.088, train_loss_epoch=0.088, valid_loss=2.49e+3]  \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151, valid_loss=2.49e+3]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767, valid_loss=2.49e+3]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0925, train_loss_epoch=0.0925, valid_loss=2.49e+3]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0888, train_loss_epoch=0.0888, valid_loss=2.49e+3]        \n",
      "Epoch 386: 100%|██████████| 1/1 [00:00<00:00, 73.11it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.49e+3]  \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.49e+3]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0829, train_loss_epoch=0.0829, valid_loss=2.49e+3]        \n",
      "Epoch 395: 100%|██████████| 1/1 [00:00<00:00, 71.74it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092, valid_loss=2.49e+3]  \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092, valid_loss=2.49e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:48,251\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 63.96it/s, v_num=0, train_loss_step=0.0753, train_loss_epoch=0.0962, valid_loss=2.49e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 132.02it/s]\u001b[A\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0903, train_loss_epoch=0.0903, valid_loss=2.1e+3]         \n",
      "Epoch 407: 100%|██████████| 1/1 [00:00<00:00, 68.53it/s, v_num=0, train_loss_step=0.0719, train_loss_epoch=0.0719, valid_loss=2.1e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 76.40it/s, v_num=0, train_loss_step=0.574]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.574, train_loss_epoch=0.574]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.467, train_loss_epoch=0.467]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.268, train_loss_epoch=0.268]         \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.371, train_loss_epoch=0.371]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]         \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 96.09it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196] \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]        \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 95.75it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260] \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260]        \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 102.92it/s, v_num=0, train_loss_step=0.260, train_loss_epoch=0.260]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.256, train_loss_epoch=0.256]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 100.87it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.218]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.59it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.09e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155, valid_loss=2.09e+3]         \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.09e+3]         \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0965, train_loss_epoch=0.0965, valid_loss=2.09e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=2.09e+3]           \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=2.09e+3]         \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 96.45it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.09e+3] \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.09e+3]        \n",
      "Epoch 145: 100%|██████████| 1/1 [00:00<00:00, 95.31it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.09e+3] \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.09e+3]        \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 94.42it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137, valid_loss=2.09e+3] \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137, valid_loss=2.09e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.09e+3]         \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.09e+3]           \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0902, train_loss_epoch=0.0902, valid_loss=2.09e+3]         \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0845, train_loss_epoch=0.0845, valid_loss=2.09e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.09e+3]           \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717, valid_loss=2.09e+3]         \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717, valid_loss=2.09e+3]\n",
      "Epoch 198: 100%|██████████| 1/1 [00:00<00:00, 96.05it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0922, valid_loss=2.09e+3] \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0922, valid_loss=2.09e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 101.66it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0922, valid_loss=2.09e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 205.52it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 54.40it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0922, valid_loss=2.44e+3] \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0715, valid_loss=2.44e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0631, train_loss_epoch=0.0631, valid_loss=2.44e+3]         \n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 100.64it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.0631, valid_loss=2.44e+3] \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150, valid_loss=2.44e+3]          \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137, valid_loss=2.44e+3]           \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0688, train_loss_epoch=0.0688, valid_loss=2.44e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0592, train_loss_epoch=0.0592, valid_loss=2.44e+3]         \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.058, train_loss_epoch=0.058, valid_loss=2.44e+3]           \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0814, train_loss_epoch=0.0814, valid_loss=2.44e+3]         \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738, valid_loss=2.44e+3]         \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738, valid_loss=2.44e+3]\n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.44e+3]           \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.44e+3]\n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.44e+3]         \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0723, train_loss_epoch=0.0723, valid_loss=2.44e+3]         \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0466, train_loss_epoch=0.0466, valid_loss=2.44e+3]         \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597, valid_loss=2.44e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 100.01it/s, v_num=0, train_loss_step=0.0513, train_loss_epoch=0.0665, valid_loss=2.44e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.87it/s]\u001b[A\n",
      "Epoch 302: 100%|██████████| 1/1 [00:00<00:00, 95.22it/s, v_num=0, train_loss_step=0.0541, train_loss_epoch=0.0541, valid_loss=2.04e+3] \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0541, train_loss_epoch=0.0541, valid_loss=2.04e+3]        \n",
      "Epoch 303: 100%|██████████| 1/1 [00:00<00:00, 100.04it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0541, valid_loss=2.04e+3]\n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0535, valid_loss=2.04e+3]         \n",
      "Epoch 304: 100%|██████████| 1/1 [00:00<00:00, 102.97it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0535, valid_loss=2.04e+3]\n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584, valid_loss=2.04e+3]         \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=2.04e+3]         \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.04e+3]         \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0469, train_loss_epoch=0.0469, valid_loss=2.04e+3]         \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0533, train_loss_epoch=0.0533, valid_loss=2.04e+3]         \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0822, train_loss_epoch=0.0822, valid_loss=2.04e+3]         \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0615, train_loss_epoch=0.0615, valid_loss=2.04e+3]         \n",
      "Epoch 374: 100%|██████████| 1/1 [00:00<00:00, 88.43it/s, v_num=0, train_loss_step=0.050, train_loss_epoch=0.050, valid_loss=2.04e+3]   \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.050, train_loss_epoch=0.050, valid_loss=2.04e+3]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0654, train_loss_epoch=0.0654, valid_loss=2.04e+3]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.041, train_loss_epoch=0.041, valid_loss=2.04e+3]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:52,834\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393: 100%|██████████| 1/1 [00:00<00:00, 86.82it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584, valid_loss=2.04e+3] \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.598, train_loss_epoch=0.598]        \n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.331, train_loss_epoch=0.331]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]       \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.236, train_loss_epoch=0.236]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]        \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 79.23it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]          \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0937, train_loss_epoch=0.0937]        \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00, 81.69it/s, v_num=0, train_loss_step=0.0775, train_loss_epoch=0.0775]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]          \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.086, train_loss_epoch=0.086]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0929, train_loss_epoch=0.0929]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0571, train_loss_epoch=0.0571]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]          \n",
      "Epoch 92: 100%|██████████| 1/1 [00:00<00:00, 78.33it/s, v_num=0, train_loss_step=0.0924, train_loss_epoch=0.117]\n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0924, train_loss_epoch=0.0924]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0785, train_loss_epoch=0.0785]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 80.96it/s, v_num=0, train_loss_step=0.0771, train_loss_epoch=0.0807]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 157.89it/s]\u001b[A\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=1.35e+3]          \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 78.42it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=1.35e+3]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.066, train_loss_epoch=0.066, valid_loss=1.35e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0699, train_loss_epoch=0.0699, valid_loss=1.35e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=1.35e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337, valid_loss=1.35e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=1.35e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0564, valid_loss=1.35e+3]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=1.35e+3]          \n",
      "Epoch 129: 100%|██████████| 1/1 [00:00<00:00, 75.06it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=1.35e+3]\n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=1.35e+3]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681, valid_loss=1.35e+3]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=1.35e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.35e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0516, train_loss_epoch=0.0516, valid_loss=1.35e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0528, train_loss_epoch=0.0528, valid_loss=1.35e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651, valid_loss=1.35e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496, valid_loss=1.35e+3]        \n",
      "Epoch 158: 100%|██████████| 1/1 [00:00<00:00, 79.70it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496, valid_loss=1.35e+3]\n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.055, train_loss_epoch=0.055, valid_loss=1.35e+3]          \n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00, 75.40it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=1.35e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=1.35e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0455, train_loss_epoch=0.0455, valid_loss=1.35e+3]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0302, train_loss_epoch=0.0302, valid_loss=1.35e+3]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=1.35e+3]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338, valid_loss=1.35e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0463, train_loss_epoch=0.0463, valid_loss=1.35e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=1.35e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.070, train_loss_epoch=0.070, valid_loss=1.35e+3]          \n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 79.59it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0376, valid_loss=1.35e+3]\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0363, valid_loss=1.35e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=1.35e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 82.13it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0285, valid_loss=1.35e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 159.58it/s]\u001b[A\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=2.43e+3]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=2.43e+3]        \n",
      "Epoch 212: 100%|██████████| 1/1 [00:00<00:00, 73.21it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.43e+3]\n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=2.43e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0371, train_loss_epoch=0.0371, valid_loss=2.43e+3]        \n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 71.19it/s, v_num=0, train_loss_step=0.0375, train_loss_epoch=0.0371, valid_loss=2.43e+3]\n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0375, train_loss_epoch=0.0375, valid_loss=2.43e+3]        \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0181, valid_loss=2.43e+3]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=2.43e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=2.43e+3]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282, valid_loss=2.43e+3]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=2.43e+3]        \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=2.43e+3]        \n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00, 78.98it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0219, valid_loss=2.43e+3]\n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=2.43e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 72.29it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=2.43e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=2.43e+3]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=2.43e+3]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=2.43e+3]          \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642, valid_loss=2.43e+3]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.049, train_loss_epoch=0.049, valid_loss=2.43e+3]          \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316, valid_loss=2.43e+3]        \n",
      "Epoch 269: 100%|██████████| 1/1 [00:00<00:00, 78.74it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0316, valid_loss=2.43e+3]\n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=2.43e+3]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=2.43e+3]          \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=2.43e+3]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0419, train_loss_epoch=0.0419, valid_loss=2.43e+3]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=2.43e+3]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.031, train_loss_epoch=0.031, valid_loss=2.43e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:47:57,434\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 80.43it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.015, valid_loss=2.43e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.19it/s]\u001b[A\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=2.38e+3]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=2.38e+3]        \n",
      "Epoch 302: 100%|██████████| 1/1 [00:00<00:00, 76.40it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=2.38e+3]\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=2.38e+3]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=2.38e+3]          \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=2.38e+3]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.38e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315: 100%|██████████| 1/1 [00:00<00:00, 73.06it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=2.38e+3]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.364, train_loss_epoch=0.364]         \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 104.67it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.270]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.249, train_loss_epoch=0.249]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244]         \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 100.09it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]         \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]         \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 100.98it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]         \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 101.55it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]  \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]           \n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 101.42it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]  \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0974, train_loss_epoch=0.0974]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0935, train_loss_epoch=0.0935]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 108.35it/s, v_num=0, train_loss_step=0.0736, train_loss_epoch=0.108] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.57it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0909, train_loss_epoch=0.0909, valid_loss=2.44e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767, valid_loss=2.44e+3]         \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 100.16it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.44e+3]  \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.44e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0957, train_loss_epoch=0.0957, valid_loss=2.44e+3]         \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 100.28it/s, v_num=0, train_loss_step=0.0846, train_loss_epoch=0.0846, valid_loss=2.44e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0846, train_loss_epoch=0.0846, valid_loss=2.44e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0673, train_loss_epoch=0.0673, valid_loss=2.44e+3]         \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0728, train_loss_epoch=0.0728, valid_loss=2.44e+3]         \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 106.92it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545, valid_loss=2.44e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0624, valid_loss=2.44e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.44e+3]           \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 101.36it/s, v_num=0, train_loss_step=0.088, train_loss_epoch=0.088, valid_loss=2.44e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.088, train_loss_epoch=0.088, valid_loss=2.44e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0999, train_loss_epoch=0.0999, valid_loss=2.44e+3]         \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 100.46it/s, v_num=0, train_loss_step=0.0617, train_loss_epoch=0.0617, valid_loss=2.44e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0617, train_loss_epoch=0.0617, valid_loss=2.44e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0623, train_loss_epoch=0.0623, valid_loss=2.44e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0826, train_loss_epoch=0.0826, valid_loss=2.44e+3]         \n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 105.46it/s, v_num=0, train_loss_step=0.0686, train_loss_epoch=0.065, valid_loss=2.44e+3] \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0686, train_loss_epoch=0.0686, valid_loss=2.44e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0626, train_loss_epoch=0.0626, valid_loss=2.44e+3]         \n",
      "Epoch 138: 100%|██████████| 1/1 [00:00<00:00, 97.63it/s, v_num=0, train_loss_step=0.0524, train_loss_epoch=0.0626, valid_loss=2.44e+3] \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0524, train_loss_epoch=0.0524, valid_loss=2.44e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.055, train_loss_epoch=0.055, valid_loss=2.44e+3]           \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 100.62it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553, valid_loss=2.44e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553, valid_loss=2.44e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.048, train_loss_epoch=0.048, valid_loss=2.44e+3]           \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0959, train_loss_epoch=0.0959, valid_loss=2.44e+3]         \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0959, train_loss_epoch=0.0959, valid_loss=2.44e+3]\n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0647, train_loss_epoch=0.0647, valid_loss=2.44e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597, valid_loss=2.44e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=2.44e+3]           \n",
      "Epoch 155: 100%|██████████| 1/1 [00:00<00:00, 99.60it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=2.44e+3]\n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=2.44e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=2.44e+3]         \n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00, 101.88it/s, v_num=0, train_loss_step=0.0628, train_loss_epoch=0.0628, valid_loss=2.44e+3]\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0628, train_loss_epoch=0.0628, valid_loss=2.44e+3]         \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0464, train_loss_epoch=0.0464, valid_loss=2.44e+3]         \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 101.56it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751, valid_loss=2.44e+3]\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751, valid_loss=2.44e+3]         \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0678, train_loss_epoch=0.0678, valid_loss=2.44e+3]         \n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00, 101.01it/s, v_num=0, train_loss_step=0.0699, train_loss_epoch=0.0699, valid_loss=2.44e+3]\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0699, train_loss_epoch=0.0699, valid_loss=2.44e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0516, train_loss_epoch=0.0516, valid_loss=2.44e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.067, train_loss_epoch=0.067, valid_loss=2.44e+3]           \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.067, train_loss_epoch=0.067, valid_loss=2.44e+3]\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483, valid_loss=2.44e+3]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 100.89it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=2.44e+3]\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=2.44e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=2.44e+3]         \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 100.75it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=2.44e+3]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=2.44e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.045, train_loss_epoch=0.045, valid_loss=2.44e+3]           \n",
      "Epoch 179: 100%|██████████| 1/1 [00:00<00:00, 101.30it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=2.44e+3]\n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=2.44e+3]         \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0369, valid_loss=2.44e+3]         \n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 104.75it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0369, valid_loss=2.44e+3]\n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 100.63it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=2.44e+3]\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=2.44e+3]         \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0347, train_loss_epoch=0.0347, valid_loss=2.44e+3]         \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 106.38it/s, v_num=0, train_loss_step=0.034, train_loss_epoch=0.0347, valid_loss=2.44e+3] \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.034, train_loss_epoch=0.034, valid_loss=2.44e+3]          \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=2.44e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0588, train_loss_epoch=0.0588, valid_loss=2.44e+3]         \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=2.44e+3]         \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=2.44e+3]\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 106.61it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0328, valid_loss=2.44e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 186.71it/s]\u001b[A\n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0594, train_loss_epoch=0.0594, valid_loss=2.37e+3]         \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=2.37e+3]           \n",
      "Epoch 207: 100%|██████████| 1/1 [00:00<00:00, 100.66it/s, v_num=0, train_loss_step=0.0512, train_loss_epoch=0.0512, valid_loss=2.37e+3]\n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0512, train_loss_epoch=0.0512, valid_loss=2.37e+3]         \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.37e+3]         \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 100.14it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=2.37e+3]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=2.37e+3]         \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0645, train_loss_epoch=0.0645, valid_loss=2.37e+3]         \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 98.90it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=2.37e+3] \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=2.37e+3]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=2.37e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551, valid_loss=2.37e+3]         \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=2.37e+3]         \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=2.37e+3]         \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.37e+3]         \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=2.37e+3]         \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=2.37e+3]          \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0466, train_loss_epoch=0.0466, valid_loss=2.37e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0398, valid_loss=2.37e+3]         \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=2.37e+3]         \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282, valid_loss=2.37e+3]         \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.37e+3]         \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 106.69it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0319, valid_loss=2.37e+3]\n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0343, valid_loss=2.37e+3]         \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0357, train_loss_epoch=0.0357, valid_loss=2.37e+3]         \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 100.51it/s, v_num=0, train_loss_step=0.0406, train_loss_epoch=0.0406, valid_loss=2.37e+3]\n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0406, train_loss_epoch=0.0406, valid_loss=2.37e+3]         \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0313, train_loss_epoch=0.0313, valid_loss=2.37e+3]         \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=2.37e+3]         \n",
      "Epoch 275: 100%|██████████| 1/1 [00:00<00:00, 101.62it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=2.37e+3]\n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=2.37e+3]         \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=2.37e+3]         \n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 94.28it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.37e+3] \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.37e+3]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=2.37e+3]         \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=2.37e+3]         \n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00, 102.38it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231, valid_loss=2.37e+3]\n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231, valid_loss=2.37e+3]         \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=2.37e+3]         \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=2.37e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 109.95it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0291, valid_loss=2.37e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 195.68it/s]\u001b[A\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=2.22e+3]         \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=2.22e+3]         \n",
      "Epoch 303: 100%|██████████| 1/1 [00:00<00:00, 103.34it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=2.22e+3]\n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=2.22e+3]         \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=2.22e+3]           \n",
      "Epoch 305: 100%|██████████| 1/1 [00:00<00:00, 101.33it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=2.22e+3]\n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=2.22e+3]         \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506, valid_loss=2.22e+3]         \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=2.22e+3]         \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=2.22e+3]\n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306, valid_loss=2.22e+3]         \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=2.22e+3]         \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305, valid_loss=2.22e+3]         \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305, valid_loss=2.22e+3]\n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=2.22e+3]         \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=2.22e+3]         \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=2.22e+3]         \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=2.22e+3]         \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=2.22e+3]         \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=2.22e+3]         \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=2.22e+3]         \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=2.22e+3]         \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=2.22e+3]         \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.22e+3]         \n",
      "Epoch 368: 100%|██████████| 1/1 [00:00<00:00, 97.94it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=2.22e+3] \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=2.22e+3]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=2.22e+3]         \n",
      "Epoch 370: 100%|██████████| 1/1 [00:00<00:00, 97.16it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=2.22e+3] \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=2.22e+3]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=2.22e+3]           \n",
      "Epoch 372: 100%|██████████| 1/1 [00:00<00:00, 96.27it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.22e+3]\n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.22e+3]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.22e+3]         \n",
      "Epoch 383: 100%|██████████| 1/1 [00:00<00:00, 94.67it/s, v_num=0, train_loss_step=0.0393, train_loss_epoch=0.0393, valid_loss=2.22e+3] \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0393, train_loss_epoch=0.0393, valid_loss=2.22e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:01,869\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=2.22e+3]         \n",
      "Epoch 394: 100%|██████████| 1/1 [00:00<00:00, 96.71it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=2.22e+3] \n",
      "Epoch 394: 100%|██████████| 1/1 [00:00<00:00, 86.57it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=2.22e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]        \n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 73.00it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]        \n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 71.06it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0996, train_loss_epoch=0.0996]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0924, train_loss_epoch=0.0924]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.072, train_loss_epoch=0.072]          \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0661, train_loss_epoch=0.0661]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151]          \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]          \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0678, train_loss_epoch=0.0678]        \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 81.69it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496]\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0909, train_loss_epoch=0.0909]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0439, train_loss_epoch=0.0439]        \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 85.91it/s, v_num=0, train_loss_step=0.0439, train_loss_epoch=0.0439]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0434]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.034, train_loss_epoch=0.034]          \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 80.81it/s, v_num=0, train_loss_step=0.0605, train_loss_epoch=0.0501]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0605, train_loss_epoch=0.0605]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.0677]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 83.88it/s, v_num=0, train_loss_step=0.0406, train_loss_epoch=0.0677]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 147.46it/s]\u001b[A\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=2.32e+3]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=2.32e+3]        \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0435, train_loss_epoch=0.0435, valid_loss=2.32e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=2.32e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0276, train_loss_epoch=0.0276, valid_loss=2.32e+3]        \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 80.20it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306, valid_loss=2.32e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0306, train_loss_epoch=0.0306, valid_loss=2.32e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=2.32e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=2.32e+3]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=2.32e+3]        \n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 81.12it/s, v_num=0, train_loss_step=0.0348, train_loss_epoch=0.0348, valid_loss=2.32e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0348, train_loss_epoch=0.0348, valid_loss=2.32e+3]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=2.32e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=2.32e+3]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=2.32e+3]        \n",
      "Epoch 174: 100%|██████████| 1/1 [00:00<00:00, 85.40it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=2.32e+3]\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.025, valid_loss=2.32e+3]          \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=2.32e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:04,570\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=2.32e+3]          \n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 77.41it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0299, valid_loss=2.32e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.541, train_loss_epoch=0.541]         \n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 115.26it/s, v_num=0, train_loss_step=0.544, train_loss_epoch=0.544]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.544, train_loss_epoch=0.544]         \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.518, train_loss_epoch=0.518]         \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 109.59it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.500, train_loss_epoch=0.500]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.436, train_loss_epoch=0.436]         \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 118.38it/s, v_num=0, train_loss_step=0.553, train_loss_epoch=0.436]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.553, train_loss_epoch=0.553]         \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.392, train_loss_epoch=0.392]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.651, train_loss_epoch=0.651]         \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 120.14it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.651]\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.513, train_loss_epoch=0.513]         \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.566, train_loss_epoch=0.566]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:04,978\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 123.87it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.265]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 182.93it/s]\u001b[A\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 60.58it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.265, valid_loss=3.83e+3]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 42.26it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255, valid_loss=3.83e+3]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]        \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 87.81it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.150, train_loss_epoch=0.150]        \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 88.75it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240]        \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 91.29it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.240]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]        \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 93.79it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.158, train_loss_epoch=0.158]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0954, train_loss_epoch=0.0954]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0961, train_loss_epoch=0.0961]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0925, train_loss_epoch=0.0925]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]          \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0845, train_loss_epoch=0.0845]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0921, train_loss_epoch=0.0921]        \n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 87.75it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]  \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0992]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0879, train_loss_epoch=0.0879]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 91.83it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.0879] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 149.95it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 46.87it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.0879, valid_loss=2.2e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=2.2e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0938, train_loss_epoch=0.0938, valid_loss=2.2e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.2e+3]          \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0653, train_loss_epoch=0.0653, valid_loss=2.2e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0859, train_loss_epoch=0.0859, valid_loss=2.2e+3]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0812, train_loss_epoch=0.0812, valid_loss=2.2e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551, valid_loss=2.2e+3]        \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 82.32it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776, valid_loss=2.2e+3]\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776, valid_loss=2.2e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0655, train_loss_epoch=0.0655, valid_loss=2.2e+3]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0649, train_loss_epoch=0.0649, valid_loss=2.2e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=2.2e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0497, train_loss_epoch=0.0497, valid_loss=2.2e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0554, train_loss_epoch=0.0554, valid_loss=2.2e+3]        \n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 87.64it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=2.2e+3]  \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=2.2e+3]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684, valid_loss=2.2e+3]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0404, train_loss_epoch=0.0404, valid_loss=2.2e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0708, train_loss_epoch=0.0708, valid_loss=2.2e+3]        \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 93.31it/s, v_num=0, train_loss_step=0.0708, train_loss_epoch=0.0708, valid_loss=2.2e+3]\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0643, train_loss_epoch=0.0643, valid_loss=2.2e+3]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=2.2e+3]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0609, train_loss_epoch=0.0609, valid_loss=2.2e+3]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551, valid_loss=2.2e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0359, train_loss_epoch=0.0359, valid_loss=2.2e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0478, train_loss_epoch=0.0478, valid_loss=2.2e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 81.77it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0408, valid_loss=2.2e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.63it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 43.62it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0408, valid_loss=2.08e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687, valid_loss=2.08e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=2.08e+3]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=2.08e+3]          \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=2.08e+3]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0446, valid_loss=2.08e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.052, valid_loss=2.08e+3]          \n",
      "Epoch 235: 100%|██████████| 1/1 [00:00<00:00, 63.19it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033, valid_loss=2.08e+3]\n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.033, train_loss_epoch=0.033, valid_loss=2.08e+3]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0388, train_loss_epoch=0.0388, valid_loss=2.08e+3]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0343, valid_loss=2.08e+3]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0308, train_loss_epoch=0.0308, valid_loss=2.08e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=2.08e+3]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=2.08e+3]        \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 77.29it/s, v_num=0, train_loss_step=0.0485, train_loss_epoch=0.0485, valid_loss=2.08e+3]\n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0485, train_loss_epoch=0.0485, valid_loss=2.08e+3]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0453, train_loss_epoch=0.0453, valid_loss=2.08e+3]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.041, train_loss_epoch=0.041, valid_loss=2.08e+3]          \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=2.08e+3]        \n",
      "Epoch 280: 100%|██████████| 1/1 [00:00<00:00, 77.17it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.08e+3]\n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.08e+3]        \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=2.08e+3]        \n",
      "Epoch 290:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.08e+3]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.08e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 92.15it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0353, valid_loss=2.08e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 172.82it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 48.97it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0353, valid_loss=2.08e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0324, valid_loss=2.08e+3]        \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=2.08e+3]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575, valid_loss=2.08e+3]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0398, valid_loss=2.08e+3]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=2.08e+3]        \n",
      "Epoch 328: 100%|██████████| 1/1 [00:00<00:00, 84.82it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=2.08e+3]\n",
      "Epoch 329:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=2.08e+3]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.08e+3]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=2.08e+3]        \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328, valid_loss=2.08e+3]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=2.08e+3]        \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.08e+3]        \n",
      "Epoch 350: 100%|██████████| 1/1 [00:00<00:00, 91.92it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0314, valid_loss=2.08e+3]\n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0232, valid_loss=2.08e+3]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=2.08e+3]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=2.08e+3]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0324, valid_loss=2.08e+3]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=2.08e+3]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=2.08e+3]        \n",
      "Epoch 372: 100%|██████████| 1/1 [00:00<00:00, 87.52it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=2.08e+3]\n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=2.08e+3]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316, valid_loss=2.08e+3]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309, valid_loss=2.08e+3]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=2.08e+3]        \n",
      "Epoch 384: 100%|██████████| 1/1 [00:00<00:00, 88.59it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=2.08e+3]\n",
      "Epoch 385:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=2.08e+3]        \n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=2.08e+3]          \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=2.08e+3]          \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=2.08e+3]        \n",
      "Epoch 396: 100%|██████████| 1/1 [00:00<00:00, 91.00it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0188, valid_loss=2.08e+3]\n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.08e+3]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0173, train_loss_epoch=0.0173, valid_loss=2.08e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 93.04it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0203, valid_loss=2.08e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.87it/s]\u001b[A\n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=2.12e+3]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0302, train_loss_epoch=0.0302, valid_loss=2.12e+3]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.12e+3]        \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=2.12e+3]        \n",
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 93.91it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=2.12e+3]\n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=2.12e+3]          \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0379, train_loss_epoch=0.0379, valid_loss=2.12e+3]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0293, train_loss_epoch=0.0293, valid_loss=2.12e+3]        \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=2.12e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:10,817\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=2.12e+3]        \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=2.12e+3]          \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=2.12e+3]        \n",
      "Epoch 448: 100%|██████████| 1/1 [00:00<00:00, 82.34it/s, v_num=0, train_loss_step=0.0329, train_loss_epoch=0.0329, valid_loss=2.12e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.304, train_loss_epoch=0.304]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.372, train_loss_epoch=0.372]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.511, train_loss_epoch=0.511]        \n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 81.47it/s, v_num=0, train_loss_step=0.563, train_loss_epoch=0.563]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.563, train_loss_epoch=0.563]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.569, train_loss_epoch=0.569]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.418, train_loss_epoch=0.418]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.400, train_loss_epoch=0.400]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237]        \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 82.30it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202]        \n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 86.14it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 85.68it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.186]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 147.70it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=2.54e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=2.54e+3]        \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 80.15it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166, valid_loss=2.54e+3]\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166, valid_loss=2.54e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177, valid_loss=2.54e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.54e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.54e+3]\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.169, train_loss_epoch=0.169, valid_loss=2.54e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157, valid_loss=2.54e+3]        \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 80.40it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=2.54e+3]\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=2.54e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.54e+3]        \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 79.70it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=2.54e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=2.54e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140, valid_loss=2.54e+3]        \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 81.24it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.54e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.54e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=2.54e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=2.54e+3]        \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 81.50it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=2.54e+3]\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=2.54e+3]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.54e+3]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.54e+3]        \n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 85.66it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=2.54e+3]\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.54e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.54e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.54e+3]          \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.54e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 81.93it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.102, valid_loss=2.54e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.51it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 44.49it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.102, valid_loss=2.57e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122, valid_loss=2.57e+3]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.57e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0975, train_loss_epoch=0.0975, valid_loss=2.57e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=2.57e+3]          \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0985, train_loss_epoch=0.0985, valid_loss=2.57e+3]        \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0972, train_loss_epoch=0.0972, valid_loss=2.57e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.57e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:14,208\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122, valid_loss=2.57e+3]          \n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.57e+3]        \n",
      "Epoch 229: 100%|██████████| 1/1 [00:00<00:00, 84.33it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.57e+3]\n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=2.57e+3]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0833, train_loss_epoch=0.0833, valid_loss=2.57e+3]        \n",
      "Epoch 239: 100%|██████████| 1/1 [00:00<00:00, 81.87it/s, v_num=0, train_loss_step=0.0955, train_loss_epoch=0.0955, valid_loss=2.57e+3]\n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0955, train_loss_epoch=0.0955, valid_loss=2.57e+3]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0951, train_loss_epoch=0.0951, valid_loss=2.57e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242: 100%|██████████| 1/1 [00:00<00:00, 74.95it/s, v_num=0, train_loss_step=0.085, train_loss_epoch=0.085, valid_loss=2.57e+3]  \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242]         \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.238, train_loss_epoch=0.238]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.175, train_loss_epoch=0.175]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.293, train_loss_epoch=0.293]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 117.37it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]         \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 108.83it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]           \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0923, train_loss_epoch=0.0923]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0782, train_loss_epoch=0.0782]         \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 116.78it/s, v_num=0, train_loss_step=0.0782, train_loss_epoch=0.0782]\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0649, train_loss_epoch=0.0649]         \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.229, train_loss_epoch=0.229]           \n",
      "Epoch 77: 100%|██████████| 1/1 [00:00<00:00, 108.96it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.097, train_loss_epoch=0.097]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0786, train_loss_epoch=0.0786]         \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]           \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0735, train_loss_epoch=0.0735]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 117.42it/s, v_num=0, train_loss_step=0.0989, train_loss_epoch=0.0643]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 175.58it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=2.21e+3]           \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 108.14it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184, valid_loss=2.21e+3]\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184, valid_loss=2.21e+3]         \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=2.21e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.143, train_loss_epoch=0.143, valid_loss=2.21e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0654, train_loss_epoch=0.0654, valid_loss=2.21e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156, valid_loss=2.21e+3]           \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0926, train_loss_epoch=0.0926, valid_loss=2.21e+3]         \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 111.32it/s, v_num=0, train_loss_step=0.0697, train_loss_epoch=0.0926, valid_loss=2.21e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0697, train_loss_epoch=0.0697, valid_loss=2.21e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0676, train_loss_epoch=0.0676, valid_loss=2.21e+3]         \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 107.21it/s, v_num=0, train_loss_step=0.0822, train_loss_epoch=0.0822, valid_loss=2.21e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0822, train_loss_epoch=0.0822, valid_loss=2.21e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171, valid_loss=2.21e+3]           \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0818, train_loss_epoch=0.0818, valid_loss=2.21e+3]         \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 112.62it/s, v_num=0, train_loss_step=0.0707, train_loss_epoch=0.0818, valid_loss=2.21e+3]\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0707, train_loss_epoch=0.0707, valid_loss=2.21e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=2.21e+3]           \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=2.21e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=2.21e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338, valid_loss=2.21e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0838, train_loss_epoch=0.0838, valid_loss=2.21e+3]         \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 111.75it/s, v_num=0, train_loss_step=0.0614, train_loss_epoch=0.0838, valid_loss=2.21e+3]\n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 107.04it/s, v_num=0, train_loss_step=0.0614, train_loss_epoch=0.0614, valid_loss=2.21e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0614, train_loss_epoch=0.0614, valid_loss=2.21e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.063, train_loss_epoch=0.063, valid_loss=2.21e+3]           \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0555, train_loss_epoch=0.0555, valid_loss=2.21e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:16,042\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0656, train_loss_epoch=0.0656, valid_loss=2.21e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0659, train_loss_epoch=0.0659, valid_loss=2.21e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0841, train_loss_epoch=0.0841, valid_loss=2.21e+3]         \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 112.45it/s, v_num=0, train_loss_step=0.0482, train_loss_epoch=0.0841, valid_loss=2.21e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0482, train_loss_epoch=0.0482, valid_loss=2.21e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483, valid_loss=2.21e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.054, train_loss_epoch=0.054, valid_loss=2.21e+3]           \n",
      "Epoch 166: 100%|██████████| 1/1 [00:00<00:00, 102.14it/s, v_num=0, train_loss_step=0.0626, train_loss_epoch=0.0626, valid_loss=2.21e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.462, train_loss_epoch=0.462]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.512, train_loss_epoch=0.512]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.502, train_loss_epoch=0.502]        \n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 81.11it/s, v_num=0, train_loss_step=0.512, train_loss_epoch=0.424] \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.512, train_loss_epoch=0.512]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.479, train_loss_epoch=0.479]        \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 75.13it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.479]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.527, train_loss_epoch=0.527]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.460, train_loss_epoch=0.460]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.410, train_loss_epoch=0.410]         \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 110.39it/s, v_num=0, train_loss_step=0.384, train_loss_epoch=0.549]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.384, train_loss_epoch=0.384]         \n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00, 104.52it/s, v_num=0, train_loss_step=0.459, train_loss_epoch=0.459]\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.459, train_loss_epoch=0.459]         \n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 104.61it/s, v_num=0, train_loss_step=0.356, train_loss_epoch=0.356]\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.356, train_loss_epoch=0.356]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 94.56it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529] \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.529, train_loss_epoch=0.529]        \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 98.61it/s, v_num=0, train_loss_step=0.509, train_loss_epoch=0.529] \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.509, train_loss_epoch=0.509]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.393, train_loss_epoch=0.393]         \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.505, train_loss_epoch=0.505]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.496, train_loss_epoch=0.496]         \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.483, train_loss_epoch=0.483]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 98.66it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.471] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 150.12it/s]\u001b[A\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.395, train_loss_epoch=0.395, valid_loss=2.83e+3]         \n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 93.86it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242, valid_loss=2.83e+3] \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242, valid_loss=2.83e+3]        \n",
      "Epoch 102: 100%|██████████| 1/1 [00:00<00:00, 98.08it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.242, valid_loss=2.83e+3] \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246, valid_loss=2.83e+3]        \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.389, train_loss_epoch=0.389, valid_loss=2.83e+3]         \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225, valid_loss=2.83e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.237, train_loss_epoch=0.237, valid_loss=2.83e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.386, train_loss_epoch=0.386, valid_loss=2.83e+3]         \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 104.16it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381, valid_loss=2.83e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381, valid_loss=2.83e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223, valid_loss=2.83e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.348, train_loss_epoch=0.348, valid_loss=2.83e+3]         \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 96.78it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=2.83e+3] \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291, valid_loss=2.83e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288, valid_loss=2.83e+3]         \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 104.37it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201, valid_loss=2.83e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.201, train_loss_epoch=0.201, valid_loss=2.83e+3]         \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.224, train_loss_epoch=0.224, valid_loss=2.83e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.184, train_loss_epoch=0.184, valid_loss=2.83e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=2.83e+3]         \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191, valid_loss=2.83e+3]         \n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 112.35it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191, valid_loss=2.83e+3]\n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177, valid_loss=2.83e+3]         \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=2.83e+3]         \n",
      "Epoch 178: 100%|██████████| 1/1 [00:00<00:00, 104.77it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197, valid_loss=2.83e+3]\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197, valid_loss=2.83e+3]         \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196, valid_loss=2.83e+3]         \n",
      "Epoch 180: 100%|██████████| 1/1 [00:00<00:00, 105.12it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216, valid_loss=2.83e+3]\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216, valid_loss=2.83e+3]         \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.275, train_loss_epoch=0.275, valid_loss=2.83e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200, valid_loss=2.83e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:19,209\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 109.04it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.253, valid_loss=2.83e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 188.73it/s]\u001b[A\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=2.82e+3]         \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211, valid_loss=2.82e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177, valid_loss=2.82e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195, valid_loss=2.82e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 93.05it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=2.82e+3] \n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.403, train_loss_epoch=0.403]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394]        \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 90.10it/s, v_num=0, train_loss_step=0.536, train_loss_epoch=0.536]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.536, train_loss_epoch=0.536]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.361, train_loss_epoch=0.361]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.608, train_loss_epoch=0.608]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299]        \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 65.07it/s, v_num=0, train_loss_step=0.339, train_loss_epoch=0.339]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.339, train_loss_epoch=0.339]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.334, train_loss_epoch=0.334]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 97.21it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.423, train_loss_epoch=0.423]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.381, train_loss_epoch=0.381]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.465, train_loss_epoch=0.465]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]        \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 90.58it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.319, train_loss_epoch=0.319]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244]        \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 97.11it/s, v_num=0, train_loss_step=0.455, train_loss_epoch=0.455]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 90.24it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.349, train_loss_epoch=0.349]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247]        \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 94.61it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.247]\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.285, train_loss_epoch=0.285]        \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 94.85it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.313]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.309, train_loss_epoch=0.309]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 91.48it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.219]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.219]        \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 93.81it/s, v_num=0, train_loss_step=0.199, train_loss_epoch=0.263]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.98it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.267, train_loss_epoch=0.267, valid_loss=3.25e+3]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209, valid_loss=3.25e+3]        \n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 94.93it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209, valid_loss=3.25e+3]\n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181, valid_loss=3.25e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.336, train_loss_epoch=0.336, valid_loss=3.25e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180, valid_loss=3.25e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166, valid_loss=3.25e+3]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190, valid_loss=3.25e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:21,103\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192, valid_loss=3.25e+3]        \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274, valid_loss=3.25e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=3.25e+3]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180, valid_loss=3.25e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.204, train_loss_epoch=0.204, valid_loss=3.25e+3]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200, valid_loss=3.25e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 84.04it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127, valid_loss=3.25e+3]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.453, train_loss_epoch=0.453]         \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.394, train_loss_epoch=0.394]         \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]         \n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 117.51it/s, v_num=0, train_loss_step=0.370, train_loss_epoch=0.370]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.370, train_loss_epoch=0.370]         \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.281, train_loss_epoch=0.281]         \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.418, train_loss_epoch=0.418]         \n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 114.41it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.463, train_loss_epoch=0.463]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]        \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 117.36it/s, v_num=0, train_loss_step=0.425, train_loss_epoch=0.425]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:21,644\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 116.66it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.271, train_loss_epoch=0.271]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.222, train_loss_epoch=0.222]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 122.60it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.222]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 181.23it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 59.69it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.222, valid_loss=2.32e+3]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 43.10it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.210, valid_loss=2.32e+3]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246]        \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 69.38it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]        \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 64.46it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.126]\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]          \n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 68.71it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]          \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]          \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0915, train_loss_epoch=0.0915]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092]          \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092]\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]          \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0998, train_loss_epoch=0.0998]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]          \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0866, train_loss_epoch=0.0866]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0723, train_loss_epoch=0.0723]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0688, train_loss_epoch=0.0688]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 70.05it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.079] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 138.61it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0567, train_loss_epoch=0.0567, valid_loss=1.62e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0419, train_loss_epoch=0.0419, valid_loss=1.62e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0422, train_loss_epoch=0.0422, valid_loss=1.62e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0425, train_loss_epoch=0.0425, valid_loss=1.62e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0796, valid_loss=1.62e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0518, train_loss_epoch=0.0518, valid_loss=1.62e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.044, train_loss_epoch=0.044, valid_loss=1.62e+3]          \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 65.44it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=1.62e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=1.62e+3]        \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 67.85it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0561, valid_loss=1.62e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=1.62e+3]        \n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 68.71it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.0572, valid_loss=1.62e+3] \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.052, valid_loss=1.62e+3]         \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0557, valid_loss=1.62e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.62e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0541, train_loss_epoch=0.0541, valid_loss=1.62e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=1.62e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=1.62e+3]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459, valid_loss=1.62e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0458, train_loss_epoch=0.0458, valid_loss=1.62e+3]        \n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 67.69it/s, v_num=0, train_loss_step=0.047, train_loss_epoch=0.047, valid_loss=1.62e+3]  \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.047, train_loss_epoch=0.047, valid_loss=1.62e+3]        \n",
      "Epoch 174: 100%|██████████| 1/1 [00:00<00:00, 67.19it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356, valid_loss=1.62e+3]\n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356, valid_loss=1.62e+3]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 68.29it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0356, valid_loss=1.62e+3]\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0336, valid_loss=1.62e+3]        \n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 69.77it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0336, valid_loss=1.62e+3]\n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0276, train_loss_epoch=0.0276, valid_loss=1.62e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=1.62e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.62e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444, valid_loss=1.62e+3]        \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 66.89it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.62e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.62e+3]        \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 68.20it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.0253, valid_loss=1.62e+3] \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.62e+3]         \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 70.02it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.62e+3]\n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0326, train_loss_epoch=0.0326, valid_loss=1.62e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 69.90it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0262, valid_loss=1.62e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.86it/s]\u001b[A\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=1.88e+3]        \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=1.88e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=1.88e+3]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.88e+3]        \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=1.88e+3]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.88e+3]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0171, train_loss_epoch=0.0171, valid_loss=1.88e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=1.88e+3]          \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0329, train_loss_epoch=0.0329, valid_loss=1.88e+3]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=1.88e+3]        \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.88e+3]        \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 66.34it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=1.88e+3]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=1.88e+3]        \n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00, 64.59it/s, v_num=0, train_loss_step=0.0287, train_loss_epoch=0.0287, valid_loss=1.88e+3]\n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337, valid_loss=1.88e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 68.85it/s, v_num=0, train_loss_step=0.0337, train_loss_epoch=0.0337, valid_loss=1.88e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=1.88e+3]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424, valid_loss=1.88e+3]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0474, train_loss_epoch=0.0474, valid_loss=1.88e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.88e+3]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 64.13it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.88e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.88e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:26,260\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=1.88e+3]        \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 63.46it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.88e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.441, train_loss_epoch=0.441]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.217, train_loss_epoch=0.217]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.549, train_loss_epoch=0.549]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.549, train_loss_epoch=0.549]\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.228, train_loss_epoch=0.228]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.246, train_loss_epoch=0.246]         \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 91.52it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]        \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 91.92it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]          \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]        \n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 93.54it/s, v_num=0, train_loss_step=0.0882, train_loss_epoch=0.135]\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0882, train_loss_epoch=0.0882]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]          \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]          \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 97.43it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.112]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.99it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 49.43it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.112, valid_loss=2.14e+3]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 38.05it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154, valid_loss=2.14e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154, valid_loss=2.14e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125, valid_loss=2.14e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.14e+3]          \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 89.26it/s, v_num=0, train_loss_step=0.0804, train_loss_epoch=0.0804, valid_loss=2.14e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0804, train_loss_epoch=0.0804, valid_loss=2.14e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0779, train_loss_epoch=0.0779, valid_loss=2.14e+3]        \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 96.73it/s, v_num=0, train_loss_step=0.0779, train_loss_epoch=0.0779, valid_loss=2.14e+3]\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144, valid_loss=2.14e+3]          \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0779, train_loss_epoch=0.0779, valid_loss=2.14e+3]        \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 91.79it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652, valid_loss=2.14e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652, valid_loss=2.14e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0861, train_loss_epoch=0.0861, valid_loss=2.14e+3]        \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 88.32it/s, v_num=0, train_loss_step=0.0578, train_loss_epoch=0.0844, valid_loss=2.14e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0578, train_loss_epoch=0.0578, valid_loss=2.14e+3]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=2.14e+3]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0507, train_loss_epoch=0.0507, valid_loss=2.14e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0507, train_loss_epoch=0.0507, valid_loss=2.14e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=2.14e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0618, train_loss_epoch=0.0618, valid_loss=2.14e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0846, train_loss_epoch=0.0846, valid_loss=2.14e+3]        \n",
      "Epoch 161: 100%|██████████| 1/1 [00:00<00:00, 78.69it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=2.14e+3]  \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=2.14e+3]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0832, train_loss_epoch=0.0832, valid_loss=2.14e+3]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787, valid_loss=2.14e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=2.14e+3]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0754, train_loss_epoch=0.0754, valid_loss=2.14e+3]        \n",
      "Epoch 180: 100%|██████████| 1/1 [00:00<00:00, 82.19it/s, v_num=0, train_loss_step=0.0649, train_loss_epoch=0.0754, valid_loss=2.14e+3]\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0649, train_loss_epoch=0.0649, valid_loss=2.14e+3]        \n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 77.56it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651, valid_loss=2.14e+3]\n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651, valid_loss=2.14e+3]        \n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00, 78.29it/s, v_num=0, train_loss_step=0.068, train_loss_epoch=0.068, valid_loss=2.14e+3]  \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.068, train_loss_epoch=0.068, valid_loss=2.14e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0594, train_loss_epoch=0.0594, valid_loss=2.14e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0658, train_loss_epoch=0.0658, valid_loss=2.14e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0577, train_loss_epoch=0.0577, valid_loss=2.14e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 79.92it/s, v_num=0, train_loss_step=0.0894, train_loss_epoch=0.0565, valid_loss=2.14e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.42it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0894, train_loss_epoch=0.0894, valid_loss=2.16e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0543, train_loss_epoch=0.0543, valid_loss=2.16e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0726, train_loss_epoch=0.0726, valid_loss=2.16e+3]        \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 80.48it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595, valid_loss=2.16e+3]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595, valid_loss=2.16e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.16e+3]          \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0639, train_loss_epoch=0.0639, valid_loss=2.16e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0412, train_loss_epoch=0.0412, valid_loss=2.16e+3]        \n",
      "Epoch 228: 100%|██████████| 1/1 [00:00<00:00, 91.77it/s, v_num=0, train_loss_step=0.0904, train_loss_epoch=0.0904, valid_loss=2.16e+3]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0904, train_loss_epoch=0.0904, valid_loss=2.16e+3]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585, valid_loss=2.16e+3]        \n",
      "Epoch 239: 100%|██████████| 1/1 [00:00<00:00, 95.45it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.0615, valid_loss=2.16e+3] \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=2.16e+3]         \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.060, train_loss_epoch=0.060, valid_loss=2.16e+3]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0326, train_loss_epoch=0.0326, valid_loss=2.16e+3]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=2.16e+3]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488, valid_loss=2.16e+3]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0564, valid_loss=2.16e+3]        \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=2.16e+3]        \n",
      "Epoch 285: 100%|██████████| 1/1 [00:00<00:00, 88.18it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=2.16e+3]\n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.057, train_loss_epoch=0.057, valid_loss=2.16e+3]          \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 82.62it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=2.16e+3]\n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=2.16e+3]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0492, train_loss_epoch=0.0492, valid_loss=2.16e+3]        \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 87.83it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=2.16e+3]\n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487, valid_loss=2.16e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0478, train_loss_epoch=0.0478, valid_loss=2.16e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 83.33it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0512, valid_loss=2.16e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 157.17it/s]\u001b[A\n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=1.93e+3]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328, valid_loss=1.93e+3]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447, valid_loss=1.93e+3]        \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0308, train_loss_epoch=0.0308, valid_loss=1.93e+3]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0448, train_loss_epoch=0.0448, valid_loss=1.93e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=1.93e+3]        \n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 82.94it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=1.93e+3]\n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=1.93e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=1.93e+3]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0336, valid_loss=1.93e+3]        \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0328, train_loss_epoch=0.0328, valid_loss=1.93e+3]        \n",
      "Epoch 360: 100%|██████████| 1/1 [00:00<00:00, 82.63it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.93e+3]  \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.93e+3]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=1.93e+3]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0347, train_loss_epoch=0.0347, valid_loss=1.93e+3]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=1.93e+3]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.93e+3]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0311, train_loss_epoch=0.0311, valid_loss=1.93e+3]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=1.93e+3]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.93e+3]        \n",
      "Epoch 390: 100%|██████████| 1/1 [00:00<00:00, 96.27it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0253, valid_loss=1.93e+3]\n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=1.93e+3]        \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305, valid_loss=1.93e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 96.86it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0311, valid_loss=1.93e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.89it/s]\u001b[A\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0274, valid_loss=1.96e+3]        \n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0186, train_loss_epoch=0.0186, valid_loss=1.96e+3]        \n",
      "Epoch 410: 100%|██████████| 1/1 [00:00<00:00, 96.33it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0296, valid_loss=1.96e+3] \n",
      "Epoch 411:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=1.96e+3]        \n",
      "Epoch 412:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=1.96e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:31,727\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 85.35it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=1.96e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 120.72it/s, v_num=0, train_loss_step=0.273, train_loss_epoch=0.273]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]         \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 116.40it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.215]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.316, train_loss_epoch=0.316]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.233, train_loss_epoch=0.233]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.303, train_loss_epoch=0.303]         \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 115.41it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142]\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.203, train_loss_epoch=0.203]         \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.173, train_loss_epoch=0.173]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]         \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 124.06it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]         \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 115.21it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]         \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 114.52it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]         \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 115.28it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 66: 100%|██████████| 1/1 [00:00<00:00, 111.87it/s, v_num=0, train_loss_step=0.098, train_loss_epoch=0.098]\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.098, train_loss_epoch=0.098]         \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0901, train_loss_epoch=0.0901]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0901, train_loss_epoch=0.0901]\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0926, train_loss_epoch=0.0926]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100]           \n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 113.71it/s, v_num=0, train_loss_step=0.0772, train_loss_epoch=0.0772]\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0772, train_loss_epoch=0.0772]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0841, train_loss_epoch=0.0841]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 102.57it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]  \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.198, train_loss_epoch=0.198]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]         \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 102.85it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]         \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 103.18it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 81: 100%|██████████| 1/1 [00:00<00:00, 101.20it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 102.76it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]         \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 106.42it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.101]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]         \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 106.28it/s, v_num=0, train_loss_step=0.0895, train_loss_epoch=0.114]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0895, train_loss_epoch=0.0895]         \n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 108.05it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.0895] \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]          \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 106.84it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.110]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]         \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 107.10it/s, v_num=0, train_loss_step=0.0919, train_loss_epoch=0.108]\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0919, train_loss_epoch=0.0919]         \n",
      "Epoch 88: 100%|██████████| 1/1 [00:00<00:00, 107.48it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.0919] \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]          \n",
      "Epoch 89: 100%|██████████| 1/1 [00:00<00:00, 107.65it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.112]\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102]         \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 108.03it/s, v_num=0, train_loss_step=0.0957, train_loss_epoch=0.102]\n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 111.92it/s, v_num=0, train_loss_step=0.0957, train_loss_epoch=0.0957]\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0974, train_loss_epoch=0.0974]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 108.34it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.0778] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.80it/s]\u001b[A\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0853, train_loss_epoch=0.0853, valid_loss=2.34e+3]         \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0761, train_loss_epoch=0.0761, valid_loss=2.34e+3]         \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0878, train_loss_epoch=0.0878, valid_loss=2.34e+3]         \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 118.29it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.0878, valid_loss=2.34e+3] \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.132, valid_loss=2.34e+3]          \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.34e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0632, train_loss_epoch=0.0632, valid_loss=2.34e+3]         \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 118.24it/s, v_num=0, train_loss_step=0.0622, train_loss_epoch=0.0632, valid_loss=2.34e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0622, train_loss_epoch=0.0622, valid_loss=2.34e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.34e+3]           \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.34e+3]         \n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00, 117.00it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.109, valid_loss=2.34e+3]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.34e+3]         \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 120.52it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.34e+3]\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0808, train_loss_epoch=0.0808, valid_loss=2.34e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135, valid_loss=2.34e+3]           \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 110.31it/s, v_num=0, train_loss_step=0.0601, train_loss_epoch=0.0601, valid_loss=2.34e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0601, train_loss_epoch=0.0601, valid_loss=2.34e+3]         \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597, valid_loss=2.34e+3]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0576, train_loss_epoch=0.0576, valid_loss=2.34e+3]         \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 111.62it/s, v_num=0, train_loss_step=0.0972, train_loss_epoch=0.0972, valid_loss=2.34e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0972, train_loss_epoch=0.0972, valid_loss=2.34e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.34e+3]           \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0861, train_loss_epoch=0.0861, valid_loss=2.34e+3]         \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 112.18it/s, v_num=0, train_loss_step=0.0921, train_loss_epoch=0.0921, valid_loss=2.34e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0921, train_loss_epoch=0.0921, valid_loss=2.34e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0976, train_loss_epoch=0.0976, valid_loss=2.34e+3]         \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0711, train_loss_epoch=0.0711, valid_loss=2.34e+3]         \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 117.38it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.0711, valid_loss=2.34e+3] \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=2.34e+3]          \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644, valid_loss=2.34e+3]         \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0806, train_loss_epoch=0.0806, valid_loss=2.34e+3]         \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 117.36it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.0806, valid_loss=2.34e+3] \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.34e+3]          \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=2.34e+3]         \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0763, train_loss_epoch=0.0763, valid_loss=2.34e+3]         \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 122.88it/s, v_num=0, train_loss_step=0.0698, train_loss_epoch=0.0698, valid_loss=2.34e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.051, train_loss_epoch=0.051, valid_loss=2.34e+3]           \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0567, train_loss_epoch=0.0567, valid_loss=2.34e+3]         \n",
      "Epoch 136: 100%|██████████| 1/1 [00:00<00:00, 111.34it/s, v_num=0, train_loss_step=0.0404, train_loss_epoch=0.0404, valid_loss=2.34e+3]\n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0404, train_loss_epoch=0.0404, valid_loss=2.34e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0387, train_loss_epoch=0.0387, valid_loss=2.34e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=2.34e+3]         \n",
      "Epoch 139: 100%|██████████| 1/1 [00:00<00:00, 111.42it/s, v_num=0, train_loss_step=0.0601, train_loss_epoch=0.0601, valid_loss=2.34e+3]\n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0601, train_loss_epoch=0.0601, valid_loss=2.34e+3]         \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0659, train_loss_epoch=0.0659, valid_loss=2.34e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.054, train_loss_epoch=0.054, valid_loss=2.34e+3]           \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 112.20it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481, valid_loss=2.34e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481, valid_loss=2.34e+3]         \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0397, valid_loss=2.34e+3]         \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=2.34e+3]         \n",
      "Epoch 145: 100%|██████████| 1/1 [00:00<00:00, 113.15it/s, v_num=0, train_loss_step=0.0724, train_loss_epoch=0.0724, valid_loss=2.34e+3]\n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0724, train_loss_epoch=0.0724, valid_loss=2.34e+3]         \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0587, train_loss_epoch=0.0587, valid_loss=2.34e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0889, train_loss_epoch=0.0889, valid_loss=2.34e+3]         \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 117.48it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0889, valid_loss=2.34e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687, valid_loss=2.34e+3]         \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.34e+3]           \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 112.07it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=2.34e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=2.34e+3]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0744, train_loss_epoch=0.0744, valid_loss=2.34e+3]         \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.066, train_loss_epoch=0.066, valid_loss=2.34e+3]           \n",
      "Epoch 153: 100%|██████████| 1/1 [00:00<00:00, 109.75it/s, v_num=0, train_loss_step=0.0813, train_loss_epoch=0.0813, valid_loss=2.34e+3]\n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 122.63it/s, v_num=0, train_loss_step=0.0813, train_loss_epoch=0.0813, valid_loss=2.34e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0775, train_loss_epoch=0.0775, valid_loss=2.34e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.34e+3]           \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 116.59it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.34e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0573, train_loss_epoch=0.0573, valid_loss=2.34e+3]         \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0596, train_loss_epoch=0.0596, valid_loss=2.34e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0568, train_loss_epoch=0.0568, valid_loss=2.34e+3]         \n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00, 123.54it/s, v_num=0, train_loss_step=0.0568, train_loss_epoch=0.0568, valid_loss=2.34e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0777, train_loss_epoch=0.0777, valid_loss=2.34e+3]         \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0682, train_loss_epoch=0.0682, valid_loss=2.34e+3]         \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.34e+3]           \n",
      "Epoch 162: 100%|██████████| 1/1 [00:00<00:00, 120.12it/s, v_num=0, train_loss_step=0.0517, train_loss_epoch=0.116, valid_loss=2.34e+3]\n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0517, train_loss_epoch=0.0517, valid_loss=2.34e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787, valid_loss=2.34e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.082, train_loss_epoch=0.082, valid_loss=2.34e+3]           \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 114.26it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.0677, valid_loss=2.34e+3]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.0677, valid_loss=2.34e+3]         \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0593, train_loss_epoch=0.0593, valid_loss=2.34e+3]         \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0596, train_loss_epoch=0.0596, valid_loss=2.34e+3]         \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 114.23it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=2.34e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=2.34e+3]         \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0674, train_loss_epoch=0.0674, valid_loss=2.34e+3]         \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0688, train_loss_epoch=0.0688, valid_loss=2.34e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:35,481\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179: 100%|██████████| 1/1 [00:00<00:00, 124.40it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.34e+3]  \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0564, train_loss_epoch=0.0564, valid_loss=2.34e+3]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069, valid_loss=2.34e+3]           \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 113.94it/s, v_num=0, train_loss_step=0.0427, train_loss_epoch=0.0427, valid_loss=2.34e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0427, train_loss_epoch=0.0427, valid_loss=2.34e+3]         \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=2.34e+3]         \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 101.89it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0386, valid_loss=2.34e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.235, train_loss_epoch=0.235]        \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 117.22it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 115.76it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.185, train_loss_epoch=0.185]         \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 109.61it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]         \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0849, train_loss_epoch=0.0849]         \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]           \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0902, train_loss_epoch=0.0902]         \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 117.57it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]  \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0987, train_loss_epoch=0.0987]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0881, train_loss_epoch=0.0881]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0645, train_loss_epoch=0.0645]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.047, train_loss_epoch=0.047]           \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 114.51it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.047]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0682, train_loss_epoch=0.0682]         \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 109.00it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]  \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651]         \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.068, train_loss_epoch=0.068]           \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0754, train_loss_epoch=0.0754]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0522]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 114.12it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.0518] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.90it/s]\u001b[A\n",
      "Epoch 102: 100%|██████████| 1/1 [00:00<00:00, 117.94it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444, valid_loss=2.14e+3]\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646, valid_loss=2.14e+3]         \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0617, train_loss_epoch=0.0617, valid_loss=2.14e+3]         \n",
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 110.06it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0617, valid_loss=2.14e+3]\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595, valid_loss=2.14e+3]         \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.061, train_loss_epoch=0.061, valid_loss=2.14e+3]           \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 107.99it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=2.14e+3]\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=2.14e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0819, train_loss_epoch=0.0819, valid_loss=2.14e+3]         \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0698, train_loss_epoch=0.0698, valid_loss=2.14e+3]         \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.078, train_loss_epoch=0.078, valid_loss=2.14e+3]           \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0513, train_loss_epoch=0.0513, valid_loss=2.14e+3]         \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0628, train_loss_epoch=0.0628, valid_loss=2.14e+3]         \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=2.14e+3]         \n",
      "Epoch 133: 100%|██████████| 1/1 [00:00<00:00, 118.10it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=2.14e+3]\n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=2.14e+3]         \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0353, train_loss_epoch=0.0353, valid_loss=2.14e+3]         \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 108.46it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=2.14e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=2.14e+3]         \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=2.14e+3]           \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=2.14e+3]         \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0416, train_loss_epoch=0.0416, valid_loss=2.14e+3]         \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 101.16it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115, valid_loss=2.14e+3]  \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115, valid_loss=2.14e+3]         \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0274, train_loss_epoch=0.0274, valid_loss=2.14e+3]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0311, train_loss_epoch=0.0311, valid_loss=2.14e+3]         \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=2.14e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487, valid_loss=2.14e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.14e+3]         \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 108.53it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=2.14e+3]\n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=2.14e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=2.14e+3]         \n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 118.12it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=2.14e+3]\n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0537, train_loss_epoch=0.0537, valid_loss=2.14e+3]         \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=2.14e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:40,648\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 97.89it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=2.14e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 37.43it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.413, train_loss_epoch=0.413]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]        \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 70.08it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127]\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0955, train_loss_epoch=0.0955]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669]        \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 65.14it/s, v_num=0, train_loss_step=0.0831, train_loss_epoch=0.0831]\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0831, train_loss_epoch=0.0831]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0984, train_loss_epoch=0.0984]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0527, train_loss_epoch=0.0527]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0623, train_loss_epoch=0.0623]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0455, train_loss_epoch=0.0455]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0538, train_loss_epoch=0.0538]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.043, train_loss_epoch=0.043]          \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.048, train_loss_epoch=0.048]          \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0474, train_loss_epoch=0.0474]        \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 63.69it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352]        \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 63.75it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305]        \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 57.13it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303]\n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0369, train_loss_epoch=0.0369]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 58.11it/s, v_num=0, train_loss_step=0.0639, train_loss_epoch=0.0452]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 126.78it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=2.18e+3]        \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 58.53it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=2.18e+3]\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=2.18e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.18e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 58.16it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=2.18e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=2.18e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=2.18e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=2.18e+3]        \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 55.42it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405, valid_loss=2.18e+3]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405, valid_loss=2.18e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=2.18e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=2.18e+3]          \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 66.76it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=2.18e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=2.18e+3]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.18e+3]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=2.18e+3]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.18e+3]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00987, train_loss_epoch=0.00987, valid_loss=2.18e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=2.18e+3]          \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=2.18e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0218, train_loss_epoch=0.0218, valid_loss=2.18e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=2.18e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0329, train_loss_epoch=0.0329, valid_loss=2.18e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 55.09it/s, v_num=0, train_loss_step=0.0255, train_loss_epoch=0.0482, valid_loss=2.18e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 137.65it/s]\u001b[A\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0329, train_loss_epoch=0.0329, valid_loss=1.4e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.4e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=1.4e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.4e+3]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=1.4e+3]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=1.4e+3]          \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=1.4e+3]\n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 48.72it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0149, valid_loss=1.4e+3]\n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=1.4e+3]        \n",
      "Epoch 230: 100%|██████████| 1/1 [00:00<00:00, 64.44it/s, v_num=0, train_loss_step=0.0141, train_loss_epoch=0.0141, valid_loss=1.4e+3]\n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0141, train_loss_epoch=0.0141, valid_loss=1.4e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=1.4e+3]        \n",
      "Epoch 244: 100%|██████████| 1/1 [00:00<00:00, 63.48it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=1.4e+3]\n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=1.4e+3]        \n",
      "Epoch 245: 100%|██████████| 1/1 [00:00<00:00, 66.55it/s, v_num=0, train_loss_step=0.00583, train_loss_epoch=0.00583, valid_loss=1.4e+3]\n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=1.4e+3]          \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0116, valid_loss=1.4e+3]          \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169, valid_loss=1.4e+3]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=1.4e+3]          \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=1.4e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=1.4e+3]        \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=1.4e+3]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245, valid_loss=1.4e+3]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=1.4e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=1.4e+3]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=1.4e+3]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.4e+3]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=1.4e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 62.91it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0142, valid_loss=1.4e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 154.45it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 36.68it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0142, valid_loss=2.06e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.06e+3]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0246, train_loss_epoch=0.0246, valid_loss=2.06e+3]          \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=2.06e+3]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=2.06e+3]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=2.06e+3]          \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=2.06e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=2.06e+3]          \n",
      "Epoch 324: 100%|██████████| 1/1 [00:00<00:00, 66.87it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=2.06e+3]\n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0254, train_loss_epoch=0.0254, valid_loss=2.06e+3]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=2.06e+3]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=2.06e+3]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=2.06e+3]        \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=2.06e+3]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=2.06e+3]        \n",
      "Epoch 359: 100%|██████████| 1/1 [00:00<00:00, 65.88it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.00993, valid_loss=2.06e+3] \n",
      "Epoch 360:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0112, train_loss_epoch=0.0112, valid_loss=2.06e+3]         \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00987, train_loss_epoch=0.00987, valid_loss=2.06e+3]        \n",
      "Epoch 373: 100%|██████████| 1/1 [00:00<00:00, 63.97it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0344, valid_loss=2.06e+3]  \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=2.06e+3]        \n",
      "Epoch 380:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.06e+3]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.06e+3]\n",
      "Epoch 381: 100%|██████████| 1/1 [00:00<00:00, 62.11it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=2.06e+3]\n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=2.06e+3]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0159, train_loss_epoch=0.0159, valid_loss=2.06e+3]        \n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0531, train_loss_epoch=0.0531, valid_loss=2.06e+3]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=2.06e+3]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=2.06e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 65.49it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.0356, valid_loss=2.06e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.52it/s]\u001b[A\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.021, train_loss_epoch=0.021, valid_loss=1.87e+3]          \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.87e+3]        \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00825, train_loss_epoch=0.00825, valid_loss=1.87e+3]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=1.87e+3]          \n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00924, train_loss_epoch=0.00924, valid_loss=1.87e+3]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00609, train_loss_epoch=0.00609, valid_loss=1.87e+3]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=1.87e+3]          \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00767, train_loss_epoch=0.00767, valid_loss=1.87e+3]        \n",
      "Epoch 436:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=1.87e+3]          \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0495, train_loss_epoch=0.0495, valid_loss=1.87e+3]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=1.87e+3]        \n",
      "Epoch 447: 100%|██████████| 1/1 [00:00<00:00, 68.13it/s, v_num=0, train_loss_step=0.022, train_loss_epoch=0.022, valid_loss=1.87e+3]  \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271, valid_loss=1.87e+3]        \n",
      "Epoch 454: 100%|██████████| 1/1 [00:00<00:00, 63.92it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=1.87e+3]\n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0152, valid_loss=1.87e+3]        \n",
      "Epoch 455: 100%|██████████| 1/1 [00:00<00:00, 63.40it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=1.87e+3]\n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=1.87e+3]        \n",
      "Epoch 461: 100%|██████████| 1/1 [00:00<00:00, 68.44it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=1.87e+3]\n",
      "Epoch 462:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.87e+3]        \n",
      "Epoch 469:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169, valid_loss=1.87e+3]        \n",
      "Epoch 475: 100%|██████████| 1/1 [00:00<00:00, 50.99it/s, v_num=0, train_loss_step=0.00968, train_loss_epoch=0.00968, valid_loss=1.87e+3]\n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00729, train_loss_epoch=0.00729, valid_loss=1.87e+3]        \n",
      "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=1.87e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:51,021\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487: 100%|██████████| 1/1 [00:00<00:00, 59.80it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=1.87e+3]  \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.200, train_loss_epoch=0.200]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.096, train_loss_epoch=0.096]          \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0916, train_loss_epoch=0.0916]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0581, train_loss_epoch=0.0581]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0581, train_loss_epoch=0.0581]\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0474, train_loss_epoch=0.0474]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0911, train_loss_epoch=0.0911]        \n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00, 73.59it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]\n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]        \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 72.83it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401]        \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 74.11it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]        \n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00, 74.36it/s, v_num=0, train_loss_step=0.0351, train_loss_epoch=0.0351]\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0351, train_loss_epoch=0.0351]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0506, train_loss_epoch=0.0506]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0867, train_loss_epoch=0.0867]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 73.42it/s, v_num=0, train_loss_step=0.041, train_loss_epoch=0.041]  \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.041, train_loss_epoch=0.041]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 75.63it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 77.93it/s, v_num=0, train_loss_step=0.0736, train_loss_epoch=0.050] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 150.51it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447, valid_loss=2.56e+3]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0547, train_loss_epoch=0.0547, valid_loss=2.56e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=2.56e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424, valid_loss=2.56e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 79.36it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=2.56e+3]\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=2.56e+3]          \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 74.51it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=2.56e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=2.56e+3]        \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 73.81it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=2.56e+3]\n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=2.56e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=2.56e+3]        \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 75.41it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.56e+3]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.56e+3]        \n",
      "Epoch 132: 100%|██████████| 1/1 [00:00<00:00, 76.73it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0226, valid_loss=2.56e+3]\n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0356, train_loss_epoch=0.0356, valid_loss=2.56e+3]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.066, train_loss_epoch=0.066, valid_loss=2.56e+3]          \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597, valid_loss=2.56e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683, valid_loss=2.56e+3]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=2.56e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.56e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=2.56e+3]        \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 80.75it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=2.56e+3]  \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=2.56e+3]        \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 72.60it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687, valid_loss=2.56e+3]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687, valid_loss=2.56e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0138, train_loss_epoch=0.0138, valid_loss=2.56e+3]        \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0535, train_loss_epoch=0.0535, valid_loss=2.56e+3]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=2.56e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.56e+3]        \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 83.09it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=2.56e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=2.56e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=2.56e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 86.26it/s, v_num=0, train_loss_step=0.0152, train_loss_epoch=0.0197, valid_loss=2.56e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 172.46it/s]\u001b[A\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=2.17e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0269, valid_loss=2.17e+3]        \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 84.11it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.0269, valid_loss=2.17e+3] \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=2.17e+3]         \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=2.17e+3]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0557, valid_loss=2.17e+3]        \n",
      "Epoch 222: 100%|██████████| 1/1 [00:00<00:00, 82.91it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646, valid_loss=2.17e+3]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646, valid_loss=2.17e+3]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0704, train_loss_epoch=0.0704, valid_loss=2.17e+3]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0382, train_loss_epoch=0.0382, valid_loss=2.17e+3]        \n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00, 82.05it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=2.17e+3]\n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=2.17e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0668, valid_loss=2.17e+3]        \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309, valid_loss=2.17e+3]        \n",
      "Epoch 244: 100%|██████████| 1/1 [00:00<00:00, 84.48it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0309, valid_loss=2.17e+3]\n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272, valid_loss=2.17e+3]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=2.17e+3]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405, valid_loss=2.17e+3]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=2.17e+3]        \n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 82.06it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=2.17e+3]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=2.17e+3]        \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261, valid_loss=2.17e+3]        \n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 76.57it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=2.17e+3]\n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=2.17e+3]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.17e+3]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00893, train_loss_epoch=0.00893, valid_loss=2.17e+3]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0153, train_loss_epoch=0.0153, valid_loss=2.17e+3]          \n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00, 80.59it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0153, valid_loss=2.17e+3]\n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=2.17e+3]        \n",
      "Epoch 293: 100%|██████████| 1/1 [00:00<00:00, 73.33it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=2.17e+3]  \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=2.17e+3]        \n",
      "Epoch 294: 100%|██████████| 1/1 [00:00<00:00, 73.44it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.17e+3]\n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.17e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 75.70it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0137, valid_loss=2.17e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 155.39it/s]\u001b[A\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.92e+3]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=1.92e+3]            \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.044, train_loss_epoch=0.044, valid_loss=1.92e+3]        \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=1.92e+3]        \n",
      "Epoch 320: 100%|██████████| 1/1 [00:00<00:00, 65.26it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=1.92e+3]\n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.92e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.92e+3]        \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.92e+3]\n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=1.92e+3]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=1.92e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=1.92e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:48:56,235\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=1.92e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.92e+3]        \n",
      "Epoch 343: 100%|██████████| 1/1 [00:00<00:00, 64.80it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.92e+3]\n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=1.92e+3]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.92e+3]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.92e+3]\n",
      "Epoch 345: 100%|██████████| 1/1 [00:00<00:00, 58.05it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=1.92e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 62.52it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.245]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]          \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100]          \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0933, train_loss_epoch=0.0933]        \n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 68.76it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638]\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638]        \n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 67.15it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0784]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0784, train_loss_epoch=0.0784]        \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0813, train_loss_epoch=0.0813]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0679, train_loss_epoch=0.0679]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0734, train_loss_epoch=0.0734]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.055, train_loss_epoch=0.055]          \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0662, train_loss_epoch=0.0662]        \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 70.30it/s, v_num=0, train_loss_step=0.0662, train_loss_epoch=0.0662]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502]        \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00, 69.33it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.0502] \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]         \n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00, 67.00it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627]\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0627, train_loss_epoch=0.0627]        \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 67.33it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473]\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0671, train_loss_epoch=0.0671]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0671, train_loss_epoch=0.0671]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486]        \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 70.69it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0366]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352]        \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 67.65it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 70.05it/s, v_num=0, train_loss_step=0.0374, train_loss_epoch=0.0727]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 134.99it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=1.72e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0269, valid_loss=1.72e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=1.72e+3]        \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 67.42it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=1.72e+3]\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0188, train_loss_epoch=0.0188, valid_loss=1.72e+3]        \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 68.12it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=1.72e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=1.72e+3]        \n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 68.13it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=1.72e+3]\n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=1.72e+3]        \n",
      "Epoch 115: 100%|██████████| 1/1 [00:00<00:00, 68.93it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.72e+3]\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.72e+3]        \n",
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00, 68.57it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.72e+3]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.72e+3]        \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 69.09it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=1.72e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=1.72e+3]        \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 68.86it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=1.72e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=1.72e+3]        \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 69.31it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.72e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.72e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 68.37it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.72e+3]\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.72e+3]        \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 68.80it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=1.72e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0194, train_loss_epoch=0.0194, valid_loss=1.72e+3]        \n",
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 69.63it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=1.72e+3]  \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=1.72e+3]        \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 68.09it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.72e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.72e+3]        \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 67.37it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=1.72e+3]\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=1.72e+3]        \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 68.01it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.72e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.72e+3]        \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 69.16it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.72e+3]\n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.72e+3]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=1.72e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=1.72e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0255, train_loss_epoch=0.0255, valid_loss=1.72e+3]        \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.72e+3]        \n",
      "Epoch 147: 100%|██████████| 1/1 [00:00<00:00, 68.01it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481, valid_loss=1.72e+3]\n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481, valid_loss=1.72e+3]        \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=1.72e+3]        \n",
      "Epoch 154: 100%|██████████| 1/1 [00:00<00:00, 62.59it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0355, valid_loss=1.72e+3]\n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=1.72e+3]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0434, valid_loss=1.72e+3]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=1.72e+3]          \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.72e+3]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=1.72e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0118, train_loss_epoch=0.0118, valid_loss=1.72e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196, valid_loss=1.72e+3]        \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 67.70it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.72e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.72e+3]        \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 67.34it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.72e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.72e+3]        \n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 67.23it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.72e+3]  \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.72e+3]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=1.72e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 72.00it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0423, valid_loss=1.72e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.17it/s]\u001b[A\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0273, train_loss_epoch=0.0273, valid_loss=1.9e+3]         \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=1.9e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=1.9e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.9e+3]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 72.14it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=1.9e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=1.9e+3]          \n",
      "Epoch 219: 100%|██████████| 1/1 [00:00<00:00, 66.74it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=1.9e+3]\n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=1.9e+3]        \n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 67.16it/s, v_num=0, train_loss_step=0.0578, train_loss_epoch=0.0578, valid_loss=1.9e+3]\n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0578, train_loss_epoch=0.0578, valid_loss=1.9e+3]        \n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 67.32it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=1.9e+3]\n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=1.9e+3]        \n",
      "Epoch 222: 100%|██████████| 1/1 [00:00<00:00, 66.88it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=1.9e+3]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=1.9e+3]        \n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 65.87it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=1.9e+3]\n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=1.9e+3]        \n",
      "Epoch 224: 100%|██████████| 1/1 [00:00<00:00, 65.99it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=1.9e+3]\n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=1.9e+3]        \n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00, 65.05it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.9e+3]\n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.9e+3]        \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 65.78it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0391, valid_loss=1.9e+3]\n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0391, train_loss_epoch=0.0391, valid_loss=1.9e+3]        \n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 65.98it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.9e+3]\n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.9e+3]        \n",
      "Epoch 228: 100%|██████████| 1/1 [00:00<00:00, 66.83it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.9e+3]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.9e+3]        \n",
      "Epoch 229: 100%|██████████| 1/1 [00:00<00:00, 66.65it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=1.9e+3]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=1.9e+3]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=1.9e+3]\n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=1.9e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=1.9e+3]          \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=1.9e+3]          \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0172, train_loss_epoch=0.0172, valid_loss=1.9e+3]          \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=1.9e+3]        \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.9e+3]          \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0159, train_loss_epoch=0.0159, valid_loss=1.9e+3]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=1.9e+3]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=1.9e+3]          \n",
      "Epoch 263: 100%|██████████| 1/1 [00:00<00:00, 69.52it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.014, valid_loss=1.9e+3]\n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0148, train_loss_epoch=0.0148, valid_loss=1.9e+3]        \n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 68.32it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0148, valid_loss=1.9e+3]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=1.9e+3]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 67.27it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=1.9e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=1.9e+3]        \n",
      "Epoch 266: 100%|██████████| 1/1 [00:00<00:00, 66.13it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=1.9e+3]\n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=1.9e+3]        \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 66.78it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=1.9e+3]\n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=1.9e+3]        \n",
      "Epoch 268: 100%|██████████| 1/1 [00:00<00:00, 66.53it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=1.9e+3]\n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=1.9e+3]        \n",
      "Epoch 269: 100%|██████████| 1/1 [00:00<00:00, 67.18it/s, v_num=0, train_loss_step=0.0233, train_loss_epoch=0.0233, valid_loss=1.9e+3]\n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0233, train_loss_epoch=0.0233, valid_loss=1.9e+3]        \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 68.07it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=1.9e+3]\n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=1.9e+3]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=1.9e+3]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=1.9e+3]          \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00917, train_loss_epoch=0.00917, valid_loss=1.9e+3]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=1.9e+3]          \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 70.54it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0104, valid_loss=1.9e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.94it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 33.70it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=2.08e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=2.08e+3]        \n",
      "Epoch 300: 100%|██████████| 1/1 [00:00<00:00, 67.80it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196, valid_loss=2.08e+3]\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196, valid_loss=2.08e+3]        \n",
      "Epoch 301: 100%|██████████| 1/1 [00:00<00:00, 67.75it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=2.08e+3]\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=2.08e+3]        \n",
      "Epoch 302: 100%|██████████| 1/1 [00:00<00:00, 68.59it/s, v_num=0, train_loss_step=0.00659, train_loss_epoch=0.00659, valid_loss=2.08e+3]\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00659, train_loss_epoch=0.00659, valid_loss=2.08e+3]        \n",
      "Epoch 309: 100%|██████████| 1/1 [00:00<00:00, 62.17it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0107, valid_loss=2.08e+3]  \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0113, train_loss_epoch=0.0113, valid_loss=2.08e+3]        \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=2.08e+3]          \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=2.08e+3]          \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0146, valid_loss=2.08e+3]        \n",
      "Epoch 326:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=2.08e+3]        \n",
      "Epoch 333: 100%|██████████| 1/1 [00:00<00:00, 71.65it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0129, valid_loss=2.08e+3]\n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=2.08e+3]        \n",
      "Epoch 334: 100%|██████████| 1/1 [00:00<00:00, 69.67it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0144, valid_loss=2.08e+3]\n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=2.08e+3]        \n",
      "Epoch 335: 100%|██████████| 1/1 [00:00<00:00, 70.66it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0193, valid_loss=2.08e+3]\n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=2.08e+3]        \n",
      "Epoch 336: 100%|██████████| 1/1 [00:00<00:00, 71.12it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=2.08e+3]\n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=2.08e+3]        \n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0111, train_loss_epoch=0.0111, valid_loss=2.08e+3]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=2.08e+3]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=2.08e+3]          \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=2.08e+3]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0085, train_loss_epoch=0.0085, valid_loss=2.08e+3]          \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00862, train_loss_epoch=0.00862, valid_loss=2.08e+3]        \n",
      "Epoch 365:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=2.08e+3]            \n",
      "Epoch 366:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00902, train_loss_epoch=0.00902, valid_loss=2.08e+3]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00828, train_loss_epoch=0.00828, valid_loss=2.08e+3]        \n",
      "Epoch 373: 100%|██████████| 1/1 [00:00<00:00, 65.56it/s, v_num=0, train_loss_step=0.00811, train_loss_epoch=0.00828, valid_loss=2.08e+3]\n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00811, train_loss_epoch=0.00811, valid_loss=2.08e+3]        \n",
      "Epoch 374: 100%|██████████| 1/1 [00:00<00:00, 69.04it/s, v_num=0, train_loss_step=0.00811, train_loss_epoch=0.00811, valid_loss=2.08e+3]\n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00943, train_loss_epoch=0.00943, valid_loss=2.08e+3]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0089, train_loss_epoch=0.0089, valid_loss=2.08e+3]          \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.009, train_loss_epoch=0.009, valid_loss=2.08e+3]          \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00798, train_loss_epoch=0.00798, valid_loss=2.08e+3]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00591, train_loss_epoch=0.00591, valid_loss=2.08e+3]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00706, train_loss_epoch=0.00706, valid_loss=2.08e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 62.15it/s, v_num=0, train_loss_step=0.00492, train_loss_epoch=0.00533, valid_loss=2.08e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 131.83it/s]\u001b[A\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00563, train_loss_epoch=0.00563, valid_loss=2.03e+3]        \n",
      "Epoch 408: 100%|██████████| 1/1 [00:00<00:00, 62.85it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0123, valid_loss=2.03e+3]  \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424, valid_loss=2.03e+3]        \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=2.03e+3]          \n",
      "Epoch 415: 100%|██████████| 1/1 [00:00<00:00, 62.99it/s, v_num=0, train_loss_step=0.0101, train_loss_epoch=0.0105, valid_loss=2.03e+3]\n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0101, train_loss_epoch=0.0101, valid_loss=2.03e+3]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=2.03e+3]        \n",
      "Epoch 422: 100%|██████████| 1/1 [00:00<00:00, 63.98it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=2.03e+3]\n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=2.03e+3]        \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=2.03e+3]        \n",
      "Epoch 429: 100%|██████████| 1/1 [00:00<00:00, 61.46it/s, v_num=0, train_loss_step=0.00954, train_loss_epoch=0.00954, valid_loss=2.03e+3]\n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00954, train_loss_epoch=0.00954, valid_loss=2.03e+3]        \n",
      "Epoch 436: 100%|██████████| 1/1 [00:00<00:00, 60.68it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=2.03e+3]  \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=2.03e+3]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0432, train_loss_epoch=0.0432, valid_loss=2.03e+3]        \n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=2.03e+3]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00678, train_loss_epoch=0.00678, valid_loss=2.03e+3]        \n",
      "Epoch 458: 100%|██████████| 1/1 [00:00<00:00, 68.13it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=2.03e+3]  \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=2.03e+3]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=2.03e+3]        \n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=2.03e+3]\n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0183, train_loss_epoch=0.0183, valid_loss=2.03e+3]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00754, train_loss_epoch=0.00754, valid_loss=2.03e+3]        \n",
      "Epoch 469:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=2.03e+3]            \n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=2.03e+3]          \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00886, train_loss_epoch=0.00886, valid_loss=2.03e+3]        \n",
      "Epoch 484: 100%|██████████| 1/1 [00:00<00:00, 67.98it/s, v_num=0, train_loss_step=0.00919, train_loss_epoch=0.00919, valid_loss=2.03e+3]\n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00919, train_loss_epoch=0.00919, valid_loss=2.03e+3]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00552, train_loss_epoch=0.00552, valid_loss=2.03e+3]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=2.03e+3]          \n",
      "Epoch 494:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=2.03e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:04,495\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497: 100%|██████████| 1/1 [00:00<00:00, 65.73it/s, v_num=0, train_loss_step=0.0141, train_loss_epoch=0.0141, valid_loss=2.03e+3]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.311, train_loss_epoch=0.311]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]        \n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 90.24it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.112]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0999, train_loss_epoch=0.0999]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.259, train_loss_epoch=0.259]          \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0727, train_loss_epoch=0.0727]        \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 87.99it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0915, train_loss_epoch=0.0915]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0711, train_loss_epoch=0.0711]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0611, train_loss_epoch=0.0611]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0666, train_loss_epoch=0.0666]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0758, train_loss_epoch=0.0758]        \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669]        \n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00, 88.28it/s, v_num=0, train_loss_step=0.0451, train_loss_epoch=0.0451]\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0451, train_loss_epoch=0.0451]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0451, train_loss_epoch=0.0451]\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]        \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 88.70it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0529]\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389]        \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032]          \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323]        \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256]\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0689, train_loss_epoch=0.0689]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354]        \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 89.11it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.0354] \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 93.96it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0473]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.56it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.18e+3]       \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 84.52it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=1.18e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=1.18e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0345, train_loss_epoch=0.0345, valid_loss=1.18e+3]        \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 91.77it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=1.18e+3]\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=1.18e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=1.18e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=1.18e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=1.18e+3]        \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 85.35it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.18e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.18e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=1.18e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=1.18e+3]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=1.18e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.015, train_loss_epoch=0.015, valid_loss=1.18e+3]          \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=1.18e+3]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0203, train_loss_epoch=0.0203, valid_loss=1.18e+3]        \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 86.29it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.18e+3]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.18e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=1.18e+3]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0398, valid_loss=1.18e+3]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 83.44it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.18e+3]  \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.18e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=1.18e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.18e+3]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=1.18e+3]        \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 86.05it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=1.18e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=1.18e+3]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352, valid_loss=1.18e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 90.64it/s, v_num=0, train_loss_step=0.0352, train_loss_epoch=0.0352, valid_loss=1.18e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=1.18e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.18e+3]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0154, train_loss_epoch=0.0154, valid_loss=1.18e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=1.18e+3]        \n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00, 88.58it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0224, valid_loss=1.18e+3]\n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0181, train_loss_epoch=0.0181, valid_loss=1.18e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00914, train_loss_epoch=0.00914, valid_loss=1.18e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=1.18e+3]          \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=1.18e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0338, train_loss_epoch=0.0338, valid_loss=1.18e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 94.07it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.0254, valid_loss=1.18e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 184.61it/s]\u001b[A\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0403, train_loss_epoch=0.0403, valid_loss=1.82e+3]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.82e+3]        \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0143, train_loss_epoch=0.0143, valid_loss=1.82e+3]        \n",
      "Epoch 213: 100%|██████████| 1/1 [00:00<00:00, 84.53it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=1.82e+3]\n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=1.82e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=1.82e+3]          \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=1.82e+3]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=1.82e+3]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.82e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0886, train_loss_epoch=0.0886, valid_loss=1.82e+3]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261, valid_loss=1.82e+3]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0105, train_loss_epoch=0.0105, valid_loss=1.82e+3]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.82e+3]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=1.82e+3]        \n",
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00, 89.83it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=1.82e+3]\n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0305, train_loss_epoch=0.0305, valid_loss=1.82e+3]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=1.82e+3]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 88.22it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=1.82e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=1.82e+3]        \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.82e+3]        \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 92.74it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.82e+3]\n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.82e+3]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=1.82e+3]        \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0165, train_loss_epoch=0.0165, valid_loss=1.82e+3]          \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=1.82e+3]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=1.82e+3]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.82e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.82e+3]        \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.82e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 90.44it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0248, valid_loss=1.82e+3]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 178.37it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 48.65it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0248, valid_loss=2.1e+3] \n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0117, train_loss_epoch=0.0117, valid_loss=2.1e+3]        \n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.1e+3]          \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=2.1e+3]          \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.080, train_loss_epoch=0.080, valid_loss=2.1e+3]          \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=2.1e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0197, train_loss_epoch=0.0197, valid_loss=2.1e+3]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=2.1e+3]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=2.1e+3]        \n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.1e+3]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=2.1e+3]        \n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 90.10it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0266, valid_loss=2.1e+3]\n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 86.95it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.1e+3]\n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.1e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=2.1e+3]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.011, train_loss_epoch=0.011, valid_loss=2.1e+3]          \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=2.1e+3]        \n",
      "Epoch 352: 100%|██████████| 1/1 [00:00<00:00, 83.33it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=2.1e+3]\n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=2.1e+3]          \n",
      "Epoch 353: 100%|██████████| 1/1 [00:00<00:00, 78.42it/s, v_num=0, train_loss_step=0.00895, train_loss_epoch=0.00895, valid_loss=2.1e+3]\n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00895, train_loss_epoch=0.00895, valid_loss=2.1e+3]        \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=2.1e+3]            \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00918, train_loss_epoch=0.00918, valid_loss=2.1e+3]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00861, train_loss_epoch=0.00861, valid_loss=2.1e+3]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=2.1e+3]          \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=2.1e+3]        \n",
      "Epoch 372: 100%|██████████| 1/1 [00:00<00:00, 85.80it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0125, valid_loss=2.1e+3]\n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=2.1e+3]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.1e+3]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=2.1e+3]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=2.1e+3]        \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=2.1e+3]        \n",
      "Epoch 394:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=2.1e+3]        \n",
      "Epoch 394: 100%|██████████| 1/1 [00:00<00:00, 86.43it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=2.1e+3]\n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=2.1e+3]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=2.1e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 93.07it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0249, valid_loss=2.1e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 180.50it/s]\u001b[A\n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.94e+3]        \n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0218, train_loss_epoch=0.0218, valid_loss=1.94e+3]        \n",
      "Epoch 405: 100%|██████████| 1/1 [00:00<00:00, 92.78it/s, v_num=0, train_loss_step=0.0218, train_loss_epoch=0.0218, valid_loss=1.94e+3]\n",
      "Epoch 406:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.94e+3]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=1.94e+3]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=1.94e+3]          \n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.94e+3]        \n",
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 86.54it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=1.94e+3]\n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=1.94e+3]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0136, train_loss_epoch=0.0136, valid_loss=1.94e+3]        \n",
      "Epoch 428: 100%|██████████| 1/1 [00:00<00:00, 93.75it/s, v_num=0, train_loss_step=0.00768, train_loss_epoch=0.00768, valid_loss=1.94e+3]\n",
      "Epoch 428: 100%|██████████| 1/1 [00:00<00:00, 91.62it/s, v_num=0, train_loss_step=0.0076, train_loss_epoch=0.00768, valid_loss=1.94e+3] \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0076, train_loss_epoch=0.0076, valid_loss=1.94e+3]         \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.010, train_loss_epoch=0.010, valid_loss=1.94e+3]          \n",
      "Epoch 439:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=1.94e+3]          \n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=1.94e+3]        \n",
      "Epoch 440: 100%|██████████| 1/1 [00:00<00:00, 86.50it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=1.94e+3]\n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=1.94e+3]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558, valid_loss=1.94e+3]        \n",
      "Epoch 450:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=1.94e+3]        \n",
      "Epoch 451:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=1.94e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:10,466\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00961, train_loss_epoch=0.00961, valid_loss=1.94e+3]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00821, train_loss_epoch=0.00821, valid_loss=1.94e+3]        \n",
      "Epoch 463: 100%|██████████| 1/1 [00:00<00:00, 82.70it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=1.94e+3]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.387, train_loss_epoch=0.387]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.299, train_loss_epoch=0.299]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]        \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 76.33it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.087, train_loss_epoch=0.087]          \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.210]          \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0818, train_loss_epoch=0.0818]        \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 74.73it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]  \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]          \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0796]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0711, train_loss_epoch=0.0711]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0886, train_loss_epoch=0.0886]        \n",
      "Epoch 77: 100%|██████████| 1/1 [00:00<00:00, 79.22it/s, v_num=0, train_loss_step=0.0886, train_loss_epoch=0.0886]\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0624]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 75.03it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]  \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0465, train_loss_epoch=0.0465]        \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0451, train_loss_epoch=0.0451]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0573, train_loss_epoch=0.0573]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]          \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 79.79it/s, v_num=0, train_loss_step=0.0868, train_loss_epoch=0.0527]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 142.29it/s]\u001b[A\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0614, train_loss_epoch=0.0614, valid_loss=1.46e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0527, train_loss_epoch=0.0527, valid_loss=1.46e+3]        \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 79.64it/s, v_num=0, train_loss_step=0.0527, train_loss_epoch=0.0527, valid_loss=1.46e+3]\n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607, valid_loss=1.46e+3]        \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 75.05it/s, v_num=0, train_loss_step=0.0472, train_loss_epoch=0.0472, valid_loss=1.46e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0472, train_loss_epoch=0.0472, valid_loss=1.46e+3]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0457, train_loss_epoch=0.0457, valid_loss=1.46e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=1.46e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0346, train_loss_epoch=0.0346, valid_loss=1.46e+3]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=1.46e+3]        \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0402, train_loss_epoch=0.0402, valid_loss=1.46e+3]        \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 76.68it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0402, valid_loss=1.46e+3]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=1.46e+3]        \n",
      "Epoch 147: 100%|██████████| 1/1 [00:00<00:00, 73.88it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=1.46e+3]  \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=1.46e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0386, valid_loss=1.46e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0258, train_loss_epoch=0.0258, valid_loss=1.46e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0932, train_loss_epoch=0.0932, valid_loss=1.46e+3]        \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 78.70it/s, v_num=0, train_loss_step=0.0932, train_loss_epoch=0.0932, valid_loss=1.46e+3]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.46e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395, valid_loss=1.46e+3]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=1.46e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487, valid_loss=1.46e+3]        \n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00, 74.16it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.46e+3]\n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.46e+3]        \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 73.92it/s, v_num=0, train_loss_step=0.0361, train_loss_epoch=0.0361, valid_loss=1.46e+3]\n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0361, train_loss_epoch=0.0361, valid_loss=1.46e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245, valid_loss=1.46e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.039, train_loss_epoch=0.039, valid_loss=1.46e+3]          \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 78.40it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0244, valid_loss=1.46e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.78it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0434, train_loss_epoch=0.0434, valid_loss=1.27e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0218, train_loss_epoch=0.0218, valid_loss=1.27e+3]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.27e+3]        \n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 76.55it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0259, valid_loss=1.27e+3]\n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.27e+3]        \n",
      "Epoch 217: 100%|██████████| 1/1 [00:00<00:00, 74.03it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196, valid_loss=1.27e+3]\n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0196, train_loss_epoch=0.0196, valid_loss=1.27e+3]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 74.24it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=1.27e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=1.27e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279, valid_loss=1.27e+3]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=1.27e+3]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.27e+3]        \n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00, 78.44it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.27e+3]\n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0527, train_loss_epoch=0.0527, valid_loss=1.27e+3]        \n",
      "Epoch 237: 100%|██████████| 1/1 [00:00<00:00, 74.04it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=1.27e+3]\n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0294, train_loss_epoch=0.0294, valid_loss=1.27e+3]        \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.27e+3]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0254, train_loss_epoch=0.0254, valid_loss=1.27e+3]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342, valid_loss=1.27e+3]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=1.27e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=1.27e+3]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.016, train_loss_epoch=0.016, valid_loss=1.27e+3]          \n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459, valid_loss=1.27e+3]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488, valid_loss=1.27e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=1.27e+3]          \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0664, train_loss_epoch=0.0664, valid_loss=1.27e+3]        \n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00, 65.61it/s, v_num=0, train_loss_step=0.0774, train_loss_epoch=0.0774, valid_loss=1.27e+3]\n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0774, train_loss_epoch=0.0774, valid_loss=1.27e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 66.86it/s, v_num=0, train_loss_step=0.0467, train_loss_epoch=0.0457, valid_loss=1.27e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 138.47it/s]\u001b[A\n",
      "Epoch 305: 100%|██████████| 1/1 [00:00<00:00, 64.54it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=994.0]  \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=994.0]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0343, valid_loss=994.0]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=994.0]        \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=994.0]        \n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=994.0]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=994.0]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=994.0]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=994.0]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=994.0]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0257, train_loss_epoch=0.0257, valid_loss=994.0]        \n",
      "Epoch 354: 100%|██████████| 1/1 [00:00<00:00, 65.19it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=994.0]\n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=994.0]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124, valid_loss=994.0]          \n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0471, train_loss_epoch=0.0471, valid_loss=994.0]        \n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0471, train_loss_epoch=0.0471, valid_loss=994.0]\n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0353, train_loss_epoch=0.0353, valid_loss=994.0]        \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=994.0]        \n",
      "Epoch 382: 100%|██████████| 1/1 [00:00<00:00, 65.40it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=994.0]\n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=994.0]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.019, train_loss_epoch=0.019, valid_loss=994.0]          \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=994.0]        \n",
      "Epoch 398:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=994.0]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 78.26it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0152, valid_loss=994.0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.84it/s]\u001b[A\n",
      "Epoch 405:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=1.71e+3]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0313, train_loss_epoch=0.0313, valid_loss=1.71e+3]        \n",
      "Epoch 413: 100%|██████████| 1/1 [00:00<00:00, 56.59it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.0313, valid_loss=1.71e+3] \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.71e+3]         \n",
      "Epoch 414: 100%|██████████| 1/1 [00:00<00:00, 55.74it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.028, valid_loss=1.71e+3]\n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129, valid_loss=1.71e+3]        \n",
      "Epoch 415: 100%|██████████| 1/1 [00:00<00:00, 53.28it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=1.71e+3]\n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=1.71e+3]        \n",
      "Epoch 416: 100%|██████████| 1/1 [00:00<00:00, 54.13it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=1.71e+3]\n",
      "Epoch 417:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0142, train_loss_epoch=0.0142, valid_loss=1.71e+3]        \n",
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 53.71it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.71e+3]\n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.71e+3]        \n",
      "Epoch 418: 100%|██████████| 1/1 [00:00<00:00, 53.93it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.71e+3]\n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.71e+3]        \n",
      "Epoch 419: 100%|██████████| 1/1 [00:00<00:00, 56.14it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0227, valid_loss=1.71e+3]\n",
      "Epoch 420:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.71e+3]        \n",
      "Epoch 420: 100%|██████████| 1/1 [00:00<00:00, 55.73it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0248, valid_loss=1.71e+3]\n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0363, train_loss_epoch=0.0363, valid_loss=1.71e+3]        \n",
      "Epoch 421: 100%|██████████| 1/1 [00:00<00:00, 55.89it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0363, valid_loss=1.71e+3]\n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0204, train_loss_epoch=0.0204, valid_loss=1.71e+3]        \n",
      "Epoch 422: 100%|██████████| 1/1 [00:00<00:00, 55.38it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0204, valid_loss=1.71e+3]\n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0174, valid_loss=1.71e+3]        \n",
      "Epoch 423: 100%|██████████| 1/1 [00:00<00:00, 55.39it/s, v_num=0, train_loss_step=0.0174, train_loss_epoch=0.0174, valid_loss=1.71e+3]\n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.71e+3]        \n",
      "Epoch 432:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=1.71e+3]        \n",
      "Epoch 432: 100%|██████████| 1/1 [00:00<00:00, 56.58it/s, v_num=0, train_loss_step=0.0114, train_loss_epoch=0.0216, valid_loss=1.71e+3]\n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0114, train_loss_epoch=0.0114, valid_loss=1.71e+3]        \n",
      "Epoch 433: 100%|██████████| 1/1 [00:00<00:00, 54.61it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=1.71e+3]\n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0103, train_loss_epoch=0.0103, valid_loss=1.71e+3]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.71e+3]        \n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0348, train_loss_epoch=0.0348, valid_loss=1.71e+3]          \n",
      "Epoch 456: 100%|██████████| 1/1 [00:00<00:00, 67.29it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=1.71e+3]\n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=1.71e+3]        \n",
      "Epoch 457: 100%|██████████| 1/1 [00:00<00:00, 69.81it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=1.71e+3]\n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0431, train_loss_epoch=0.0431, valid_loss=1.71e+3]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=1.71e+3]        \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332, valid_loss=1.71e+3]        \n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=1.71e+3]          \n",
      "Epoch 483:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=1.71e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:18,268\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.71e+3]        \n",
      "Epoch 495: 100%|██████████| 1/1 [00:00<00:00, 70.30it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=1.71e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.443, train_loss_epoch=0.443]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.167, train_loss_epoch=0.167]        \n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 77.80it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.163, train_loss_epoch=0.163]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0814, train_loss_epoch=0.0814]        \n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 82.58it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]  \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0969, train_loss_epoch=0.0969]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.076, train_loss_epoch=0.076]          \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0659, train_loss_epoch=0.0659]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.045, train_loss_epoch=0.045]          \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0752, train_loss_epoch=0.0752]        \n",
      "Epoch 83: 100%|██████████| 1/1 [00:00<00:00, 81.16it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0464, train_loss_epoch=0.0464]        \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 82.25it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 86.53it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0483]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 153.13it/s]\u001b[A\n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=2.08e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0235, train_loss_epoch=0.0235, valid_loss=2.08e+3]        \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 80.50it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.08e+3]\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.08e+3]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316, valid_loss=2.08e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0161, train_loss_epoch=0.0161, valid_loss=2.08e+3]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0134, train_loss_epoch=0.0134, valid_loss=2.08e+3]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=2.08e+3]        \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 81.84it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.049, valid_loss=2.08e+3] \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 79.21it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.08e+3]\n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.08e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=2.08e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0495, train_loss_epoch=0.0495, valid_loss=2.08e+3]        \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 80.05it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=2.08e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=2.08e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=2.08e+3]          \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0261, train_loss_epoch=0.0261, valid_loss=2.08e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=2.08e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=2.08e+3]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.075, train_loss_epoch=0.075, valid_loss=2.08e+3]          \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354, valid_loss=2.08e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0419, train_loss_epoch=0.0419, valid_loss=2.08e+3]        \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 86.92it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0391, valid_loss=2.08e+3]\n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=2.08e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=2.08e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0224, train_loss_epoch=0.0224, valid_loss=2.08e+3]\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=2.08e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 83.86it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.0329, valid_loss=2.08e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 169.23it/s]\u001b[A\n",
      "Epoch 203: 100%|██████████| 1/1 [00:00<00:00, 70.79it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=1.56e+3]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0232, valid_loss=1.56e+3]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=1.56e+3]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0221, train_loss_epoch=0.0221, valid_loss=1.56e+3]\n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.56e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=1.56e+3]        \n",
      "Epoch 223: 100%|██████████| 1/1 [00:00<00:00, 70.21it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=1.56e+3]  \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=1.56e+3]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=1.56e+3]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=1.56e+3]          \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0172, train_loss_epoch=0.0172, valid_loss=1.56e+3]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.56e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0361, train_loss_epoch=0.0361, valid_loss=1.56e+3]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.56e+3]        \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0198, train_loss_epoch=0.0198, valid_loss=1.56e+3]        \n",
      "Epoch 287: 100%|██████████| 1/1 [00:00<00:00, 69.09it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=1.56e+3]\n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=1.56e+3]        \n",
      "Epoch 289:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=1.56e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0151, train_loss_epoch=0.0151, valid_loss=1.56e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 87.25it/s, v_num=0, train_loss_step=0.00809, train_loss_epoch=0.0166, valid_loss=1.56e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 171.43it/s]\u001b[A\n",
      "Epoch 305: 100%|██████████| 1/1 [00:00<00:00, 70.10it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=2.08e+3]  \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=2.08e+3]        \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0158, train_loss_epoch=0.0158, valid_loss=2.08e+3]        \n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=2.08e+3]          \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=2.08e+3]          \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0178, train_loss_epoch=0.0178, valid_loss=2.08e+3]          \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0156, train_loss_epoch=0.0156, valid_loss=2.08e+3]        \n",
      "Epoch 343:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129, valid_loss=2.08e+3]        \n",
      "Epoch 343: 100%|██████████| 1/1 [00:00<00:00, 59.11it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=2.08e+3]\n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=2.08e+3]        \n",
      "Epoch 344: 100%|██████████| 1/1 [00:00<00:00, 58.59it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.08e+3]\n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=2.08e+3]        \n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0137, train_loss_epoch=0.0137, valid_loss=2.08e+3]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0149, train_loss_epoch=0.0149, valid_loss=2.08e+3]        \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017, valid_loss=2.08e+3]          \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00767, train_loss_epoch=0.00767, valid_loss=2.08e+3]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=2.08e+3]            \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00986, train_loss_epoch=0.00986, valid_loss=2.08e+3]        \n",
      "Epoch 391:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00791, train_loss_epoch=0.00791, valid_loss=2.08e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 76.27it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0119, valid_loss=2.08e+3]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 129.84it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 38.47it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0119, valid_loss=1.93e+3]\n",
      "Epoch 400:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=1.93e+3]        \n",
      "Epoch 408: 100%|██████████| 1/1 [00:00<00:00, 69.38it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=1.93e+3]\n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0147, train_loss_epoch=0.0147, valid_loss=1.93e+3]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0128, train_loss_epoch=0.0128, valid_loss=1.93e+3]        \n",
      "Epoch 419:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00843, train_loss_epoch=0.00843, valid_loss=1.93e+3]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00624, train_loss_epoch=0.00624, valid_loss=1.93e+3]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0119, train_loss_epoch=0.0119, valid_loss=1.93e+3]          \n",
      "Epoch 446:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0438, train_loss_epoch=0.0438, valid_loss=1.93e+3]          \n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0177, train_loss_epoch=0.0177, valid_loss=1.93e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:24,787\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460: 100%|██████████| 1/1 [00:00<00:00, 78.07it/s, v_num=0, train_loss_step=0.0127, train_loss_epoch=0.0127, valid_loss=1.93e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.210]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.210, train_loss_epoch=0.210]          \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0943, train_loss_epoch=0.0943]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 68.06it/s, v_num=0, train_loss_step=0.0905, train_loss_epoch=0.0905]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0905, train_loss_epoch=0.0905]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]          \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]          \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 77.40it/s, v_num=0, train_loss_step=0.0881, train_loss_epoch=0.125]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0881, train_loss_epoch=0.0881]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0857, train_loss_epoch=0.0857]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0792, train_loss_epoch=0.0792]        \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 75.68it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]  \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0801, train_loss_epoch=0.0801]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0618, train_loss_epoch=0.0618]        \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 74.58it/s, v_num=0, train_loss_step=0.0839, train_loss_epoch=0.0839]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0839, train_loss_epoch=0.0839]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.056, train_loss_epoch=0.056]          \n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0988, train_loss_epoch=0.0988]        \n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 74.31it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0691, train_loss_epoch=0.0691]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0655, train_loss_epoch=0.0655]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683]        \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 77.04it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0683]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0563, train_loss_epoch=0.0563]        \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 74.97it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0479, train_loss_epoch=0.0479]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 77.43it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.0479] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.92it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 43.17it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.0479, valid_loss=1.66e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=1.66e+3]        \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 72.59it/s, v_num=0, train_loss_step=0.0465, train_loss_epoch=0.0465, valid_loss=1.66e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0465, train_loss_epoch=0.0465, valid_loss=1.66e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0532, train_loss_epoch=0.0532, valid_loss=1.66e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0354, train_loss_epoch=0.0354, valid_loss=1.66e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 74.85it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836, valid_loss=1.66e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836, valid_loss=1.66e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545, valid_loss=1.66e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0634, train_loss_epoch=0.0634, valid_loss=1.66e+3]        \n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 75.30it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=1.66e+3]\n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=1.66e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389, valid_loss=1.66e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0334, train_loss_epoch=0.0334, valid_loss=1.66e+3]        \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 77.59it/s, v_num=0, train_loss_step=0.058, train_loss_epoch=0.0334, valid_loss=1.66e+3] \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 75.13it/s, v_num=0, train_loss_step=0.058, train_loss_epoch=0.058, valid_loss=1.66e+3] \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.058, train_loss_epoch=0.058, valid_loss=1.66e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=1.66e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331, valid_loss=1.66e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0246, train_loss_epoch=0.0246, valid_loss=1.66e+3]        \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 75.76it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.66e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0227, train_loss_epoch=0.0227, valid_loss=1.66e+3]        \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 69.67it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0227, valid_loss=1.66e+3]\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=1.66e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.66e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0417, train_loss_epoch=0.0417, valid_loss=1.66e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0885, train_loss_epoch=0.0885, valid_loss=1.66e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=1.66e+3]        \n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00, 69.44it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0381, valid_loss=1.66e+3]\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=1.66e+3]        \n",
      "Epoch 173: 100%|██████████| 1/1 [00:00<00:00, 70.62it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=1.66e+3]\n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=1.66e+3]        \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=1.66e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0277, train_loss_epoch=0.0277, valid_loss=1.66e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=1.66e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.66e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 79.55it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0132, valid_loss=1.66e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.93it/s]\u001b[A\n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0416, train_loss_epoch=0.0416, valid_loss=1.53e+3]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.025, valid_loss=1.53e+3]          \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=1.53e+3]        \n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=1.53e+3]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=1.53e+3]          \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=1.53e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=1.53e+3]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0133, train_loss_epoch=0.0133, valid_loss=1.53e+3]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0232, valid_loss=1.53e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0357, train_loss_epoch=0.0357, valid_loss=1.53e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 69.39it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0357, valid_loss=1.53e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.53e+3]        \n",
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00, 70.71it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.53e+3]\n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.53e+3]        \n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272, valid_loss=1.53e+3]        \n",
      "Epoch 259: 100%|██████████| 1/1 [00:00<00:00, 76.33it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=1.53e+3]\n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=1.53e+3]        \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 67.62it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0206, valid_loss=1.53e+3]\n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0232, train_loss_epoch=0.0232, valid_loss=1.53e+3]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=1.53e+3]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=1.53e+3]          \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 80.60it/s, v_num=0, train_loss_step=0.0443, train_loss_epoch=0.0443, valid_loss=1.53e+3]\n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.025, train_loss_epoch=0.025, valid_loss=1.53e+3]          \n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 75.64it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=1.53e+3]\n",
      "Epoch 278:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0559, train_loss_epoch=0.0559, valid_loss=1.53e+3]        \n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00, 67.47it/s, v_num=0, train_loss_step=0.063, train_loss_epoch=0.0559, valid_loss=1.53e+3] \n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.063, train_loss_epoch=0.063, valid_loss=1.53e+3]         \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 79.21it/s, v_num=0, train_loss_step=0.0348, train_loss_epoch=0.0661, valid_loss=1.53e+3]\n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0348, train_loss_epoch=0.0348, valid_loss=1.53e+3]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.53e+3]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0213, train_loss_epoch=0.0213, valid_loss=1.53e+3]        \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 75.84it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=1.53e+3]  \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.014, train_loss_epoch=0.014, valid_loss=1.53e+3]        \n",
      "Epoch 297: 100%|██████████| 1/1 [00:00<00:00, 68.08it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.014, valid_loss=1.53e+3]\n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.53e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 68.26it/s, v_num=0, train_loss_step=0.0116, train_loss_epoch=0.0176, valid_loss=1.53e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 141.99it/s]\u001b[A\n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0472, train_loss_epoch=0.0472, valid_loss=1.87e+3]        \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=1.87e+3]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=1.87e+3]        \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0193, train_loss_epoch=0.0193, valid_loss=1.87e+3]        \n",
      "Epoch 322:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=1.87e+3]        \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0164, train_loss_epoch=0.0164, valid_loss=1.87e+3]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0169, train_loss_epoch=0.0169, valid_loss=1.87e+3]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=1.87e+3]        \n",
      "Epoch 339: 100%|██████████| 1/1 [00:00<00:00, 80.42it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=1.87e+3]\n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=1.87e+3]        \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00, 75.75it/s, v_num=0, train_loss_step=0.00909, train_loss_epoch=0.00909, valid_loss=1.87e+3]\n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00909, train_loss_epoch=0.00909, valid_loss=1.87e+3]        \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=1.87e+3]          \n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=1.87e+3]        \n",
      "Epoch 350: 100%|██████████| 1/1 [00:00<00:00, 80.93it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=1.87e+3]\n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0284, train_loss_epoch=0.0284, valid_loss=1.87e+3]        \n",
      "Epoch 351: 100%|██████████| 1/1 [00:00<00:00, 76.07it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=1.87e+3]\n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=1.87e+3]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=1.87e+3]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:30,224\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.87e+3]        \n",
      "Epoch 361: 100%|██████████| 1/1 [00:00<00:00, 76.22it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=1.87e+3]\n",
      "Epoch 361: 100%|██████████| 1/1 [00:00<00:00, 70.48it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=1.87e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.332, train_loss_epoch=0.332]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.284, train_loss_epoch=0.284]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.356, train_loss_epoch=0.356]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.226, train_loss_epoch=0.226]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255]        \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 89.78it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.240, train_loss_epoch=0.240]        \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 93.08it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.240]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.242, train_loss_epoch=0.242]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]          \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 87.16it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.125]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 133.46it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 42.91it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.125, valid_loss=2.08e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=2.08e+3]       \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.08e+3]          \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.08e+3]        \n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00, 81.79it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.08e+3]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103, valid_loss=2.08e+3]        \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 88.71it/s, v_num=0, train_loss_step=0.0926, train_loss_epoch=0.0926, valid_loss=2.08e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0816, train_loss_epoch=0.0816, valid_loss=2.08e+3]        \n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00, 88.65it/s, v_num=0, train_loss_step=0.0846, train_loss_epoch=0.0846, valid_loss=2.08e+3]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0929, train_loss_epoch=0.0929, valid_loss=2.08e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=2.08e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695, valid_loss=2.08e+3]\n",
      "Epoch 147: 100%|██████████| 1/1 [00:00<00:00, 91.16it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146, valid_loss=2.08e+3]  \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146, valid_loss=2.08e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0814, train_loss_epoch=0.0814, valid_loss=2.08e+3]        \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 85.70it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=2.08e+3]  \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165, valid_loss=2.08e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0583, train_loss_epoch=0.0583, valid_loss=2.08e+3]        \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 94.21it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0583, valid_loss=2.08e+3]\n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0591, valid_loss=2.08e+3]        \n",
      "Epoch 153:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684, valid_loss=2.08e+3]        \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=2.08e+3]         \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.08e+3]           \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0609, train_loss_epoch=0.0609, valid_loss=2.08e+3]         \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0647, train_loss_epoch=0.0647, valid_loss=2.08e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0547, train_loss_epoch=0.0547, valid_loss=2.08e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0553, train_loss_epoch=0.0553, valid_loss=2.08e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 94.23it/s, v_num=0, train_loss_step=0.0849, train_loss_epoch=0.0553, valid_loss=2.08e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.42it/s]\u001b[A\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0567, train_loss_epoch=0.0567, valid_loss=2.17e+3]        \n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 90.16it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.17e+3]\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.17e+3]        \n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0694, train_loss_epoch=0.0694, valid_loss=2.17e+3]        \n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00, 89.55it/s, v_num=0, train_loss_step=0.0621, train_loss_epoch=0.0621, valid_loss=2.17e+3]\n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0621, train_loss_epoch=0.0621, valid_loss=2.17e+3]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119, valid_loss=2.17e+3]          \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0555, train_loss_epoch=0.0555, valid_loss=2.17e+3]         \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=2.17e+3]         \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 96.96it/s, v_num=0, train_loss_step=0.0456, train_loss_epoch=0.0456, valid_loss=2.17e+3]\n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0557, train_loss_epoch=0.0557, valid_loss=2.17e+3]        \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496, valid_loss=2.17e+3]        \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.17e+3]         \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=2.17e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=2.17e+3]          \n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00, 83.14it/s, v_num=0, train_loss_step=0.0618, train_loss_epoch=0.040, valid_loss=2.17e+3]\n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0618, train_loss_epoch=0.0618, valid_loss=2.17e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 75.81it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=2.17e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=2.17e+3]        \n",
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00, 79.16it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=2.17e+3]\n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=2.17e+3]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0515, train_loss_epoch=0.0515, valid_loss=2.17e+3]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0583, train_loss_epoch=0.0583, valid_loss=2.17e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0511, train_loss_epoch=0.0511, valid_loss=2.17e+3]        \n",
      "Epoch 261: 100%|██████████| 1/1 [00:00<00:00, 80.37it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473, valid_loss=2.17e+3]\n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473, valid_loss=2.17e+3]        \n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00, 73.78it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395, valid_loss=2.17e+3] \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395, valid_loss=2.17e+3]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0412, train_loss_epoch=0.0412, valid_loss=2.17e+3]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405, valid_loss=2.17e+3]         \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=2.17e+3]         \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=2.17e+3]\n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=2.17e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 96.49it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0317, valid_loss=2.17e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.03it/s]\u001b[A\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=1.83e+3]        \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0474, train_loss_epoch=0.0474, valid_loss=1.83e+3]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=1.83e+3]         \n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0316, train_loss_epoch=0.0316, valid_loss=1.83e+3]         \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0525, train_loss_epoch=0.0525, valid_loss=1.83e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342, valid_loss=1.83e+3]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0349, train_loss_epoch=0.0349, valid_loss=1.83e+3]        \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0327, train_loss_epoch=0.0327, valid_loss=1.83e+3]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.83e+3]           \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=1.83e+3]        \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.83e+3]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=1.83e+3]         \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.023, train_loss_epoch=0.023, valid_loss=1.83e+3]           \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.83e+3]         \n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0145, train_loss_epoch=0.0145, valid_loss=1.83e+3]         \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 97.34it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0138, valid_loss=1.83e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 166.00it/s]\u001b[A\n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.94e+3]        \n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=1.94e+3]        \n",
      "Epoch 402: 100%|██████████| 1/1 [00:00<00:00, 90.81it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.94e+3]\n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0144, train_loss_epoch=0.0144, valid_loss=1.94e+3]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=1.94e+3]        \n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=1.94e+3]         \n",
      "Epoch 424:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0595, train_loss_epoch=0.0595, valid_loss=1.94e+3]         \n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=1.94e+3]         \n",
      "Epoch 443: 100%|██████████| 1/1 [00:00<00:00, 92.98it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.94e+3] \n",
      "Epoch 444:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.94e+3]        \n",
      "Epoch 445:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=1.94e+3]        \n",
      "Epoch 445: 100%|██████████| 1/1 [00:00<00:00, 90.60it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=1.94e+3]\n",
      "Epoch 446:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0139, train_loss_epoch=0.0139, valid_loss=1.94e+3]        \n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0104, train_loss_epoch=0.0104, valid_loss=1.94e+3]        \n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0107, train_loss_epoch=0.0107, valid_loss=1.94e+3]        \n",
      "Epoch 456: 100%|██████████| 1/1 [00:00<00:00, 82.73it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.94e+3]\n",
      "Epoch 457:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0109, train_loss_epoch=0.0109, valid_loss=1.94e+3]        \n",
      "Epoch 458:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=1.94e+3]        \n",
      "Epoch 466: 100%|██████████| 1/1 [00:00<00:00, 82.51it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=1.94e+3]  \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0121, train_loss_epoch=0.0121, valid_loss=1.94e+3]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0124, train_loss_epoch=0.0124, valid_loss=1.94e+3]        \n",
      "Epoch 476: 100%|██████████| 1/1 [00:00<00:00, 83.27it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.94e+3]\n",
      "Epoch 476:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.94e+3]        \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0168, train_loss_epoch=0.0168, valid_loss=1.94e+3]\n",
      "Epoch 478:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0126, train_loss_epoch=0.0126, valid_loss=1.94e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:36,490\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=1.94e+3]          \n",
      "Epoch 495: 100%|██████████| 1/1 [00:00<00:00, 85.24it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0285, valid_loss=1.94e+3]\n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=1.94e+3]        \n",
      "Epoch 497:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0219, train_loss_epoch=0.0219, valid_loss=1.94e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498: 100%|██████████| 1/1 [00:00<00:00, 76.16it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.94e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.193]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 66.36it/s, v_num=0, train_loss_step=0.209, train_loss_epoch=0.209]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 64.74it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.209]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]        \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.215, train_loss_epoch=0.215]\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0875, train_loss_epoch=0.0875]        \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]          \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0886, train_loss_epoch=0.0886]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0939, train_loss_epoch=0.0939]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0858, train_loss_epoch=0.0858]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.076, train_loss_epoch=0.076]          \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0579, train_loss_epoch=0.0579]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418]        \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 64.19it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069]  \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069]\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 88.79it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.0361] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 154.67it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 46.40it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.0361, valid_loss=1.43e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=1.43e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0561, train_loss_epoch=0.0561, valid_loss=1.43e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.067, train_loss_epoch=0.067, valid_loss=1.43e+3]          \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.047, train_loss_epoch=0.047, valid_loss=1.43e+3]          \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.050, train_loss_epoch=0.050, valid_loss=1.43e+3]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0329, train_loss_epoch=0.0329, valid_loss=1.43e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0532, train_loss_epoch=0.0532, valid_loss=1.43e+3]        \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0272, train_loss_epoch=0.0272, valid_loss=1.43e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0399, train_loss_epoch=0.0399, valid_loss=1.43e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0309, train_loss_epoch=0.0309, valid_loss=1.43e+3]        \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=1.43e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0367, train_loss_epoch=0.0367, valid_loss=1.43e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.43e+3]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=1.43e+3]          \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0307, train_loss_epoch=0.0307, valid_loss=1.43e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0308, train_loss_epoch=0.0308, valid_loss=1.43e+3]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0634, train_loss_epoch=0.0634, valid_loss=1.43e+3]        \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487, valid_loss=1.43e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0277, train_loss_epoch=0.0277, valid_loss=1.43e+3]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0179, train_loss_epoch=0.0179, valid_loss=1.43e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 89.12it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0179, valid_loss=1.43e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.52it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 39.78it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=2.03e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0238, train_loss_epoch=0.0238, valid_loss=2.03e+3]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.03e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=2.03e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0216, train_loss_epoch=0.0216, valid_loss=2.03e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=2.03e+3]          \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.017, train_loss_epoch=0.017, valid_loss=2.03e+3]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0343, valid_loss=2.03e+3]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0302, train_loss_epoch=0.0302, valid_loss=2.03e+3]        \n",
      "Epoch 240:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0279, train_loss_epoch=0.0279, valid_loss=2.03e+3]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=2.03e+3]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=2.03e+3]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=2.03e+3]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0404, train_loss_epoch=0.0404, valid_loss=2.03e+3]        \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=2.03e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=2.03e+3]\n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0162, train_loss_epoch=0.0162, valid_loss=2.03e+3]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00999, train_loss_epoch=0.00999, valid_loss=2.03e+3]        \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.020, train_loss_epoch=0.020, valid_loss=2.03e+3]            \n",
      "Epoch 272: 100%|██████████| 1/1 [00:00<00:00, 89.09it/s, v_num=0, train_loss_step=0.00815, train_loss_epoch=0.020, valid_loss=2.03e+3]\n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00815, train_loss_epoch=0.00815, valid_loss=2.03e+3]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0106, train_loss_epoch=0.0106, valid_loss=2.03e+3]          \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0123, train_loss_epoch=0.0123, valid_loss=2.03e+3]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0141, train_loss_epoch=0.0141, valid_loss=2.03e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:40,495\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=2.03e+3]        \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0182, train_loss_epoch=0.0182, valid_loss=2.03e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 92.16it/s, v_num=0, train_loss_step=0.0146, train_loss_epoch=0.0207, valid_loss=2.03e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 178.24it/s]\u001b[A\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0163, train_loss_epoch=0.0163, valid_loss=1.7e+3]         \n",
      "Epoch 302: 100%|██████████| 1/1 [00:00<00:00, 87.14it/s, v_num=0, train_loss_step=0.0576, train_loss_epoch=0.0576, valid_loss=1.7e+3]\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0576, train_loss_epoch=0.0576, valid_loss=1.7e+3]        \n",
      "Epoch 303: 100%|██████████| 1/1 [00:00<00:00, 79.12it/s, v_num=0, train_loss_step=0.0574, train_loss_epoch=0.0574, valid_loss=1.7e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.472, train_loss_epoch=0.472]        \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.353, train_loss_epoch=0.353]       \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.342, train_loss_epoch=0.342]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.560, train_loss_epoch=0.560]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.383, train_loss_epoch=0.383]        \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 57.26it/s, v_num=0, train_loss_step=0.593, train_loss_epoch=0.593]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.322, train_loss_epoch=0.322]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.188, train_loss_epoch=0.188]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.238, train_loss_epoch=0.238]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142]        \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 69.48it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.142]\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 63.39it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]        \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]        \n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 51.30it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]        \n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 67.76it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 55.66it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.190]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 134.29it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194, valid_loss=2.77e+3]       \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.189, train_loss_epoch=0.189, valid_loss=2.77e+3]        \n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146, valid_loss=2.77e+3]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216, valid_loss=2.77e+3]        \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.188, train_loss_epoch=0.188, valid_loss=2.77e+3]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187, valid_loss=2.77e+3]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.77e+3]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.164, train_loss_epoch=0.164, valid_loss=2.77e+3]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=2.77e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.77e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114, valid_loss=2.77e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=2.77e+3]        \n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00, 53.95it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=2.77e+3]\n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=2.77e+3]        \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 69.47it/s, v_num=0, train_loss_step=0.241, train_loss_epoch=0.241, valid_loss=2.77e+3]\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=2.77e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=2.77e+3]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.143, train_loss_epoch=0.143, valid_loss=2.77e+3]        \n",
      "Epoch 178: 100%|██████████| 1/1 [00:00<00:00, 60.98it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.143, valid_loss=2.77e+3]\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122, valid_loss=2.77e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=2.77e+3]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.132, train_loss_epoch=0.132, valid_loss=2.77e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127, valid_loss=2.77e+3]        \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134, valid_loss=2.77e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 62.88it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.0917, valid_loss=2.77e+3] \n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 148.95it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 37.22it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.0917, valid_loss=2.53e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202, valid_loss=2.53e+3]         \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133, valid_loss=2.53e+3]          \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=2.53e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168, valid_loss=2.53e+3]          \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122, valid_loss=2.53e+3]          \n",
      "Epoch 228:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=2.53e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=2.53e+3]          \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.099, train_loss_epoch=0.099, valid_loss=2.53e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.53e+3]          \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0723, train_loss_epoch=0.0723, valid_loss=2.53e+3]        \n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0907, train_loss_epoch=0.0907, valid_loss=2.53e+3]        \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114, valid_loss=2.53e+3]          \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145, valid_loss=2.53e+3]          \n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.077, train_loss_epoch=0.077, valid_loss=2.53e+3]        \n",
      "Epoch 283: 100%|██████████| 1/1 [00:00<00:00, 68.19it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.53e+3]  \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.53e+3]        \n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00, 56.61it/s, v_num=0, train_loss_step=0.0679, train_loss_epoch=0.0679, valid_loss=2.53e+3]\n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0679, train_loss_epoch=0.0679, valid_loss=2.53e+3]        \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0872, train_loss_epoch=0.0872, valid_loss=2.53e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 72.05it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0816, valid_loss=2.53e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 145.78it/s]\u001b[A\n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190, valid_loss=1.96e+3]          \n",
      "Epoch 310: 100%|██████████| 1/1 [00:00<00:00, 68.87it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089, valid_loss=1.96e+3]  \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089, valid_loss=1.96e+3]        \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0887, train_loss_epoch=0.0887, valid_loss=1.96e+3]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767, valid_loss=1.96e+3]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0817, train_loss_epoch=0.0817, valid_loss=1.96e+3]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0758, train_loss_epoch=0.0758, valid_loss=1.96e+3]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0768, train_loss_epoch=0.0768, valid_loss=1.96e+3]        \n",
      "Epoch 339: 100%|██████████| 1/1 [00:00<00:00, 73.01it/s, v_num=0, train_loss_step=0.0768, train_loss_epoch=0.0768, valid_loss=1.96e+3]\n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0763, train_loss_epoch=0.0763, valid_loss=1.96e+3]        \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00, 72.85it/s, v_num=0, train_loss_step=0.0763, train_loss_epoch=0.0763, valid_loss=1.96e+3]\n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0762, train_loss_epoch=0.0762, valid_loss=1.96e+3]        \n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0731, train_loss_epoch=0.0731, valid_loss=1.96e+3]        \n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0539, train_loss_epoch=0.0539, valid_loss=1.96e+3]        \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0811, train_loss_epoch=0.0811, valid_loss=1.96e+3]        \n",
      "Epoch 367: 100%|██████████| 1/1 [00:00<00:00, 66.71it/s, v_num=0, train_loss_step=0.0724, train_loss_epoch=0.0724, valid_loss=1.96e+3]\n",
      "Epoch 368:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0724, train_loss_epoch=0.0724, valid_loss=1.96e+3]        \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0554, train_loss_epoch=0.0554, valid_loss=1.96e+3]        \n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0708, train_loss_epoch=0.0708, valid_loss=1.96e+3]        \n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0567, train_loss_epoch=0.0567, valid_loss=1.96e+3]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=1.96e+3]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0715, valid_loss=1.96e+3]        \n",
      "Epoch 388:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0715, valid_loss=1.96e+3]\n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0665, train_loss_epoch=0.0665, valid_loss=1.96e+3]        \n",
      "Epoch 399: 100%|██████████| 1/1 [00:00<00:00, 70.72it/s, v_num=0, train_loss_step=0.0635, train_loss_epoch=0.0646, valid_loss=1.96e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.27it/s]\u001b[A\n",
      "Epoch 401:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481, valid_loss=1.86e+3]        \n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545, valid_loss=1.86e+3]        \n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.052, valid_loss=1.86e+3]          \n",
      "Epoch 415: 100%|██████████| 1/1 [00:00<00:00, 53.80it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.052, valid_loss=1.86e+3]\n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0587, train_loss_epoch=0.0587, valid_loss=1.86e+3]        \n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0622, train_loss_epoch=0.0622, valid_loss=1.86e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:48,241\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429: 100%|██████████| 1/1 [00:00<00:00, 59.29it/s, v_num=0, train_loss_step=0.0581, train_loss_epoch=0.0581, valid_loss=1.86e+3]\n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=1.86e+3]          \n",
      "Epoch 435: 100%|██████████| 1/1 [00:00<00:00, 55.63it/s, v_num=0, train_loss_step=0.0655, train_loss_epoch=0.0655, valid_loss=1.86e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113]        \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 57.87it/s, v_num=0, train_loss_step=0.0947, train_loss_epoch=0.113]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0947, train_loss_epoch=0.0947]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0781, train_loss_epoch=0.0781]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274]          \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0938, train_loss_epoch=0.0938]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0712, train_loss_epoch=0.0712]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787]        \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0993, train_loss_epoch=0.0993]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333]        \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0816, train_loss_epoch=0.0816]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0453, train_loss_epoch=0.0453]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0738, train_loss_epoch=0.0738]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0651]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 72.18it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0382]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 139.37it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 39.96it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0382, valid_loss=2.3e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502, valid_loss=2.3e+3]       \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=2.3e+3]        \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.3e+3]          \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=2.3e+3]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=2.3e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0728, train_loss_epoch=0.0728, valid_loss=2.3e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684, valid_loss=2.3e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.3e+3]          \n",
      "Epoch 134: 100%|██████████| 1/1 [00:00<00:00, 78.63it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.3e+3]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0574, train_loss_epoch=0.0574, valid_loss=2.3e+3]        \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 82.49it/s, v_num=0, train_loss_step=0.0645, train_loss_epoch=0.0645, valid_loss=2.3e+3]\n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0645, train_loss_epoch=0.0645, valid_loss=2.3e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0591, valid_loss=2.3e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751, valid_loss=2.3e+3]        \n",
      "Epoch 145: 100%|██████████| 1/1 [00:00<00:00, 65.76it/s, v_num=0, train_loss_step=0.0477, train_loss_epoch=0.0477, valid_loss=2.3e+3]\n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0477, train_loss_epoch=0.0477, valid_loss=2.3e+3]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=2.3e+3]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=2.3e+3]        \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=2.3e+3]        \n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00, 83.20it/s, v_num=0, train_loss_step=0.0293, train_loss_epoch=0.0293, valid_loss=2.3e+3]\n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0293, train_loss_epoch=0.0293, valid_loss=2.3e+3]        \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.3e+3]          \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871, valid_loss=2.3e+3]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0397, valid_loss=2.3e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=2.3e+3]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0265, train_loss_epoch=0.0265, valid_loss=2.3e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0428, train_loss_epoch=0.0428, valid_loss=2.3e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=2.3e+3]        \n",
      "Epoch 192: 100%|██████████| 1/1 [00:00<00:00, 82.15it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=2.3e+3]\n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0589, train_loss_epoch=0.0589, valid_loss=2.3e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0376, train_loss_epoch=0.0376, valid_loss=2.3e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 87.72it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0439, valid_loss=2.3e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 170.84it/s]\u001b[A\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.13e+3]        \n",
      "Epoch 201: 100%|██████████| 1/1 [00:00<00:00, 76.79it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.0314, valid_loss=2.13e+3] \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.032, train_loss_epoch=0.032, valid_loss=2.13e+3]         \n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 80.55it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282, valid_loss=2.13e+3]\n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0282, train_loss_epoch=0.0282, valid_loss=2.13e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=2.13e+3]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0486, train_loss_epoch=0.0486, valid_loss=2.13e+3]        \n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0466, train_loss_epoch=0.0466, valid_loss=2.13e+3]        \n",
      "Epoch 227:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0324, train_loss_epoch=0.0324, valid_loss=2.13e+3]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483, valid_loss=2.13e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365, valid_loss=2.13e+3]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0428, train_loss_epoch=0.0428, valid_loss=2.13e+3]        \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0792, train_loss_epoch=0.0792, valid_loss=2.13e+3]        \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0415, train_loss_epoch=0.0415, valid_loss=2.13e+3]        \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0524, train_loss_epoch=0.0524, valid_loss=2.13e+3]        \n",
      "Epoch 258: 100%|██████████| 1/1 [00:00<00:00, 84.37it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0524, valid_loss=2.13e+3]\n",
      "Epoch 259:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0398, valid_loss=2.13e+3]        \n",
      "Epoch 259: 100%|██████████| 1/1 [00:00<00:00, 80.72it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=2.13e+3]  \n",
      "Epoch 260:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=2.13e+3]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0245, train_loss_epoch=0.0245, valid_loss=2.13e+3]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0217, train_loss_epoch=0.0217, valid_loss=2.13e+3]        \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0613, train_loss_epoch=0.0613, valid_loss=2.13e+3]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.13e+3]        \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 68.35it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.13e+3]\n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.13e+3]        \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=2.13e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0454, train_loss_epoch=0.0454, valid_loss=2.13e+3]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=2.13e+3]        \n",
      "Epoch 293: 100%|██████████| 1/1 [00:00<00:00, 59.50it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=2.13e+3]\n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=2.13e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 71.37it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0386, valid_loss=2.13e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 149.03it/s]\u001b[A\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=928.0]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:52,997\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0351, train_loss_epoch=0.0351, valid_loss=928.0]        \n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0299, valid_loss=928.0]        \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=928.0]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=928.0]          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314: 100%|██████████| 1/1 [00:00<00:00, 54.09it/s, v_num=0, train_loss_step=0.0175, train_loss_epoch=0.0175, valid_loss=928.0]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 99.89it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.314]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]         \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0824, train_loss_epoch=0.0824]         \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 95.06it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]   \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.265, train_loss_epoch=0.265]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.158, train_loss_epoch=0.158]          \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.158, train_loss_epoch=0.158]        \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0623, train_loss_epoch=0.0623]         \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 95.55it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0672] \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0672]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0805, train_loss_epoch=0.0805]        \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 95.06it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646] \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0646, train_loss_epoch=0.0646]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0862, train_loss_epoch=0.0862]        \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 93.69it/s, v_num=0, train_loss_step=0.0729, train_loss_epoch=0.0729]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0729, train_loss_epoch=0.0729]        \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]           \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 98.80it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.105]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0638, train_loss_epoch=0.0638]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.062, train_loss_epoch=0.062]          \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599]\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0453, train_loss_epoch=0.0453]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:54,528\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 99.12it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0556] \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.070, train_loss_epoch=0.070]           \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 97.90it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.070]\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 165.97it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 51.40it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.070, valid_loss=1.49e+3]\n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 38.80it/s, v_num=0, train_loss_step=0.0401, train_loss_epoch=0.0401, valid_loss=1.49e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]        \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 84.71it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.114]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]        \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0821, train_loss_epoch=0.0821]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 68.23it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0974, train_loss_epoch=0.0974]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]          \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0847]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0847, train_loss_epoch=0.0847]\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0715, train_loss_epoch=0.0715]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0979, train_loss_epoch=0.0979]        \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.133, train_loss_epoch=0.133]          \n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.044, train_loss_epoch=0.044]          \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030]          \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 88.08it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0455]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 144.46it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 44.35it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0455, valid_loss=2.72e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0672, valid_loss=2.72e+3]       \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0762, train_loss_epoch=0.0762, valid_loss=2.72e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0843, train_loss_epoch=0.0843, valid_loss=2.72e+3]        \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 89.10it/s, v_num=0, train_loss_step=0.0411, train_loss_epoch=0.0411, valid_loss=2.72e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=2.72e+3]        \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 81.18it/s, v_num=0, train_loss_step=0.0689, train_loss_epoch=0.0689, valid_loss=2.72e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0689, train_loss_epoch=0.0689, valid_loss=2.72e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0437, train_loss_epoch=0.0437, valid_loss=2.72e+3]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0699, train_loss_epoch=0.0699, valid_loss=2.72e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=2.72e+3]          \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389, valid_loss=2.72e+3]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389, valid_loss=2.72e+3]\n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0539, train_loss_epoch=0.0539, valid_loss=2.72e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0427, train_loss_epoch=0.0427, valid_loss=2.72e+3]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 86.30it/s, v_num=0, train_loss_step=0.0513, train_loss_epoch=0.0619, valid_loss=2.72e+3]\n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0513, train_loss_epoch=0.0513, valid_loss=2.72e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0739, train_loss_epoch=0.0739, valid_loss=2.72e+3]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=2.72e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.061, train_loss_epoch=0.061, valid_loss=2.72e+3]          \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0264, train_loss_epoch=0.0264, valid_loss=2.72e+3]        \n",
      "Epoch 176:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=2.72e+3]        \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=2.72e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.72e+3]        \n",
      "Epoch 185: 100%|██████████| 1/1 [00:00<00:00, 85.24it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0421, valid_loss=2.72e+3]\n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0297, train_loss_epoch=0.0297, valid_loss=2.72e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.72e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=2.72e+3]        \n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0947, train_loss_epoch=0.0947, valid_loss=2.72e+3]        \n",
      "Epoch 196: 100%|██████████| 1/1 [00:00<00:00, 81.94it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=2.72e+3]\n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=2.72e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.72e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 75.20it/s, v_num=0, train_loss_step=0.064, train_loss_epoch=0.0208, valid_loss=2.72e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 142.76it/s]\u001b[A\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.069, train_loss_epoch=0.069, valid_loss=1.63e+3]          \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.63e+3]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0166, train_loss_epoch=0.0166, valid_loss=1.63e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0308, train_loss_epoch=0.0308, valid_loss=1.63e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0371, train_loss_epoch=0.0371, valid_loss=1.63e+3]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=1.63e+3]        \n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=1.63e+3]        \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 83.33it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=1.63e+3]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=1.63e+3]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.027, train_loss_epoch=0.027, valid_loss=1.63e+3]          \n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=1.63e+3]        \n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.016, train_loss_epoch=0.016, valid_loss=1.63e+3]          \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462, valid_loss=1.63e+3]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0192, train_loss_epoch=0.0192, valid_loss=1.63e+3]        \n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 59.14it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0192, valid_loss=1.63e+3]\n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 57.13it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0312, valid_loss=1.63e+3]\n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0312, train_loss_epoch=0.0312, valid_loss=1.63e+3]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=1.63e+3]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0359, train_loss_epoch=0.0359, valid_loss=1.63e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 89.11it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0367, valid_loss=1.63e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 159.18it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 45.57it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0367, valid_loss=1.62e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.62e+3]        \n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0596, train_loss_epoch=0.0596, valid_loss=1.62e+3]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0416, train_loss_epoch=0.0416, valid_loss=1.62e+3]        \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.62e+3]        \n",
      "Epoch 327:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0241, train_loss_epoch=0.0241, valid_loss=1.62e+3]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=1.62e+3]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=1.62e+3]          \n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0358, train_loss_epoch=0.0358, valid_loss=1.62e+3]        \n",
      "Epoch 337: 100%|██████████| 1/1 [00:00<00:00, 76.78it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.62e+3]\n",
      "Epoch 338:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.62e+3]        \n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=1.62e+3]        \n",
      "Epoch 347: 100%|██████████| 1/1 [00:00<00:00, 85.20it/s, v_num=0, train_loss_step=0.0917, train_loss_epoch=0.0321, valid_loss=1.62e+3]\n",
      "Epoch 348:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0917, train_loss_epoch=0.0917, valid_loss=1.62e+3]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0248, train_loss_epoch=0.0248, valid_loss=1.62e+3]        \n",
      "Epoch 357: 100%|██████████| 1/1 [00:00<00:00, 85.18it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.0715, valid_loss=1.62e+3] \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.012, train_loss_epoch=0.012, valid_loss=1.62e+3]         \n",
      "Epoch 359:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0102, train_loss_epoch=0.0102, valid_loss=1.62e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:49:59,769\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367: 100%|██████████| 1/1 [00:00<00:00, 83.54it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.62e+3]\n",
      "Epoch 367: 100%|██████████| 1/1 [00:00<00:00, 76.46it/s, v_num=0, train_loss_step=0.0131, train_loss_epoch=0.0131, valid_loss=1.62e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]         \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 98.77it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153] \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.232, train_loss_epoch=0.232]           \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]         \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0797, train_loss_epoch=0.0797]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]           \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089]           \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 104.15it/s, v_num=0, train_loss_step=0.089, train_loss_epoch=0.089]\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0476, train_loss_epoch=0.0476]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0501, train_loss_epoch=0.0501]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:01,078\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0604, train_loss_epoch=0.0604]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]           \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 104.73it/s, v_num=0, train_loss_step=0.066, train_loss_epoch=0.0547] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 167.07it/s]\u001b[A\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.219, valid_loss=3.07e+3]           \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121, valid_loss=3.07e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: 100%|██████████| 1/1 [00:00<00:00, 86.99it/s, v_num=0, train_loss_step=0.0825, train_loss_epoch=0.0825, valid_loss=3.07e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.314, train_loss_epoch=0.314]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.185, train_loss_epoch=0.185]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.396, train_loss_epoch=0.396]        \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 86.32it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.396]\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.286, train_loss_epoch=0.286]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.254, train_loss_epoch=0.254]        \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 90.16it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137]        \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 90.22it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971]        \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 87.55it/s, v_num=0, train_loss_step=0.0835, train_loss_epoch=0.0835]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0835, train_loss_epoch=0.0835]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0799, train_loss_epoch=0.0799]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:02,222\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 94.53it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.183] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 158.71it/s]\u001b[A\n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 37.32it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642, valid_loss=3e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0989, train_loss_epoch=0.0989]         \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.219, train_loss_epoch=0.219]           \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]           \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 110.04it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]  \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0933, train_loss_epoch=0.0933]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0863, train_loss_epoch=0.0863]         \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584]         \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585]         \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871]         \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 107.00it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 111.18it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.0429] \n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 171.16it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 53.69it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.0429, valid_loss=2.11e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.11e+3]        \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 108.01it/s, v_num=0, train_loss_step=0.0528, train_loss_epoch=0.109, valid_loss=2.11e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0528, train_loss_epoch=0.0528, valid_loss=2.11e+3]         \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0437, train_loss_epoch=0.0437, valid_loss=2.11e+3]         \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.092, train_loss_epoch=0.092, valid_loss=2.11e+3]           \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 110.42it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.092, valid_loss=2.11e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=2.11e+3]         \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0437, train_loss_epoch=0.0437, valid_loss=2.11e+3]         \n",
      "Epoch 115: 100%|██████████| 1/1 [00:00<00:00, 109.62it/s, v_num=0, train_loss_step=0.0437, train_loss_epoch=0.0437, valid_loss=2.11e+3]\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0416, train_loss_epoch=0.0416, valid_loss=2.11e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0531, train_loss_epoch=0.0531, valid_loss=2.11e+3]         \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0924, train_loss_epoch=0.0924, valid_loss=2.11e+3]         \n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0461, train_loss_epoch=0.0461, valid_loss=2.11e+3]         \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0745, train_loss_epoch=0.0745, valid_loss=2.11e+3]         \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0539, train_loss_epoch=0.0539, valid_loss=2.11e+3]         \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 108.23it/s, v_num=0, train_loss_step=0.0374, train_loss_epoch=0.0374, valid_loss=2.11e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0374, train_loss_epoch=0.0374, valid_loss=2.11e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=2.11e+3]         \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 106.04it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.11e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0344, train_loss_epoch=0.0344, valid_loss=2.11e+3]         \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0362, train_loss_epoch=0.0362, valid_loss=2.11e+3]         \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 106.09it/s, v_num=0, train_loss_step=0.0346, train_loss_epoch=0.0346, valid_loss=2.11e+3]\n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0346, train_loss_epoch=0.0346, valid_loss=2.11e+3]         \n",
      "Epoch 146:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0432, train_loss_epoch=0.0432, valid_loss=2.11e+3]         \n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 108.39it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.11e+3]  \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.11e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0475, train_loss_epoch=0.0475, valid_loss=2.11e+3]         \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 107.50it/s, v_num=0, train_loss_step=0.0428, train_loss_epoch=0.0428, valid_loss=2.11e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0428, train_loss_epoch=0.0428, valid_loss=2.11e+3]         \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815, valid_loss=2.11e+3]         \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 116.05it/s, v_num=0, train_loss_step=0.0815, train_loss_epoch=0.0815, valid_loss=2.11e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=2.11e+3]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=2.11e+3]         \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271, valid_loss=2.11e+3]         \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042, valid_loss=2.11e+3]           \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 116.77it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042, valid_loss=2.11e+3]\n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0505, train_loss_epoch=0.0505, valid_loss=2.11e+3]         \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=2.11e+3]         \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0492, train_loss_epoch=0.0492, valid_loss=2.11e+3]         \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0628, train_loss_epoch=0.0628, valid_loss=2.11e+3]         \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=2.11e+3]         \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=2.11e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 113.20it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0183, valid_loss=2.11e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 196.98it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 57.21it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0183, valid_loss=1.96e+3] \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=1.96e+3]        \n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=1.96e+3]         \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=1.96e+3]         \n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.96e+3]           \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0288, train_loss_epoch=0.0288, valid_loss=1.96e+3]         \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=1.96e+3]           \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0233, train_loss_epoch=0.0233, valid_loss=1.96e+3]         \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0258, train_loss_epoch=0.0258, valid_loss=1.96e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0172, train_loss_epoch=0.0172, valid_loss=1.96e+3]         \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0157, train_loss_epoch=0.0157, valid_loss=1.96e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:05,031\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00, 99.42it/s, v_num=0, train_loss_step=0.0255, train_loss_epoch=0.0255, valid_loss=1.96e+3] \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.421, train_loss_epoch=0.421]        \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 94.66it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.178]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]        \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 94.65it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.132]\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 92.59it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187]   \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0911, train_loss_epoch=0.0911]        \n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 93.25it/s, v_num=0, train_loss_step=0.0811, train_loss_epoch=0.0811]\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0811, train_loss_epoch=0.0811]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]          \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 95.36it/s, v_num=0, train_loss_step=0.0838, train_loss_epoch=0.183]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0838, train_loss_epoch=0.0838]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0816, train_loss_epoch=0.0816]        \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153]           \n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 97.95it/s, v_num=0, train_loss_step=0.0969, train_loss_epoch=0.153]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 94.40it/s, v_num=0, train_loss_step=0.0969, train_loss_epoch=0.0969]\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0969, train_loss_epoch=0.0969]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]          \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0634, train_loss_epoch=0.0634]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0597, train_loss_epoch=0.0597]         \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 97.35it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0597]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409]        \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0726, train_loss_epoch=0.0726]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 94.33it/s, v_num=0, train_loss_step=0.0968, train_loss_epoch=0.0968] \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0968, train_loss_epoch=0.0968]        \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642]        \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 93.13it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368]         \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 93.75it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042]          \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 92.93it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 97.36it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0325]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.58it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 49.22it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0325, valid_loss=1.81e+3]\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 38.26it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0668, valid_loss=1.81e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0668, train_loss_epoch=0.0668, valid_loss=1.81e+3]       \n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=1.81e+3]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0418, train_loss_epoch=0.0418, valid_loss=1.81e+3]         \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 98.07it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0418, valid_loss=1.81e+3] \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0289, train_loss_epoch=0.0289, valid_loss=1.81e+3]        \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0698, train_loss_epoch=0.0698, valid_loss=1.81e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331, valid_loss=1.81e+3]         \n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 92.63it/s, v_num=0, train_loss_step=0.064, train_loss_epoch=0.064, valid_loss=1.81e+3]  \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.064, train_loss_epoch=0.064, valid_loss=1.81e+3]        \n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.076, train_loss_epoch=0.076, valid_loss=1.81e+3]        \n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 99.73it/s, v_num=0, train_loss_step=0.076, train_loss_epoch=0.076, valid_loss=1.81e+3]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0318, train_loss_epoch=0.0318, valid_loss=1.81e+3]        \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0266, train_loss_epoch=0.0266, valid_loss=1.81e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0545, train_loss_epoch=0.0545, valid_loss=1.81e+3]         \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462, valid_loss=1.81e+3]         \n",
      "Epoch 147: 100%|██████████| 1/1 [00:00<00:00, 96.77it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0462, valid_loss=1.81e+3]\n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0714, valid_loss=1.81e+3]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0381, train_loss_epoch=0.0381, valid_loss=1.81e+3]         \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.056, train_loss_epoch=0.056, valid_loss=1.81e+3]           \n",
      "Epoch 159: 100%|██████████| 1/1 [00:00<00:00, 97.12it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.056, valid_loss=1.81e+3]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483, valid_loss=1.81e+3]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0286, train_loss_epoch=0.0286, valid_loss=1.81e+3]         \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447, valid_loss=1.81e+3]         \n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00, 94.42it/s, v_num=0, train_loss_step=0.0588, train_loss_epoch=0.0588, valid_loss=1.81e+3] \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0588, train_loss_epoch=0.0588, valid_loss=1.81e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0994, train_loss_epoch=0.0994, valid_loss=1.81e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0365, train_loss_epoch=0.0365, valid_loss=1.81e+3]        \n",
      "Epoch 183:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0275, train_loss_epoch=0.0275, valid_loss=1.81e+3]        \n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 80.86it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=1.81e+3]\n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=1.81e+3]        \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.81e+3]        \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0315, train_loss_epoch=0.0315, valid_loss=1.81e+3]         \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0367, train_loss_epoch=0.0367, valid_loss=1.81e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 99.99it/s, v_num=0, train_loss_step=0.0504, train_loss_epoch=0.0269, valid_loss=1.81e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 187.03it/s]\u001b[A\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0361, train_loss_epoch=0.0361, valid_loss=1.93e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.93e+3]           \n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00, 98.10it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=1.93e+3]\n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=1.93e+3]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0347, train_loss_epoch=0.0347, valid_loss=1.93e+3]        \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=1.93e+3]           \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=1.93e+3]         \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=1.93e+3]         \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0335, train_loss_epoch=0.0335, valid_loss=1.93e+3]         \n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0731, train_loss_epoch=0.0731, valid_loss=1.93e+3]           \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=1.93e+3]         \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=1.93e+3]        \n",
      "Epoch 285:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0301, train_loss_epoch=0.0301, valid_loss=1.93e+3]        \n",
      "Epoch 285: 100%|██████████| 1/1 [00:00<00:00, 79.52it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.93e+3]\n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0222, train_loss_epoch=0.0222, valid_loss=1.93e+3]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0244, train_loss_epoch=0.0244, valid_loss=1.93e+3]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602, valid_loss=1.93e+3]        \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=1.93e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 83.69it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0391, valid_loss=1.93e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 161.74it/s]\u001b[A\n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=1.81e+3]        \n",
      "Epoch 304:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0302, train_loss_epoch=0.0302, valid_loss=1.81e+3]        \n",
      "Epoch 304: 100%|██████████| 1/1 [00:00<00:00, 89.56it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.81e+3]\n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.81e+3]        \n",
      "Epoch 306:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=1.81e+3]        \n",
      "Epoch 306: 100%|██████████| 1/1 [00:00<00:00, 96.39it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0191, valid_loss=1.81e+3]\n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0208, train_loss_epoch=0.0208, valid_loss=1.81e+3]        \n",
      "Epoch 308:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0432, train_loss_epoch=0.0432, valid_loss=1.81e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:09,004\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0446, valid_loss=1.81e+3]         \n",
      "Epoch 320: 100%|██████████| 1/1 [00:00<00:00, 86.68it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442, valid_loss=1.81e+3] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.540, train_loss_epoch=0.540]         \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.508, train_loss_epoch=0.508]         \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.653, train_loss_epoch=0.653]         \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 97.85it/s, v_num=0, train_loss_step=0.330, train_loss_epoch=0.330] \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.330, train_loss_epoch=0.330]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.538, train_loss_epoch=0.538]         \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 103.73it/s, v_num=0, train_loss_step=0.538, train_loss_epoch=0.538]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.404, train_loss_epoch=0.404]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.456, train_loss_epoch=0.456]         \n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 107.35it/s, v_num=0, train_loss_step=0.296, train_loss_epoch=0.296]\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.289, train_loss_epoch=0.289]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.325, train_loss_epoch=0.325]         \n",
      "Epoch 31: 100%|██████████| 1/1 [00:00<00:00, 99.72it/s, v_num=0, train_loss_step=0.320, train_loss_epoch=0.320] \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.320, train_loss_epoch=0.320]        \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.364, train_loss_epoch=0.364]         \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 99.54it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.364] \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.291, train_loss_epoch=0.291]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.302, train_loss_epoch=0.302]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.244, train_loss_epoch=0.244]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.253, train_loss_epoch=0.253]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.270, train_loss_epoch=0.270]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.263, train_loss_epoch=0.263]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.204, train_loss_epoch=0.204]         \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.239, train_loss_epoch=0.239]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.223, train_loss_epoch=0.223]         \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 101.50it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]         \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.188, train_loss_epoch=0.188]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151]         \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 99.18it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.151] \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 106.44it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.236, train_loss_epoch=0.236]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.211, train_loss_epoch=0.211]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.193, train_loss_epoch=0.193]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 102.36it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.223]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.91it/s]\u001b[A\n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 94.07it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=1.78e+3] \n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=1.78e+3]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160, valid_loss=1.78e+3]         \n",
      "Epoch 105: 100%|██████████| 1/1 [00:00<00:00, 90.69it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.160, valid_loss=1.78e+3]\n",
      "Epoch 105: 100%|██████████| 1/1 [00:00<00:00, 87.51it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=1.78e+3]\n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.162, train_loss_epoch=0.162, valid_loss=1.78e+3]        \n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160, valid_loss=1.78e+3]         \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161, valid_loss=1.78e+3]         \n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 81.53it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149, valid_loss=1.78e+3] \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=1.78e+3]        \n",
      "Epoch 127: 100%|██████████| 1/1 [00:00<00:00, 81.22it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172, valid_loss=1.78e+3]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.172, train_loss_epoch=0.172, valid_loss=1.78e+3]        \n",
      "Epoch 128: 100%|██████████| 1/1 [00:00<00:00, 82.96it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.172, valid_loss=1.78e+3]\n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142, valid_loss=1.78e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=1.78e+3]        \n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 82.93it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=1.78e+3]\n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=1.78e+3]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.153, valid_loss=1.78e+3]         \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178, valid_loss=1.78e+3]         \n",
      "Epoch 157: 100%|██████████| 1/1 [00:00<00:00, 86.24it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=1.78e+3] \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=1.78e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130, valid_loss=1.78e+3]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118, valid_loss=1.78e+3]         \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=1.78e+3]         \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 83.65it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129, valid_loss=1.78e+3]\n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129, valid_loss=1.78e+3]        \n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=1.78e+3]        \n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=1.78e+3]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115, valid_loss=1.78e+3]        \n",
      "Epoch 180: 100%|██████████| 1/1 [00:00<00:00, 83.75it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.115, valid_loss=1.78e+3]\n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=1.78e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=1.78e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128, valid_loss=1.78e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=1.78e+3]        \n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0857, train_loss_epoch=0.0857, valid_loss=1.78e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 88.07it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.0857, valid_loss=1.78e+3] \n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.36it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 46.59it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.0857, valid_loss=2.49e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121, valid_loss=2.49e+3]         \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0986, train_loss_epoch=0.0986, valid_loss=2.49e+3]        \n",
      "Epoch 208: 100%|██████████| 1/1 [00:00<00:00, 78.75it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.0986, valid_loss=2.49e+3] \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.100, train_loss_epoch=0.100, valid_loss=2.49e+3]         \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.49e+3]          \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108, valid_loss=2.49e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:11,892\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0817, train_loss_epoch=0.0817, valid_loss=2.49e+3]         \n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 73.86it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.49e+3]  \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.130, train_loss_epoch=0.130]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]        \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.271, train_loss_epoch=0.271]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]        \n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.247, train_loss_epoch=0.247]          \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]          \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0776, train_loss_epoch=0.0776]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0968, train_loss_epoch=0.0968]         \n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00, 101.78it/s, v_num=0, train_loss_step=0.074, train_loss_epoch=0.074]  \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.074, train_loss_epoch=0.074]         \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0951, train_loss_epoch=0.0951]         \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 106.79it/s, v_num=0, train_loss_step=0.0936, train_loss_epoch=0.0951]\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0936, train_loss_epoch=0.0936]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]           \n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 108.97it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0794, train_loss_epoch=0.0794]         \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0765, train_loss_epoch=0.0765]         \n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 102.95it/s, v_num=0, train_loss_step=0.0616, train_loss_epoch=0.0616]\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0616, train_loss_epoch=0.0616]         \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0442, train_loss_epoch=0.0442]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 100.75it/s, v_num=0, train_loss_step=0.0868, train_loss_epoch=0.0868]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0868, train_loss_epoch=0.0868]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0889, train_loss_epoch=0.0889]         \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 102.18it/s, v_num=0, train_loss_step=0.0476, train_loss_epoch=0.0476]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0476, train_loss_epoch=0.0476]         \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424]         \n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 107.26it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0424]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0644, train_loss_epoch=0.0644]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0548, train_loss_epoch=0.0548]         \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 106.56it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0548]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0467, train_loss_epoch=0.0467]         \n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 102.81it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642]\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0642, train_loss_epoch=0.0642]         \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0767, train_loss_epoch=0.0767]         \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 101.90it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]  \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.055, train_loss_epoch=0.055]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 94.21it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.059]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 150.32it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0691, train_loss_epoch=0.0691, valid_loss=1.74e+3]         \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 105.34it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0486, valid_loss=1.74e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=1.74e+3]         \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0472, train_loss_epoch=0.0472, valid_loss=1.74e+3]         \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 106.34it/s, v_num=0, train_loss_step=0.0493, train_loss_epoch=0.0472, valid_loss=1.74e+3]\n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0493, train_loss_epoch=0.0493, valid_loss=1.74e+3]         \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=1.74e+3]         \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436, valid_loss=1.74e+3]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0446, valid_loss=1.74e+3]         \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0393, train_loss_epoch=0.0393, valid_loss=1.74e+3]         \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0411, train_loss_epoch=0.0411, valid_loss=1.74e+3]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0249, train_loss_epoch=0.0249, valid_loss=1.74e+3]         \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0237, train_loss_epoch=0.0237, valid_loss=1.74e+3]         \n",
      "Epoch 156:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126, valid_loss=1.74e+3]           \n",
      "Epoch 166: 100%|██████████| 1/1 [00:00<00:00, 101.26it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.74e+3]\n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.74e+3]         \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.74e+3]         \n",
      "Epoch 168: 100%|██████████| 1/1 [00:00<00:00, 102.35it/s, v_num=0, train_loss_step=0.0364, train_loss_epoch=0.0364, valid_loss=1.74e+3]\n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0364, train_loss_epoch=0.0364, valid_loss=1.74e+3]         \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0451, train_loss_epoch=0.0451, valid_loss=1.74e+3]         \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 106.22it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0451, valid_loss=1.74e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0413, train_loss_epoch=0.0413, valid_loss=1.74e+3]         \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.74e+3]           \n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00, 108.72it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.74e+3]\n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0449, train_loss_epoch=0.0449, valid_loss=1.74e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0522, valid_loss=1.74e+3]         \n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00, 102.13it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=1.74e+3]\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=1.74e+3]         \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=1.74e+3]         \n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 102.02it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=1.74e+3]  \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=1.74e+3]         \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0201, train_loss_epoch=0.0201, valid_loss=1.74e+3]         \n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 105.84it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0201, valid_loss=1.74e+3]\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669, valid_loss=1.74e+3]         \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0155, train_loss_epoch=0.0155, valid_loss=1.74e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 105.60it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0106, valid_loss=1.74e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 189.69it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 54.17it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0106, valid_loss=1.92e+3] \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0384, train_loss_epoch=0.0384, valid_loss=1.92e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0317, train_loss_epoch=0.0317, valid_loss=1.92e+3]         \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=1.92e+3]         \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.92e+3]         \n",
      "Epoch 212: 100%|██████████| 1/1 [00:00<00:00, 106.51it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0247, valid_loss=1.92e+3]\n",
      "Epoch 213:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0214, train_loss_epoch=0.0214, valid_loss=1.92e+3]         \n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0228, train_loss_epoch=0.0228, valid_loss=1.92e+3]         \n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00, 101.49it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.92e+3]\n",
      "Epoch 215:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0263, train_loss_epoch=0.0263, valid_loss=1.92e+3]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0303, train_loss_epoch=0.0303, valid_loss=1.92e+3]         \n",
      "Epoch 216: 100%|██████████| 1/1 [00:00<00:00, 99.82it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.92e+3] \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=1.92e+3]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=1.92e+3]        \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 105.67it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=1.92e+3]\n",
      "Epoch 219:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=1.92e+3]         \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0207, train_loss_epoch=0.0207, valid_loss=1.92e+3]         \n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 102.34it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=1.92e+3]\n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0199, train_loss_epoch=0.0199, valid_loss=1.92e+3]         \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=1.92e+3]           \n",
      "Epoch 222: 100%|██████████| 1/1 [00:00<00:00, 101.76it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.92e+3]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.92e+3]         \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.018, train_loss_epoch=0.018, valid_loss=1.92e+3]           \n",
      "Epoch 224: 100%|██████████| 1/1 [00:00<00:00, 105.39it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.018, valid_loss=1.92e+3]\n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0184, train_loss_epoch=0.0184, valid_loss=1.92e+3]         \n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00, 106.20it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0184, valid_loss=1.92e+3]\n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0129, train_loss_epoch=0.0129, valid_loss=1.92e+3]         \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0125, train_loss_epoch=0.0125, valid_loss=1.92e+3]         \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=1.92e+3]         \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0122, train_loss_epoch=0.0122, valid_loss=1.92e+3]         \n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.00986, train_loss_epoch=0.00986, valid_loss=1.92e+3]         \n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00, 101.48it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=1.92e+3]  \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0189, train_loss_epoch=0.0189, valid_loss=1.92e+3]         \n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=1.92e+3]         \n",
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00, 107.81it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=1.92e+3]\n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0167, train_loss_epoch=0.0167, valid_loss=1.92e+3]         \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=1.92e+3]         \n",
      "Epoch 252: 100%|██████████| 1/1 [00:00<00:00, 106.30it/s, v_num=0, train_loss_step=0.022, train_loss_epoch=0.0285, valid_loss=1.92e+3] \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.022, train_loss_epoch=0.022, valid_loss=1.92e+3]          \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=1.92e+3]         \n",
      "Epoch 254: 100%|██████████| 1/1 [00:00<00:00, 102.24it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.92e+3]\n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0247, train_loss_epoch=0.0247, valid_loss=1.92e+3]         \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0262, train_loss_epoch=0.0262, valid_loss=1.92e+3]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 106.51it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0227, valid_loss=1.92e+3]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=1.92e+3]         \n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0325, train_loss_epoch=0.0325, valid_loss=1.92e+3]         \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 101.39it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.92e+3]  \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.028, train_loss_epoch=0.028, valid_loss=1.92e+3]         \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=1.92e+3]         \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=1.92e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:15,280\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116, valid_loss=1.92e+3]           \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0209, train_loss_epoch=0.0209, valid_loss=1.92e+3]         \n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.085, train_loss_epoch=0.085, valid_loss=1.92e+3]           \n",
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00, 106.35it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.085, valid_loss=1.92e+3]\n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0191, train_loss_epoch=0.0191, valid_loss=1.92e+3]         \n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0373, train_loss_epoch=0.0373, valid_loss=1.92e+3]        \n",
      "Epoch 293: 100%|██████████| 1/1 [00:00<00:00, 94.50it/s, v_num=0, train_loss_step=0.0239, train_loss_epoch=0.0239, valid_loss=1.92e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 110.19it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.280, train_loss_epoch=0.280]         \n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]         \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.221, train_loss_epoch=0.221]           \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.091, train_loss_epoch=0.091]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:16,072\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 115.52it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.106]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 198.04it/s]\u001b[A\n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 43.92it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137, valid_loss=2.55e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]        \n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 89.60it/s, v_num=0, train_loss_step=0.317, train_loss_epoch=0.317]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.336, train_loss_epoch=0.336]        \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.274, train_loss_epoch=0.274]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.333, train_loss_epoch=0.333]        \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 88.81it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]        \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.363, train_loss_epoch=0.363]\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.218, train_loss_epoch=0.218]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.192, train_loss_epoch=0.192]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.258, train_loss_epoch=0.258]        \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.225, train_loss_epoch=0.225]        \n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00, 86.26it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.225]\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.207, train_loss_epoch=0.207]        \n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.315, train_loss_epoch=0.315]        \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]        \n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144]        \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123]        \n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]        \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 94.35it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.105]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 160.18it/s]\u001b[A\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140, valid_loss=1.31e+3]       \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=1.31e+3]          \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0927, train_loss_epoch=0.0927, valid_loss=1.31e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 88.11it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793, valid_loss=1.31e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0793, train_loss_epoch=0.0793, valid_loss=1.31e+3]        \n",
      "Epoch 111: 100%|██████████| 1/1 [00:00<00:00, 76.52it/s, v_num=0, train_loss_step=0.0984, train_loss_epoch=0.0984, valid_loss=1.31e+3]\n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0984, train_loss_epoch=0.0984, valid_loss=1.31e+3]        \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142, valid_loss=1.31e+3]          \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142, valid_loss=1.31e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0759, train_loss_epoch=0.0759, valid_loss=1.31e+3]        \n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0938, train_loss_epoch=0.0938, valid_loss=1.31e+3]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0903, train_loss_epoch=0.0903, valid_loss=1.31e+3]        \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0783, train_loss_epoch=0.0783, valid_loss=1.31e+3]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0948, train_loss_epoch=0.0948, valid_loss=1.31e+3]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0871, train_loss_epoch=0.0871, valid_loss=1.31e+3]        \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0672, valid_loss=1.31e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0665, train_loss_epoch=0.0665, valid_loss=1.31e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0581, train_loss_epoch=0.0581, valid_loss=1.31e+3]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836, valid_loss=1.31e+3]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0707, train_loss_epoch=0.0707, valid_loss=1.31e+3]        \n",
      "Epoch 178:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0922, valid_loss=1.31e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0594, train_loss_epoch=0.0594, valid_loss=1.31e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0683, train_loss_epoch=0.0683, valid_loss=1.31e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602, valid_loss=1.31e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=1.31e+3]        \n",
      "Epoch 198: 100%|██████████| 1/1 [00:00<00:00, 62.82it/s, v_num=0, train_loss_step=0.0515, train_loss_epoch=0.0515, valid_loss=1.31e+3]\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0515, train_loss_epoch=0.0515, valid_loss=1.31e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 66.44it/s, v_num=0, train_loss_step=0.0937, train_loss_epoch=0.0515, valid_loss=1.31e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 130.33it/s]\u001b[A\n",
      "                                                                       \u001b[A\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0937, train_loss_epoch=0.0937, valid_loss=2.57e+3]        \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0937, train_loss_epoch=0.0937, valid_loss=2.57e+3]\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0478, train_loss_epoch=0.0478, valid_loss=2.57e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0936, train_loss_epoch=0.0936, valid_loss=2.57e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0678, train_loss_epoch=0.0678, valid_loss=2.57e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0491, train_loss_epoch=0.0491, valid_loss=2.57e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607, valid_loss=2.57e+3]        \n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 65.12it/s, v_num=0, train_loss_step=0.0516, train_loss_epoch=0.0607, valid_loss=2.57e+3]\n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0516, train_loss_epoch=0.0516, valid_loss=2.57e+3]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0574, train_loss_epoch=0.0574, valid_loss=2.57e+3]        \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0498, train_loss_epoch=0.0498, valid_loss=2.57e+3]        \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0474, train_loss_epoch=0.0474, valid_loss=2.57e+3]        \n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0488, train_loss_epoch=0.0488, valid_loss=2.57e+3]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0426, train_loss_epoch=0.0426, valid_loss=2.57e+3]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0333, train_loss_epoch=0.0333, valid_loss=2.57e+3]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.57e+3]          \n",
      "Epoch 252: 100%|██████████| 1/1 [00:00<00:00, 78.83it/s, v_num=0, train_loss_step=0.0704, train_loss_epoch=0.0704, valid_loss=2.57e+3]\n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0704, train_loss_epoch=0.0704, valid_loss=2.57e+3]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0657, train_loss_epoch=0.0657, valid_loss=2.57e+3]        \n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 61.05it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=2.57e+3]\n",
      "Epoch 263:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=2.57e+3]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575, valid_loss=2.57e+3]        \n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0694, train_loss_epoch=0.0694, valid_loss=2.57e+3]        \n",
      "Epoch 273: 100%|██████████| 1/1 [00:00<00:00, 65.93it/s, v_num=0, train_loss_step=0.0635, train_loss_epoch=0.0635, valid_loss=2.57e+3]\n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0635, train_loss_epoch=0.0635, valid_loss=2.57e+3]        \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0666, train_loss_epoch=0.0666, valid_loss=2.57e+3]        \n",
      "Epoch 292:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0499, train_loss_epoch=0.0499, valid_loss=2.57e+3]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=2.57e+3]        \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 92.98it/s, v_num=0, train_loss_step=0.052, train_loss_epoch=0.0594, valid_loss=2.57e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.78it/s]\u001b[A\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0462, train_loss_epoch=0.0462, valid_loss=2.11e+3]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447, valid_loss=2.11e+3]        \n",
      "Epoch 311:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.11e+3]        \n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0397, valid_loss=2.11e+3]        \n",
      "Epoch 312: 100%|██████████| 1/1 [00:00<00:00, 90.80it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0397, valid_loss=2.11e+3]\n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0714, train_loss_epoch=0.0714, valid_loss=2.11e+3]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0497, train_loss_epoch=0.0497, valid_loss=2.11e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:20,592\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0332, train_loss_epoch=0.0332, valid_loss=2.11e+3]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0493, train_loss_epoch=0.0493, valid_loss=2.11e+3]        \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0304, train_loss_epoch=0.0304, valid_loss=2.11e+3]        \n",
      "Epoch 334:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0592, train_loss_epoch=0.0592, valid_loss=2.11e+3]        \n",
      "Epoch 334: 100%|██████████| 1/1 [00:00<00:00, 88.02it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558, valid_loss=2.11e+3]\n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558, valid_loss=2.11e+3]        \n",
      "Epoch 336:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551, valid_loss=2.11e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337: 100%|██████████| 1/1 [00:00<00:00, 81.44it/s, v_num=0, train_loss_step=0.0459, train_loss_epoch=0.0459, valid_loss=2.11e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.180, train_loss_epoch=0.180]         \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]         \n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]         \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0973, train_loss_epoch=0.0973]         \n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.149, train_loss_epoch=0.149]           \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0818, train_loss_epoch=0.0818]         \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.142, train_loss_epoch=0.142]           \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0904, train_loss_epoch=0.0904]         \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 115.17it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]  \n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0981, train_loss_epoch=0.0981]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687]         \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 113.51it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787]\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0787, train_loss_epoch=0.0787]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]           \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751]         \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00, 114.92it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836]\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0836, train_loss_epoch=0.0836]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0756, train_loss_epoch=0.0756]         \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0869, train_loss_epoch=0.0869]         \n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0975, train_loss_epoch=0.0975]         \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0673, train_loss_epoch=0.0673]         \n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0681, train_loss_epoch=0.0681]         \n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00, 115.45it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585]         \n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0481, train_loss_epoch=0.0481]         \n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00, 102.86it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716]\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0716, train_loss_epoch=0.0716]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0687, train_loss_epoch=0.0687]         \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0568, train_loss_epoch=0.0568]         \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0375, train_loss_epoch=0.0375]         \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0965, train_loss_epoch=0.0965]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0529, train_loss_epoch=0.0529]        \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 83.21it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0607, train_loss_epoch=0.0607]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 88.36it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.0607] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 129.06it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 42.14it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.0607, valid_loss=2.28e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109, valid_loss=2.28e+3]        \n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 81.52it/s, v_num=0, train_loss_step=0.0477, train_loss_epoch=0.0477, valid_loss=2.28e+3]\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0477, train_loss_epoch=0.0477, valid_loss=2.28e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0616, train_loss_epoch=0.0616, valid_loss=2.28e+3]        \n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0694, train_loss_epoch=0.0694, valid_loss=2.28e+3]        \n",
      "Epoch 103: 100%|██████████| 1/1 [00:00<00:00, 88.81it/s, v_num=0, train_loss_step=0.0694, train_loss_epoch=0.0694, valid_loss=2.28e+3]\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0454, train_loss_epoch=0.0454, valid_loss=2.28e+3]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0439, train_loss_epoch=0.0439, valid_loss=2.28e+3]        \n",
      "Epoch 106:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0511, train_loss_epoch=0.0511, valid_loss=2.28e+3]        \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.053, train_loss_epoch=0.053, valid_loss=2.28e+3]           \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 82.87it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.28e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.28e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=2.28e+3]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0601, train_loss_epoch=0.0601, valid_loss=2.28e+3]        \n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 86.29it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0601, valid_loss=2.28e+3]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0397, train_loss_epoch=0.0397, valid_loss=2.28e+3]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0824, train_loss_epoch=0.0824, valid_loss=2.28e+3]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.044, train_loss_epoch=0.044, valid_loss=2.28e+3]          \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=2.28e+3]           \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=2.28e+3]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=2.28e+3]\n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0501, train_loss_epoch=0.0501, valid_loss=2.28e+3]         \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 87.06it/s, v_num=0, train_loss_step=0.0798, train_loss_epoch=0.0501, valid_loss=2.28e+3]\n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0798, train_loss_epoch=0.0798, valid_loss=2.28e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0388, train_loss_epoch=0.0388, valid_loss=2.28e+3]        \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0446, valid_loss=2.28e+3]        \n",
      "Epoch 164:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0469, train_loss_epoch=0.0469, valid_loss=2.28e+3]         \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 83.35it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=2.28e+3]  \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.040, train_loss_epoch=0.040, valid_loss=2.28e+3]        \n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0299, valid_loss=2.28e+3]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=2.28e+3]        \n",
      "Epoch 167: 100%|██████████| 1/1 [00:00<00:00, 82.41it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395, valid_loss=2.28e+3]\n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0395, train_loss_epoch=0.0395, valid_loss=2.28e+3]        \n",
      "Epoch 169:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0336, train_loss_epoch=0.0336, valid_loss=2.28e+3]        \n",
      "Epoch 170:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.28e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 88.43it/s, v_num=0, train_loss_step=0.0409, train_loss_epoch=0.0409, valid_loss=2.28e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0412, train_loss_epoch=0.0412, valid_loss=2.28e+3]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0299, train_loss_epoch=0.0299, valid_loss=2.28e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.046, train_loss_epoch=0.046, valid_loss=2.28e+3]          \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=2.28e+3]         \n",
      "Epoch 185: 100%|██████████| 1/1 [00:00<00:00, 83.02it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=2.28e+3]\n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0378, train_loss_epoch=0.0378, valid_loss=2.28e+3]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0497, train_loss_epoch=0.0497, valid_loss=2.28e+3]        \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=2.28e+3]          \n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 85.52it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.030, valid_loss=2.28e+3]\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.0677, valid_loss=2.28e+3]        \n",
      "Epoch 190:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0313, train_loss_epoch=0.0313, valid_loss=2.28e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.035, train_loss_epoch=0.035, valid_loss=2.28e+3]          \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 120.97it/s, v_num=0, train_loss_step=0.0651, train_loss_epoch=0.0391, valid_loss=2.28e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 204.46it/s]\u001b[A\n",
      "Epoch 201:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0268, train_loss_epoch=0.0268, valid_loss=2.12e+3]         \n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0302, train_loss_epoch=0.0302, valid_loss=2.12e+3]        \n",
      "Epoch 203:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0528, train_loss_epoch=0.0528, valid_loss=2.12e+3]        \n",
      "Epoch 203: 100%|██████████| 1/1 [00:00<00:00, 82.30it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=2.12e+3]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0544, train_loss_epoch=0.0544, valid_loss=2.12e+3]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0285, train_loss_epoch=0.0285, valid_loss=2.12e+3]        \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722, valid_loss=2.12e+3]        \n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00, 85.12it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0722, valid_loss=2.12e+3]\n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0269, train_loss_epoch=0.0269, valid_loss=2.12e+3]        \n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=2.12e+3]          \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=2.12e+3]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.12e+3]         \n",
      "Epoch 232: 100%|██████████| 1/1 [00:00<00:00, 91.06it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=2.12e+3] \n",
      "Epoch 233:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0206, train_loss_epoch=0.0206, valid_loss=2.12e+3]        \n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0202, train_loss_epoch=0.0202, valid_loss=2.12e+3]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0236, train_loss_epoch=0.0236, valid_loss=2.12e+3]        \n",
      "Epoch 235: 100%|██████████| 1/1 [00:00<00:00, 84.79it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0236, valid_loss=2.12e+3]\n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0215, train_loss_epoch=0.0215, valid_loss=2.12e+3]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.024, train_loss_epoch=0.024, valid_loss=2.12e+3]          \n",
      "Epoch 238:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0251, train_loss_epoch=0.0251, valid_loss=2.12e+3]        \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 94.89it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0286, valid_loss=2.12e+3] \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 84.23it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0386, valid_loss=2.12e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0386, train_loss_epoch=0.0386, valid_loss=2.12e+3]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=2.12e+3]        \n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0577, train_loss_epoch=0.0577, valid_loss=2.12e+3]        \n",
      "Epoch 264:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0429, train_loss_epoch=0.0429, valid_loss=2.12e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:23,595\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00, 103.03it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=2.12e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 96.55it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.428]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.385, train_loss_epoch=0.385]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.448, train_loss_epoch=0.448]        \n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.448, train_loss_epoch=0.448]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 95.50it/s, v_num=0, train_loss_step=0.442, train_loss_epoch=0.448]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.442, train_loss_epoch=0.442]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]        \n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.521, train_loss_epoch=0.521]        \n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 98.65it/s, v_num=0, train_loss_step=0.283, train_loss_epoch=0.283]\n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 96.44it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.283]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]        \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]         \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161]         \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.217, train_loss_epoch=0.217]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.135, train_loss_epoch=0.135]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.124, train_loss_epoch=0.124]        \n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00, 86.90it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.116, train_loss_epoch=0.116]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.156, train_loss_epoch=0.156]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]         \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]          \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.145, train_loss_epoch=0.145]        \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 93.44it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 94.28it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.114]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 96.39it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.111]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 163.83it/s]\u001b[A\n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.57e+3]          \n",
      "Epoch 113:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177, valid_loss=2.57e+3]          \n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 83.08it/s, v_num=0, train_loss_step=0.0813, train_loss_epoch=0.0813, valid_loss=2.57e+3]\n",
      "Epoch 114:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0813, train_loss_epoch=0.0813, valid_loss=2.57e+3]        \n",
      "Epoch 115:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0842, train_loss_epoch=0.0842, valid_loss=2.57e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:25,817\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 77.43it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837, valid_loss=2.57e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]        \n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.181, train_loss_epoch=0.181]         \n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.270, train_loss_epoch=0.270]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 38: 100%|██████████| 1/1 [00:00<00:00, 71.58it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.107]\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137]        \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.126, train_loss_epoch=0.126]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:26,782\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.138, train_loss_epoch=0.138]          \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]        \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 91.18it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0921]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 139.78it/s]\u001b[A\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 35.17it/s, v_num=0, train_loss_step=0.0992, train_loss_epoch=0.0992, valid_loss=2.3e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.121, train_loss_epoch=0.121]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.182, train_loss_epoch=0.182]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]         \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.171, train_loss_epoch=0.171]         \n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 105.22it/s, v_num=0, train_loss_step=0.165, train_loss_epoch=0.165]\n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 109.11it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.165]\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.0802]         \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971]         \n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 101.60it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971]\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]           \n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0833, train_loss_epoch=0.0833]         \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 90.04it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695] \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0695, train_loss_epoch=0.0695]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0834, train_loss_epoch=0.0834]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 99.49it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0834] \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0796, train_loss_epoch=0.0796]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526]         \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]           \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00, 99.86it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.183] \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0502, train_loss_epoch=0.0502]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445]         \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 94.46it/s, v_num=0, train_loss_step=0.0399, train_loss_epoch=0.0399] \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0399, train_loss_epoch=0.0399]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.154, train_loss_epoch=0.154]          \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0669, train_loss_epoch=0.0669]         \n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 95.57it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496] \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0496, train_loss_epoch=0.0496]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0671, train_loss_epoch=0.0671]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0377, train_loss_epoch=0.0377]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 99.39it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.0377]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.68it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0471, train_loss_epoch=0.0471, valid_loss=1.77e+3]         \n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0584, train_loss_epoch=0.0584, valid_loss=1.77e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0466, train_loss_epoch=0.0466, valid_loss=1.77e+3]         \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 95.36it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684, valid_loss=1.77e+3] \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0684, train_loss_epoch=0.0684, valid_loss=1.77e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0521, train_loss_epoch=0.0521, valid_loss=1.77e+3]         \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0331, train_loss_epoch=0.0331, valid_loss=1.77e+3]         \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 95.54it/s, v_num=0, train_loss_step=0.0612, train_loss_epoch=0.0612, valid_loss=1.77e+3] \n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0612, train_loss_epoch=0.0612, valid_loss=1.77e+3]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.042, train_loss_epoch=0.042, valid_loss=1.77e+3]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:29,137\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0281, train_loss_epoch=0.0281, valid_loss=1.77e+3]         \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 99.57it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0281, valid_loss=1.77e+3] \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0256, train_loss_epoch=0.0256, valid_loss=1.77e+3]        \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0487, train_loss_epoch=0.0487, valid_loss=1.77e+3]         \n",
      "Epoch 148: 100%|██████████| 1/1 [00:00<00:00, 85.98it/s, v_num=0, train_loss_step=0.0375, train_loss_epoch=0.0375, valid_loss=1.77e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.202, train_loss_epoch=0.202]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.368, train_loss_epoch=0.368]         \n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 109.64it/s, v_num=0, train_loss_step=0.230, train_loss_epoch=0.230]\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.230, train_loss_epoch=0.230]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.378, train_loss_epoch=0.378]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251]         \n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.255, train_loss_epoch=0.255]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.216, train_loss_epoch=0.216]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.161, train_loss_epoch=0.161]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]         \n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 111.74it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.195]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.197, train_loss_epoch=0.197]         \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]         \n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00, 108.04it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.220, train_loss_epoch=0.220]         \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]         \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.178, train_loss_epoch=0.178]         \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.191, train_loss_epoch=0.191]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 78.59it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.185, train_loss_epoch=0.185]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.114, train_loss_epoch=0.114]        \n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120]         \n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.151, train_loss_epoch=0.151]        \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 75.79it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]        \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 91.25it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.196, train_loss_epoch=0.196]           \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 114.49it/s, v_num=0, train_loss_step=0.153, train_loss_epoch=0.104]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.56it/s]\u001b[A\n",
      "Epoch 107:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.111, train_loss_epoch=0.111, valid_loss=2.46e+3]         \n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 84.74it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.111, valid_loss=2.46e+3]\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0971, train_loss_epoch=0.0971, valid_loss=2.46e+3]        \n",
      "Epoch 109:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152, valid_loss=2.46e+3]           \n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00, 109.04it/s, v_num=0, train_loss_step=0.0887, train_loss_epoch=0.0887, valid_loss=2.46e+3]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0887, train_loss_epoch=0.0887, valid_loss=2.46e+3]         \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.113, train_loss_epoch=0.113, valid_loss=2.46e+3]           \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.120, train_loss_epoch=0.120, valid_loss=2.46e+3]         \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166, valid_loss=2.46e+3]           \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104, valid_loss=2.46e+3]         \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 106.40it/s, v_num=0, train_loss_step=0.0882, train_loss_epoch=0.0882, valid_loss=2.46e+3]\n",
      "Epoch 125:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0882, train_loss_epoch=0.0882, valid_loss=2.46e+3]         \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.46e+3]           \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.115, train_loss_epoch=0.115, valid_loss=2.46e+3]         \n",
      "Epoch 138:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0823, train_loss_epoch=0.0823, valid_loss=2.46e+3]         \n",
      "Epoch 139:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0723, train_loss_epoch=0.0723, valid_loss=2.46e+3]         \n",
      "Epoch 150:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139, valid_loss=2.46e+3]           \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0674, train_loss_epoch=0.0674, valid_loss=2.46e+3]         \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110, valid_loss=2.46e+3]           \n",
      "Epoch 163:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0579, train_loss_epoch=0.0579, valid_loss=2.46e+3]         \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0677, train_loss_epoch=0.0677, valid_loss=2.46e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0608, train_loss_epoch=0.0608, valid_loss=2.46e+3]         \n",
      "Epoch 184: 100%|██████████| 1/1 [00:00<00:00, 97.68it/s, v_num=0, train_loss_step=0.0537, train_loss_epoch=0.0537, valid_loss=2.46e+3] \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0537, train_loss_epoch=0.0537, valid_loss=2.46e+3]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.46e+3]           \n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 101.92it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.46e+3]\n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112, valid_loss=2.46e+3]         \n",
      "Epoch 188:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0696, train_loss_epoch=0.0696, valid_loss=2.46e+3]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0576, train_loss_epoch=0.0576, valid_loss=2.46e+3]         \n",
      "Epoch 198: 100%|██████████| 1/1 [00:00<00:00, 101.38it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0576, valid_loss=2.46e+3]\n",
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0522, train_loss_epoch=0.0522, valid_loss=2.46e+3]         \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 101.79it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0522, valid_loss=2.46e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 172.46it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 51.08it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0522, valid_loss=2.27e+3] \n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602, valid_loss=2.27e+3]        \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0717, train_loss_epoch=0.0717, valid_loss=2.27e+3]         \n",
      "Epoch 210: 100%|██████████| 1/1 [00:00<00:00, 102.06it/s, v_num=0, train_loss_step=0.056, train_loss_epoch=0.0717, valid_loss=2.27e+3] \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.056, train_loss_epoch=0.056, valid_loss=2.27e+3]          \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0833, train_loss_epoch=0.0833, valid_loss=2.27e+3]        \n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0427, train_loss_epoch=0.0427, valid_loss=2.27e+3]         \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0712, train_loss_epoch=0.0712, valid_loss=2.27e+3]         \n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00, 108.43it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.27e+3]\n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.27e+3]         \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0478, train_loss_epoch=0.0478, valid_loss=2.27e+3]         \n",
      "Epoch 235: 100%|██████████| 1/1 [00:00<00:00, 108.82it/s, v_num=0, train_loss_step=0.0592, train_loss_epoch=0.0592, valid_loss=2.27e+3]\n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0383, train_loss_epoch=0.0383, valid_loss=2.27e+3]         \n",
      "Epoch 247:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0657, train_loss_epoch=0.0657, valid_loss=2.27e+3]         \n",
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 108.87it/s, v_num=0, train_loss_step=0.0519, train_loss_epoch=0.0519, valid_loss=2.27e+3]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0519, train_loss_epoch=0.0519, valid_loss=2.27e+3]         \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128, valid_loss=2.27e+3]           \n",
      "Epoch 249: 100%|██████████| 1/1 [00:00<00:00, 101.86it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=2.27e+3]\n",
      "Epoch 250:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0321, train_loss_epoch=0.0321, valid_loss=2.27e+3]         \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=2.27e+3]         \n",
      "Epoch 251: 100%|██████████| 1/1 [00:00<00:00, 108.99it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722, valid_loss=2.27e+3]\n",
      "Epoch 252:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0722, train_loss_epoch=0.0722, valid_loss=2.27e+3]         \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0897, train_loss_epoch=0.0897, valid_loss=2.27e+3]         \n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0587, train_loss_epoch=0.0587, valid_loss=2.27e+3]         \n",
      "Epoch 256: 100%|██████████| 1/1 [00:00<00:00, 118.05it/s, v_num=0, train_loss_step=0.0472, train_loss_epoch=0.0472, valid_loss=2.27e+3]\n",
      "Epoch 257:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.067, train_loss_epoch=0.067, valid_loss=2.27e+3]           \n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0764, train_loss_epoch=0.0764, valid_loss=2.27e+3]         \n",
      "Epoch 269:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0551, train_loss_epoch=0.0551, valid_loss=2.27e+3]         \n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.123, train_loss_epoch=0.123, valid_loss=2.27e+3]           \n",
      "Epoch 270: 100%|██████████| 1/1 [00:00<00:00, 112.87it/s, v_num=0, train_loss_step=0.086, train_loss_epoch=0.123, valid_loss=2.27e+3]\n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.086, train_loss_epoch=0.086, valid_loss=2.27e+3]         \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0435, train_loss_epoch=0.0435, valid_loss=2.27e+3]         \n",
      "Epoch 272: 100%|██████████| 1/1 [00:00<00:00, 109.04it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536, valid_loss=2.27e+3]\n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0536, train_loss_epoch=0.0536, valid_loss=2.27e+3]         \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0602, train_loss_epoch=0.0602, valid_loss=2.27e+3]         \n",
      "Epoch 281: 100%|██████████| 1/1 [00:00<00:00, 106.47it/s, v_num=0, train_loss_step=0.0534, train_loss_epoch=0.0534, valid_loss=2.27e+3]\n",
      "Epoch 282:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=2.27e+3]         \n",
      "Epoch 283:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0414, train_loss_epoch=0.0414, valid_loss=2.27e+3]         \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=2.27e+3]         \n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652, valid_loss=2.27e+3]         \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0652, train_loss_epoch=0.0652, valid_loss=2.27e+3]\n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0252, train_loss_epoch=0.0252, valid_loss=2.27e+3]         \n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.27e+3]         \n",
      "Epoch 297:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0421, train_loss_epoch=0.0421, valid_loss=2.27e+3]\n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0323, train_loss_epoch=0.0323, valid_loss=2.27e+3]         \n",
      "Epoch 299:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0606, train_loss_epoch=0.0606, valid_loss=2.27e+3]         \n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 112.12it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0606, valid_loss=2.27e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 193.57it/s]\u001b[A\n",
      "Epoch 300: 100%|██████████| 1/1 [00:00<00:00, 109.86it/s, v_num=0, train_loss_step=0.0446, train_loss_epoch=0.0446, valid_loss=2.18e+3]\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0231, train_loss_epoch=0.0231, valid_loss=2.18e+3]         \n",
      "Epoch 301: 100%|██████████| 1/1 [00:00<00:00, 90.80it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=2.18e+3]\n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=2.18e+3]        \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0558, train_loss_epoch=0.0558, valid_loss=2.18e+3]         \n",
      "Epoch 313:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0523, train_loss_epoch=0.0523, valid_loss=2.18e+3]         \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00, 100.64it/s, v_num=0, train_loss_step=0.0435, train_loss_epoch=0.0435, valid_loss=2.18e+3]\n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0283, train_loss_epoch=0.0283, valid_loss=2.18e+3]         \n",
      "Epoch 330:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0298, train_loss_epoch=0.0298, valid_loss=2.18e+3]         \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00, 114.71it/s, v_num=0, train_loss_step=0.0242, train_loss_epoch=0.0242, valid_loss=2.18e+3]\n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0353, train_loss_epoch=0.0353, valid_loss=2.18e+3]         \n",
      "Epoch 342:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0489, train_loss_epoch=0.0489, valid_loss=2.18e+3]         \n",
      "Epoch 353:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0243, train_loss_epoch=0.0243, valid_loss=2.18e+3]         \n",
      "Epoch 353: 100%|██████████| 1/1 [00:00<00:00, 106.75it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=2.18e+3]\n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0267, train_loss_epoch=0.0267, valid_loss=2.18e+3]         \n",
      "Epoch 355:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0254, train_loss_epoch=0.0254, valid_loss=2.18e+3]         \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0226, train_loss_epoch=0.0226, valid_loss=2.18e+3]         \n",
      "Epoch 360: 100%|██████████| 1/1 [00:00<00:00, 111.43it/s, v_num=0, train_loss_step=0.026, train_loss_epoch=0.026, valid_loss=2.18e+3]  \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0234, train_loss_epoch=0.0234, valid_loss=2.18e+3]        \n",
      "Epoch 362:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.029, train_loss_epoch=0.029, valid_loss=2.18e+3]           \n",
      "Epoch 373:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0271, train_loss_epoch=0.0271, valid_loss=2.18e+3]         \n",
      "Epoch 373: 100%|██████████| 1/1 [00:00<00:00, 108.76it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0591, valid_loss=2.18e+3]\n",
      "Epoch 374:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0591, train_loss_epoch=0.0591, valid_loss=2.18e+3]         \n",
      "Epoch 375:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0343, train_loss_epoch=0.0343, valid_loss=2.18e+3]         \n",
      "Epoch 375: 100%|██████████| 1/1 [00:00<00:00, 96.84it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342, valid_loss=2.18e+3] \n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0342, train_loss_epoch=0.0342, valid_loss=2.18e+3]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0314, train_loss_epoch=0.0314, valid_loss=2.18e+3]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:35,020\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380: 100%|██████████| 1/1 [00:00<00:00, 105.03it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=2.18e+3]\n",
      "Epoch 381:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0225, train_loss_epoch=0.0225, valid_loss=2.18e+3]         \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0399, train_loss_epoch=0.0399, valid_loss=2.18e+3]        \n",
      "Epoch 387: 100%|██████████| 1/1 [00:00<00:00, 89.51it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=2.18e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.144, train_loss_epoch=0.144]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.276, train_loss_epoch=0.276]        \n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 107.04it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.276]\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0922, train_loss_epoch=0.0922]         \n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.177, train_loss_epoch=0.177]           \n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 104.11it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 72.80it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]        \n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 88.41it/s, v_num=0, train_loss_step=0.301, train_loss_epoch=0.301]\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.203, train_loss_epoch=0.203]        \n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 85.13it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.206, train_loss_epoch=0.206]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194]        \n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.279, train_loss_epoch=0.279]         \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 98.85it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288] \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]        \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 98.39it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.185, train_loss_epoch=0.185]        \n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.195, train_loss_epoch=0.195]        \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 95.04it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134] \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.134, train_loss_epoch=0.134]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.118, train_loss_epoch=0.118]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141]\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117]         \n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00, 97.44it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.117] \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.103, train_loss_epoch=0.103]           \n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 107.70it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101]\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0892, train_loss_epoch=0.0892]        \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 109.54it/s, v_num=0, train_loss_step=0.0859, train_loss_epoch=0.0926]\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0859, train_loss_epoch=0.0859]         \n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]           \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0891, train_loss_epoch=0.0891]         \n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]         \n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0701, train_loss_epoch=0.0701]\n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s, v_num=0, train_loss_step=0.0702, train_loss_epoch=0.0701]\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0702, train_loss_epoch=0.0702]        \n",
      "Epoch 95: 100%|██████████| 1/1 [00:00<00:00, 104.68it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0702]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0599, train_loss_epoch=0.0599]         \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 107.33it/s, v_num=0, train_loss_step=0.0981, train_loss_epoch=0.0599]\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0981, train_loss_epoch=0.0981]         \n",
      "Epoch 97: 100%|██████████| 1/1 [00:00<00:00, 103.86it/s, v_num=0, train_loss_step=0.0633, train_loss_epoch=0.0633]\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0633, train_loss_epoch=0.0633]         \n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 104.46it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575]\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0575, train_loss_epoch=0.0575]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 108.56it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.0575] \n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 174.21it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 53.57it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.0575, valid_loss=2.07e+3]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.155, train_loss_epoch=0.155, valid_loss=2.07e+3]        \n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0751, train_loss_epoch=0.0751, valid_loss=2.07e+3]         \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0672, train_loss_epoch=0.0672, valid_loss=2.07e+3]         \n",
      "Epoch 112:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0643, train_loss_epoch=0.0643, valid_loss=2.07e+3]         \n",
      "Epoch 117: 100%|██████████| 1/1 [00:00<00:00, 114.06it/s, v_num=0, train_loss_step=0.0688, train_loss_epoch=0.0688, valid_loss=2.07e+3]\n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0718, train_loss_epoch=0.0718, valid_loss=2.07e+3]         \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0827, train_loss_epoch=0.0827, valid_loss=2.07e+3]         \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0526, train_loss_epoch=0.0526, valid_loss=2.07e+3]         \n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 100.76it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=2.07e+3]\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=2.07e+3]         \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 98.02it/s, v_num=0, train_loss_step=0.0629, train_loss_epoch=0.0629, valid_loss=2.07e+3]\n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0748, train_loss_epoch=0.0748, valid_loss=2.07e+3]        \n",
      "Epoch 141: 100%|██████████| 1/1 [00:00<00:00, 94.88it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=2.07e+3] \n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=2.07e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=2.07e+3]\n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436, valid_loss=2.07e+3]         \n",
      "Epoch 143: 100%|██████████| 1/1 [00:00<00:00, 94.83it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=2.07e+3]   \n",
      "Epoch 144:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=2.07e+3]        \n",
      "Epoch 145:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0448, train_loss_epoch=0.0448, valid_loss=2.07e+3]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0572, train_loss_epoch=0.0572, valid_loss=2.07e+3]         \n",
      "Epoch 165:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0445, train_loss_epoch=0.0445, valid_loss=2.07e+3]         \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=2.07e+3]         \n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0408, train_loss_epoch=0.0408, valid_loss=2.07e+3]         \n",
      "Epoch 194:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0394, train_loss_epoch=0.0394, valid_loss=2.07e+3]        \n",
      "Epoch 195:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0447, train_loss_epoch=0.0447, valid_loss=2.07e+3]        \n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 95.18it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0447, valid_loss=2.07e+3]\n",
      "Epoch 196:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=2.07e+3]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0319, train_loss_epoch=0.0319, valid_loss=2.07e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 105.10it/s, v_num=0, train_loss_step=0.050, train_loss_epoch=0.0452, valid_loss=2.07e+3] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 181.98it/s]\u001b[A\n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0433, train_loss_epoch=0.0433, valid_loss=1.67e+3]         \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0355, train_loss_epoch=0.0355, valid_loss=1.67e+3]         \n",
      "Epoch 207: 100%|██████████| 1/1 [00:00<00:00, 100.93it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=1.67e+3]\n",
      "Epoch 208:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0341, train_loss_epoch=0.0341, valid_loss=1.67e+3]         \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0398, train_loss_epoch=0.0398, valid_loss=1.67e+3]         \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0562, train_loss_epoch=0.0562, valid_loss=1.67e+3]         \n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0562, train_loss_epoch=0.0562, valid_loss=1.67e+3]\n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0205, train_loss_epoch=0.0205, valid_loss=1.67e+3]         \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0296, train_loss_epoch=0.0296, valid_loss=1.67e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:40,942\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0385, train_loss_epoch=0.0385, valid_loss=1.67e+3]         \n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0389, train_loss_epoch=0.0389, valid_loss=1.67e+3]         \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0176, train_loss_epoch=0.0176, valid_loss=1.67e+3]         \n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 104.70it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=1.67e+3]  \n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.030, train_loss_epoch=0.030, valid_loss=1.67e+3]         \n",
      "Epoch 245:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0212, train_loss_epoch=0.0212, valid_loss=1.67e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247: 100%|██████████| 1/1 [00:00<00:00, 86.30it/s, v_num=0, train_loss_step=0.0211, train_loss_epoch=0.0211, valid_loss=1.67e+3] \n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.251, train_loss_epoch=0.251]         \n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00, 92.62it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 93.27it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]        \n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 98.08it/s, v_num=0, train_loss_step=0.112, train_loss_epoch=0.112]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.250, train_loss_epoch=0.250]        \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 94.06it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.282, train_loss_epoch=0.282]        \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.194, train_loss_epoch=0.194]         \n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.109, train_loss_epoch=0.109]           \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136]           \n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 95.41it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146] \n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]        \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119]         \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 100.09it/s, v_num=0, train_loss_step=0.119, train_loss_epoch=0.119]\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]         \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0658, train_loss_epoch=0.0658]         \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0658, train_loss_epoch=0.0658]         \n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0712, train_loss_epoch=0.0712]        \n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0463, train_loss_epoch=0.0463]         \n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00, 101.12it/s, v_num=0, train_loss_step=0.0834, train_loss_epoch=0.0463]\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0834, train_loss_epoch=0.0834]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0775, train_loss_epoch=0.0775]         \n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0483, train_loss_epoch=0.0483]         \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0478, train_loss_epoch=0.0478]         \n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0405, train_loss_epoch=0.0405]         \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 101.56it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.0405] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 162.51it/s]\u001b[A\n",
      "Epoch 108:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0637, train_loss_epoch=0.0637, valid_loss=1.74e+3]         \n",
      "Epoch 118:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.059, train_loss_epoch=0.059, valid_loss=1.74e+3]           \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 94.43it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585, valid_loss=1.74e+3]\n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0585, train_loss_epoch=0.0585, valid_loss=1.74e+3]        \n",
      "Epoch 120:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0728, train_loss_epoch=0.0728, valid_loss=1.74e+3]         \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=1.74e+3]         \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0322, train_loss_epoch=0.0322, valid_loss=1.74e+3]\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0861, train_loss_epoch=0.0861, valid_loss=1.74e+3]        \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0817, train_loss_epoch=0.0817, valid_loss=1.74e+3]        \n",
      "Epoch 140: 100%|██████████| 1/1 [00:00<00:00, 86.98it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473, valid_loss=1.74e+3]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0473, train_loss_epoch=0.0473, valid_loss=1.74e+3]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0468, train_loss_epoch=0.0468, valid_loss=1.74e+3]        \n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0444, train_loss_epoch=0.0444, valid_loss=1.74e+3]        \n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0452, train_loss_epoch=0.0452, valid_loss=1.74e+3]        \n",
      "Epoch 160: 100%|██████████| 1/1 [00:00<00:00, 88.52it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424, valid_loss=1.74e+3]\n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0424, train_loss_epoch=0.0424, valid_loss=1.74e+3]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0501, train_loss_epoch=0.0501, valid_loss=1.74e+3]        \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 94.75it/s, v_num=0, train_loss_step=0.0436, train_loss_epoch=0.0436, valid_loss=1.74e+3]\n",
      "Epoch 171:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.037, train_loss_epoch=0.037, valid_loss=1.74e+3]          \n",
      "Epoch 171: 100%|██████████| 1/1 [00:00<00:00, 82.73it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.74e+3]\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0253, train_loss_epoch=0.0253, valid_loss=1.74e+3]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.036, train_loss_epoch=0.036, valid_loss=1.74e+3]          \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0679, train_loss_epoch=0.0679, valid_loss=1.74e+3]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0441, train_loss_epoch=0.0441, valid_loss=1.74e+3]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0372, train_loss_epoch=0.0372, valid_loss=1.74e+3]        \n",
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 91.11it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0282, valid_loss=1.74e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 164.39it/s]\u001b[A\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0624, valid_loss=1.27e+3]        \n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0223, train_loss_epoch=0.0223, valid_loss=1.27e+3]        \n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 85.84it/s, v_num=0, train_loss_step=0.0609, train_loss_epoch=0.0609, valid_loss=1.27e+3]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0609, train_loss_epoch=0.0609, valid_loss=1.27e+3]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0368, train_loss_epoch=0.0368, valid_loss=1.27e+3]        \n",
      "Epoch 220:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.27e+3]        \n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 94.18it/s, v_num=0, train_loss_step=0.0292, train_loss_epoch=0.0292, valid_loss=1.27e+3]\n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0416, train_loss_epoch=0.0416, valid_loss=1.27e+3]        \n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 87.84it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.27e+3]\n",
      "Epoch 222:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0259, train_loss_epoch=0.0259, valid_loss=1.27e+3]        \n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0295, train_loss_epoch=0.0295, valid_loss=1.27e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:44,230\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n",
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 77.13it/s, v_num=0, train_loss_step=0.0278, train_loss_epoch=0.0278, valid_loss=1.27e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.160, train_loss_epoch=0.160]         \n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.365, train_loss_epoch=0.365]        \n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.152, train_loss_epoch=0.152]         \n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.208, train_loss_epoch=0.208]         \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.252, train_loss_epoch=0.252]         \n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.262, train_loss_epoch=0.262]         \n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.264, train_loss_epoch=0.264]         \n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.183, train_loss_epoch=0.183]         \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.137, train_loss_epoch=0.137]         \n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.122, train_loss_epoch=0.122]         \n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 118.29it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.139, train_loss_epoch=0.139]         \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]         \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.143, train_loss_epoch=0.143]         \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00, 118.38it/s, v_num=0, train_loss_step=0.0991, train_loss_epoch=0.0991]\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0991, train_loss_epoch=0.0991]         \n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.140, train_loss_epoch=0.140]           \n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 118.30it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.110, train_loss_epoch=0.110]         \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.108, train_loss_epoch=0.108]         \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.104, train_loss_epoch=0.104]         \n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00, 122.97it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.104]\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.125, train_loss_epoch=0.125]         \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.127, train_loss_epoch=0.127]         \n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107]         \n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.093, train_loss_epoch=0.093]           \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851]         \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 127.89it/s, v_num=0, train_loss_step=0.0851, train_loss_epoch=0.0851]\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0754, train_loss_epoch=0.0754]         \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.168, train_loss_epoch=0.168]           \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865]         \n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0865, train_loss_epoch=0.0865]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105]           \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0783, train_loss_epoch=0.0783]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:45,722\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 116.86it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837]\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0837, train_loss_epoch=0.0837]         \n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0939, train_loss_epoch=0.0939]         \n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 125.99it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.0785] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 179.70it/s]\u001b[A\n",
      "Epoch 90: 100%|██████████| 1/1 [00:00<00:00, 43.71it/s, v_num=0, train_loss_step=0.105, train_loss_epoch=0.105, valid_loss=2.9e+3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train_tune pid=6950)\u001b[0m Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.399, train_loss_epoch=0.399]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.318, train_loss_epoch=0.318]        \n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.277, train_loss_epoch=0.277]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.257, train_loss_epoch=0.257]        \n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 70.17it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269]\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.269, train_loss_epoch=0.269]        \n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 73.93it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.288, train_loss_epoch=0.288]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.170, train_loss_epoch=0.170]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.190, train_loss_epoch=0.190]        \n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.187, train_loss_epoch=0.187]        \n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.157, train_loss_epoch=0.157]        \n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.128, train_loss_epoch=0.128]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.174, train_loss_epoch=0.174]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 79.33it/s, v_num=0, train_loss_step=0.146, train_loss_epoch=0.146]\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.148, train_loss_epoch=0.148]        \n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.129, train_loss_epoch=0.129]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.186, train_loss_epoch=0.186]        \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.159, train_loss_epoch=0.159]        \n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 76.22it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.165]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 151.47it/s]\u001b[A\n",
      "Epoch 101:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.136, train_loss_epoch=0.136, valid_loss=2.46e+3]        \n",
      "Epoch 102:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.131, train_loss_epoch=0.131, valid_loss=2.46e+3]        \n",
      "Epoch 109: 100%|██████████| 1/1 [00:00<00:00, 73.45it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.46e+3]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.107, train_loss_epoch=0.107, valid_loss=2.46e+3]        \n",
      "Epoch 110: 100%|██████████| 1/1 [00:00<00:00, 73.23it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.46e+3]\n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.117, train_loss_epoch=0.117, valid_loss=2.46e+3]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0966, train_loss_epoch=0.0966, valid_loss=2.46e+3]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.166, train_loss_epoch=0.166, valid_loss=2.46e+3]          \n",
      "Epoch 127:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.106, train_loss_epoch=0.106, valid_loss=2.46e+3]        \n",
      "Epoch 134:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0993, train_loss_epoch=0.0993, valid_loss=2.46e+3]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.102, train_loss_epoch=0.102, valid_loss=2.46e+3]          \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.46e+3]          \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.147, train_loss_epoch=0.147, valid_loss=2.46e+3]\n",
      "Epoch 151:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0902, train_loss_epoch=0.0902, valid_loss=2.46e+3]        \n",
      "Epoch 158:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0978, train_loss_epoch=0.0978, valid_loss=2.46e+3]        \n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.101, train_loss_epoch=0.101, valid_loss=2.46e+3]          \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0764, train_loss_epoch=0.0764, valid_loss=2.46e+3]        \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.141, train_loss_epoch=0.141, valid_loss=2.46e+3]          \n",
      "Epoch 181:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0754, train_loss_epoch=0.0754, valid_loss=2.46e+3]        \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0904, train_loss_epoch=0.0904, valid_loss=2.46e+3]        \n",
      "Epoch 189: 100%|██████████| 1/1 [00:00<00:00, 74.20it/s, v_num=0, train_loss_step=0.087, train_loss_epoch=0.087, valid_loss=2.46e+3]  \n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.087, train_loss_epoch=0.087, valid_loss=2.46e+3]        \n",
      "Epoch 190: 100%|██████████| 1/1 [00:00<00:00, 76.91it/s, v_num=0, train_loss_step=0.087, train_loss_epoch=0.087, valid_loss=2.46e+3]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0802, train_loss_epoch=0.0802, valid_loss=2.46e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:50:49,239\tINFO tensorboardx.py:275 -- Removed the following hyperparameter values when logging to tensorboard: {'futr_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'hist_exog_list': ('f2f_calls', 'remote_calls', 'ae_sent', 'evnt_invited'), 'loss': ('__ref_ph', 'de895953'), 'stat_exog_list': ('total_hcp_cnt',), 'valid_loss': ('__ref_ph', '004b9a7a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0, train_loss_step=0.0706, train_loss_epoch=0.0706, valid_loss=2.46e+3]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:00<00:00, 76.34it/s, v_num=0, train_loss_step=0.0637, train_loss_epoch=0.0706, valid_loss=2.46e+3]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 152.60it/s]\u001b[A\n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00, 68.99it/s, v_num=0, train_loss_step=0.0624, train_loss_epoch=0.0624, valid_loss=2.16e+3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32875a3917bd40beb9cada0b1a8c2102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489b211dbbda454fa7dd6cf1312bd58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75959a97cd204351a0077c941fe782b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ef303e5a154a5cb6b6b4b48f168ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d3f293e25d49f09f88783a8f3a30de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nf.fit(df=df_nixtla2,\n",
    "       static_df=static_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9f8ab965-c89f-4a29-adb7-63d30c05617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model:\n",
    "\n",
    "nf.save(path='models', model_index=None, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "df093536-9cee-4787-bccc-d9d4f9654b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "# load trained model:\n",
    "\n",
    "fcst_pre_trained = NeuralForecast.load(path='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "22c755fb-61b6-4e9f-895f-f297880b3c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc616b7802042ed8e154d24604f9680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x350 with 1 Axes>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nixtla_test = df_nixtla[df_nixtla.ds<='2022-05-01'].copy()\n",
    "\n",
    "futr_df = df_nixtla[['unique_id', 'ds', 'f2f_calls', 'remote_calls','ae_sent','evnt_invited']]\n",
    "\n",
    "y_hat_test = fcst_pre_trained.predict(df=df_nixtla_test,\n",
    "                                      static_df=static_df,\n",
    "                                      futr_df=futr_df).reset_index()\n",
    "\n",
    "sf.plot(df_nixtla, y_hat_test, engine='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "88b2fd7b-181f-47b4-b45e-da56dfe912fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load different country - same brand:\n",
    "\n",
    "dfy = pd.read_csv('s3://dsaa-cph-ai-s3-dev/jan_rathfelder/impact_data/it_eylea_20230215.csv')\n",
    "\n",
    "dfy = create_date_format(dfy, 'yrmo')\n",
    "\n",
    "static_df_new = dfy.pivot_table(values='total_hcp_cnt', index=['country_cd'], columns='cstmr_1_id', aggfunc='sum', fill_value=0).sum(axis=1).reset_index(name='total_hcp_cnt')\n",
    "static_df_new.rename(columns={'country_cd': 'unique_id', 'yyyymm': 'ds'}, inplace=True)\n",
    "\n",
    "df_nixtla_new = dfy.groupby(['country_cd', 'yyyymm'])[['sales_unit','f2f_calls', 'remote_calls','ae_sent','evnt_invited']].sum().reset_index()\n",
    "df_nixtla_new.rename(columns={'country_cd': 'unique_id', 'yyyymm': 'ds', 'sales_unit': 'y'}, inplace=True)\n",
    "df_nixtla_new2 = df_nixtla_new[(df_nixtla_new.ds<='2022-05-01')].copy()\n",
    "\n",
    "futr_df_new = df_nixtla_new[['unique_id', 'ds', 'f2f_calls', 'remote_calls','ae_sent','evnt_invited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1ff81754-45a8-4d08-80f1-e12889fc5e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c5b17f87e34879bf72ae3e349273a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make 6 months forecast based on input:\n",
    "\n",
    "y_hat_new = fcst_pre_trained.predict(df=df_nixtla_new2,\n",
    "                                     static_df=static_df_new,\n",
    "                                     futr_df=futr_df_new).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8ddd0d67-37fd-4717-9e0f-fdbcef520625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a8932c1053435e8072f8be591927de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatetimeArray>\n",
      "['2022-03-01 00:00:00', '2022-04-01 00:00:00', '2022-05-01 00:00:00']\n",
      "Length: 3, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# make forecast using only 3 months of input! --> seq2seq\n",
    "\n",
    "y_hat_new2 = fcst_pre_trained.predict(df=df_nixtla_new[(df_nixtla_new.ds>='2022-03-01')&(df_nixtla_new.ds<='2022-05-01')],\n",
    "                                     static_df=static_df_new,\n",
    "                                     futr_df=futr_df_new\n",
    "                                     ).reset_index()\n",
    "\n",
    "print(df_nixtla_new[(df_nixtla_new.ds>='2022-03-01')&(df_nixtla_new.ds<='2022-05-01')].ds.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "12c918bf-3151-40f1-91f8-d3cb7c8c632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SeasonalNaive]\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=models,  # selected models \n",
    "    freq='MS', # frequency denotes the granularity of the data. M for months in this case\n",
    "    n_jobs=-1,\n",
    "    #fallback_model = SeasonalNaive(season_length=12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2d685647-cf63-4057-93c2-02185b226277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB6EAAAFjCAYAAAB8PX4+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hT5fvH8XeSJt2D0ckqe2+QIYLIHioOFEUERFCGiqj4RQVRHF/5unDyc4ELcaMgAhUZCpUpyN6zE2hp6U6T/P4ojVZWC2nT8XldVy/anCfPuROSk/bc574fg8PhcCAiIiIiIiIiIiIiIiIiIuICRncHICIiIiIiIiIiIiIiIiIi5YeS0CIiIiIiIiIiIiIiIiIi4jJKQouIiIiIiIiIiIiIiIiIiMsoCS0iIiIiIiIiIiIiIiIiIi6jJLSIiIiIiIiIiIiIiIiIiLiMktAiIiIiIiIiIiIiIiIiIuIySkKLiIiIiIiIiIiIiIiIiIjLKAktIiIiIiIiIiIiIiIiIiIuoyS0iIiIiIiIiIiIiIiIiIi4jJLQIiIiIiIiIiIiIiIiIiLiMkpCi4iIiIiIiJRyc+fOxWAwsHHjRgwGQ6G+Dh8+7O6wRUREREREpILycHcAIiIiIiIiIlI46enpfPrppwVue+WVVzh+/DivvfZagduDg4NLMjQRERERERERJyWhRURERERERMoIX19f7rrrrgK3zZ8/n+Tk5HNuFxEREREREXEXteMWERERERERERERERERERGXURJaRERERERERERERERERERcRkloERERERERERERERERERFxGSWhRURERERERERERERERETEZZSEFhERERERERERERERERERl1ESWkREREREREREREREREREXEZJaBERERERERERERERERERcRkloUVERERERERERERERERExGWUhBYREREREREREREREREREZdRElpERERERERERERERERERFzG4HA4HO4OQkREREREREREREREREREygdVQouIiIiIiIiIiIiIiIiIiMsoCS0iIiIiIiIiIiIiIiIiIi6jJLSIiIiIiIiIiIiIiIiIiLiMktAiIiIiIiIiIiIiIiIiIuIySkKLiIiIiIiIiIiIiIiIiIjLKAktIiIiIiIiIiIiIiIiIiIu4+HuAMoLu91ObGws/v7+GAwGd4cjIiIiIiIiIiIiIiIiIuIyDoeDM2fOEBERgdF48VpnJaFdJDY2lho1arg7DBERERERERERERERERGRYnPs2DGqV69+0TFKQruIv78/kPekBwQEuDkakbLHarWybNkyevfujdlsdnc4IiIiIiIiIiIiRaLzW1LWpKenExERAeQV2vn6+ro5IhEp7VJTU6lRo4YzL3oxSkK7SH4L7oCAACWhRS6D1WrFx8eHgIAA/ZIuIiIiIiIiIiJljs5vSVljMpmc3wcEBCgJLSKFVpiliS/erFtERERERERERERERERERKQIlIQWERERERERERERERERERGXUTtuERERERERERERERGRCsbX1xeHw+HuMESknFISuoTZbDasVqu7w5AKyGw2F1jjQ0RERERERERERERERKQ4KAldQhwOB/Hx8Zw+fdrdoUgFFhQURFhYWKEWjBcRERERERERERERERG5HEpCl5D8BHRISAg+Pj5KAkqJcjgcZGRkkJiYCEB4eLibIxIRERERERERERERd8rKymLYsGEAfPrpp3h5ebk5IhEpT5SELgE2m82ZgK5SpYq7w5EKytvbG4DExERCQkLUmltERERERERERESkArPZbHzzzTcAzJ07173BiEi5Y3R3ABVB/hrQPj4+bo5EKrr816DWJRcREREREREREREREZHiokroEqQW3IWXkZPL6QwrFg8jVf083R1OuaHXoIiIiIiIiIiIiIiIiBQ3VUJLqZSda+dkWjYpGarYFRERERERERERERERESlLlISWUsnbnLdecZbVhsPhcHM0IiIiIiIiIiIiIiIiIlJYSkJLqWTxMGIwGLA5HOTY7O4OR0REREREREREREREREQKSUloKZWMBgNeHnkvzyyrktAiIiIiIiIiIiIiIiIiZYVbk9Bnzpxh4sSJ1KpVC29vbzp37syGDRuc2x0OB9OmTSM8PBxvb2969uzJvn37CsyRlJTE0KFDCQgIICgoiFGjRpGWllZgzF9//cU111yDl5cXNWrUYObMmefE8vXXX9OoUSO8vLxo3rw5ixcvLp4HXYZ88sknVKlShezs7AK3Dxo0iGHDhhX7/r3OtuTOtNqKfV8iIiIiIiIiIiIiIhWJj48PaWlppKWl4ePj4+5wRKSccWsS+t577yUqKopPP/2Ubdu20bt3b3r27ElMTAwAM2fO5I033mD27NmsW7cOX19f+vTpQ1ZWlnOOoUOHsmPHDqKioli0aBGrV69mzJgxzu2pqan07t2bWrVqsWnTJv73v/8xffp03nvvPeeYtWvXcscddzBq1Cj+/PNPBg0axKBBg9i+fXuxPXaHw0FGTm6JfxVlfeXBgwdjs9n48ccfnbclJiby008/cc899xTH01KAt+XsutA5SkKLiIiIiIiIiIiIiLiSwWDA19cXX19fDAaDu8MRkXLG4ChKVtKFMjMz8ff354cffmDAgAHO29u2bUu/fv2YMWMGERERPPLIIzz66KMApKSkEBoayty5cxkyZAi7du2iSZMmbNiwgXbt2gGwZMkS+vfvz/Hjx4mIiODdd9/lySefJD4+HovFAsB//vMfFixYwO7duwG4/fbbSU9PZ9GiRc44OnbsSKtWrZg9e3ahHk9qaiqBgYGkpKQQEBBQYFtWVhaHDh2idu3aeHl5AZCRk0uTaUsv89m7fDuf7YOPxaPQ48eNG8fhw4edleGvvvoqb7/9Nvv37y/2D6W07FwOnkjDYjLSKDzg0neQSzrfa7G0sFqtLF68mP79+2M2m90djoiIiIiIiIiISJHo/JaIiJR3F8uH/lvhs5Eulpubi81mOycR5u3tze+//86hQ4eIj4+nZ8+ezm2BgYF06NCB6OhohgwZQnR0NEFBQc4ENEDPnj0xGo2sW7eOm266iejoaLp27epMQAP06dOHl156ieTkZCpVqkR0dDSTJk0qEEefPn1YsGDBBePPzs4u0KY6NTUVyPtFw2q1FhhrtVpxOBzY7Xbs9rz1jfP/LWn/jKEwRo0aRYcOHTh27BjVqlVj7ty5DB8+HIfDUaSq6svh5ZGX5M6x2cm12TDqSqwrZrfbcTgcWK1WTCaTu8MpIP998+/3j4iIiIiIiIiISFmg81tS1mRnZzNu3DgA3nnnHTw9Pd0ckYiUdkX5jHNbEtrf359OnToxY8YMGjduTGhoKF988QXR0dHUq1eP+Ph4AEJDQwvcLzQ01LktPj6ekJCQAts9PDyoXLlygTG1a9c+Z478bZUqVSI+Pv6i+zmfF198kWeeeeac25ctW3bO2gkeHh6EhYWRlpZGTk4OkNeOO3pSxwvOX1ysmemkZhU+mVu3bl2aNWvG+++/z3XXXceOHTuYN2+eM+le3DwMkOuAk8ln8HLbq7X8yMnJITMzk9WrV5Obm+vucM4rKirK3SGIiIiIiIiIiIhcNp3fkrIiKyuLTz/9FIABAwaUuu6ZIlL6ZGRkFHqsW9N6n376Kffccw/VqlXDZDLRpk0b7rjjDjZt2uTOsAplypQpBaqnU1NTqVGjBr179z5vO+5jx47h5+dX4CAeWGLRXpnRo0fzxhtvcOrUKXr06EGTJk1KbN+nczNJzbJi8vQiwNdy6TvIRWVlZeHt7U3Xrl1L3S8UVquVqKgoevXqpXZFIiIiIiIiIiJS5uj8lpQ16enpzu/79OmDr6+vG6MRkbKgKEWqbk1C161bl1WrVpGenk5qairh4eHcfvvt1KlTh7CwMAASEhIIDw933ichIYFWrVoBEBYWRmJiYoE5c3NzSUpKct4/LCyMhISEAmPyf77UmPzt5+Pp6Xne1hRms/mcXzBsNhsGgwGj0YjRaLzgnKXVXXfdxeTJk/nggw/45JNPSvQxeFlMpGZZybLay+RzV9oYjUYMBsN5X6elRWmOTURERERERERE5FJ0fkvKin++TvW6FZHCKMpxolRk9Xx9fQkPDyc5OZmlS5dy4403Urt2bcLCwli+fLlzXGpqKuvWraNTp04AdOrUidOnTxeonP7111+x2+106NDBOWb16tUFepRHRUXRsGFDKlWq5Bzzz/3kj8nfT0UXGBjILbfcgp+fH4MGDSrRfXub89YtzrLaSnS/IiIiIiIiIiIiIiIiInJ53JqEXrp0KUuWLOHQoUNERUXRvXt3GjVqxMiRIzEYDEycOJHnnnuOH3/8kW3btnH33XcTERHhTIQ2btyYvn37Mnr0aNavX8+aNWuYMGECQ4YMISIiAoA777wTi8XCqFGj2LFjB19++SWzZs0q0Er7oYceYsmSJbzyyivs3r2b6dOns3HjRiZMmOCOp6VUiomJYejQoeet/i5OXua8l2hWrh27w1Gi+xYRERERERERERERERGRonNrO+6UlBSmTJnC8ePHqVy5MrfccgvPP/+8s5R78uTJpKenM2bMGE6fPk2XLl1YsmRJgbVsP//8cyZMmECPHj0wGo3ccsstvPHGG87tgYGBLFu2jPHjx9O2bVuqVq3KtGnTGDNmjHNM586dmTdvHk899RRPPPEE9evXZ8GCBTRr1qzknoxSKjk5mZUrV7Jy5UreeeedEt+/xWTEZDBgczjIybXjdbYyWkRERERERERERERERERKJ4PDofJSV0hNTSUwMJCUlBQCAgIKbMvKyuLQoUPUrl27QAK9LIiMjCQ5OZmpU6fy6KOPuiWGA4lppOfkUqOyD5V8LG6Jobwoza9Fq9XK4sWL6d+/v9YeERERERERERGRMkfnt6SsSU9Px8/PD4C0tDR8fX3dHJGIlHYXy4f+m1sroaX0O3z4sLtDwMtiIj0nV+tCi4iIiIiIiIiIiIi4iI+PD4mJic7vRURcSUloKfW8z64LnZmjJLSIiIiIiIiIiIiIiCsYDAaCg4PdHYaIlFNGdwcgcin560BnWe2oe7yIiIiIiIiIiIiIiIhI6aYktJR6Xh4mDECu3U6uXUloEREREREREREREZErlZ2dzfjx4xk/fjzZ2dnuDkdEyhkloaXUMxoNeHrkVUNnal1oEREREREREREREZErlpubyzvvvMM777xDbm6uu8MRkXJGSWgpE5wtubUutIiIiIiIiIiIiIiIiEippiS0lAlelryXapYqoUVERERERERERERERERKNSWh5bJde+21TJw4sUT25W3Ob8dtB+Dw4cMYDAa2bNlSIvsXERERERERERERERERkcJRElouasSIERgMhnO+9u/fz3fffceMGTOuaH6DwcCCBQsuOS6/HXd2rg2b3XFF+4SLP66yau7cuQQFBbk7DBEREREREREREREREangPNwdgJR+ffv2Zc6cOQVuCw4OxmQyXfR+OTk5WCwWl8RgNhnxMBrJtdvJdlFL7gs9rsvhyscqIiIiIiIiIiIiIiIiUpapElouydPTk7CwsAJfJpPpnHbckZGRzJgxg7vvvpuAgADGjBlDTk4OEyZMIDw8HC8vL2rVqsWLL77oHA9w0003YTAYnD+fz/r16xnc5xra1wujc6cO/PnnnwW2JycnM3ToUIKDg/H29qZ+/frnJJgL+7gAVq1axVVXXYWnpyfh4eH85z//ITc313nfa6+9lgkTJjBx4kSqVq1Knz59ANi+fTv9+vXDz8+P0NBQhg0bxsmTJ533s9vtzJw5k3r16uHp6UnNmjV5/vnnndsff/xxGjRogI+PD3Xq1GHq1KlYrVbn9q1bt9K9e3f8/f0JCAigbdu2bNy4kZUrVzJy5EhSUlKcVd3Tp0+/6OMXERERERERERERERERKQ6qhHaz9PT0C24zmUx4eXkVaqzRaMTb2/uiY319fS8zysJ7+eWXmTZtGk8//TQAb7zxBj/++CNfffUVNWvW5NixYxw7dgyADRs2EBISwpw5c+jbt+8FK6vT0tIYOHAgXa69jhmvz+bMiVgeffTRAmOmTp3Kzp07+fnnn6latSr79+8nMzPzsh5DTEwM/fv3Z8SIEXzyySfs3r2b0aNH4+XlVSCx+/HHHzN27FjWrFkDwOnTp7nuuuu49957ee2118jMzOTxxx/ntttu49dffwVgypQpvP/++7z22mt06dKFuLg4du/e7ZzT39+fuXPnEhERwbZt2xg9ejT+/v5MnjwZgKFDh9K6dWveffddTCYTW7ZswWw207lzZ15//XWmTZvGnj17APDz87usxy8iIiIiIiIiIiIi5Z+3tzeHDh1yfi8i4kpKQrvZxRKF/fv356effnL+HBISQkZGxnnHduvWjZUrVzp/joyMLFCBC+BwXN5ayosWLSoQZ79+/fj666/PO/a6667jkUcecf589OhR6tevT5cuXTAYDNSqVcu5Lb/1dVBQEGFhYRfc/7x587Db7bwz+z0SM+z4NG/OY4+dYOzYsQX207p1a9q1awdw0arqSz2ud955hxo1avDWW29hMBho1KgRsbGxPP7440ybNg2jMa+BQP369Zk5c6bz/s899xytW7fmhRdecN720UcfUaNGDfbu3Ut4eDizZs3irbfeYvjw4QDUrVuXLl26OMc/9dRTzu8jIyN59NFHmT9/vjMJffToUR577DEaNWrkjCFfYGAgBoPhos+liIiIiIiIiIiIiAjkFbcV5ly6iMjlUBJaLql79+68++67zp8vVlGdnwTON2LECHr16kXDhg3p27cvAwcOpHfv3kXa/65du2jRogVB/r4kZpwhy2qjY8eOBcaMHTuWW265hc2bN9O7d28GDRpE586dL+tx7dq1i06dOmEwGJzbrr76atLS0jh+/Dg1a9YEoG3btgXm27p1KytWrDjvhQUHDhzg9OnTZGdn06NHjwvG9OWXX/LGG29w4MAB0tLSyM3NJSAgwLl90qRJ3Hvvvbz34Vyu7tadMcPvpF69ehd9nCIiIiIiIiIiIiIiIiIlSUloN0tLS7vgtn+3p05MTLzg2Pzq3HyHDx++orj+ydfXt9CJzn8nqNu0acOhQ4f4+eef+eWXX7jtttvo2bMn33zzTZHj8PQwYjQYsDscWHPtBbb169ePI0eOsHjxYqKioujRowfjx4/n5ZdfdsnjutD9/yktLY3rr7+el1566Zyx4eHhHDx48KLzRUdHM3ToUJ555hn69OlDYGAg8+fP55VXXnGOmT59Ojffehuffr2A31b8wqyZzzN//nxuuummy34cIiIiIiIiIiIiIlLx5OTk8OSTTwLw/PPPY7FY3ByRiJQnSkK7WVHWaS6uscUtICCA22+/ndtvv51bb72Vvn37kpSUROXKlTGbzdhstovev3Hjxnz66adkZ2fjaTaSmWNjTXT0OeOCg4MZPnw4w4cP55prruGxxx67aBL6Yvv79ttvcTgczmroNWvW4O/vT/Xq1S94vzZt2vDtt98SGRmJh8e5b6369evj7e3N8uXLuffee8/ZvnbtWmrVquX80Ac4cuTIOeOq167LsNHjGDZ6HFMeuJc5c+Zw0003YbFYLvlcioiIiIiIiIiIiIgAWK1W5zn06dOnKwktIi5lvPQQkcv36quv8sUXX7B792727t3L119/TVhYGEFBQUDeusfLly8nPj6e5OTk885x5513YjAYGD16NMcO7OW3X5fxxuuvFRgzbdo0fvjhB/bv38+OHTtYtGgRjRs3vqyYx40bx7Fjx3jggQfYvXs3P/zwA08//TSTJk06p+L8n8aPH09SUhJ33HEHGzZs4MCBAyxdupSRI0dis9nw8vLi8ccfZ/LkyXzyySccOHCAP/74gw8//BDIS1IfPXqU+fPnc+DAAd544w2+//575/yZmZlMmDCB5b+uJPb4Uf7c8Afb/txMZL0GzucyLS2N5cuXc/LkyQuuHy4iIiIiIiIiIiIiIiJSnJSElmLl7+/PzJkzadeuHe3bt+fw4cMsXrzYmcx95ZVXiIqKokaNGrRu3fq8c/j5+bFw4UK2bdtGv2s78+bM55g89dkCYywWC1OmTKFFixZ07doVk8nE/PnzLyvmatWqsXjxYtavX0/Lli25//77GTVqFE899dRF7xcREcGaNWuw2Wz07t2b5s2bM3HiRIKCgpyPd+rUqTzyyCNMmzaNxo0bc/vttzvbrN9www08/PDDTJgwgVatWrF27VqmTp3qnN9kMnHq1CkeGnsvN3Rrz+Pj7uHq7j0Z9eBk7A4HnTt35v777+f2228nODiYmTNnXtbjFxEREREREREREREREbkSBofD4XB3EOVBamoqgYGBpKSkEBAQUGBbVlYWhw4donbt2nh5ebkpwvIhPTuXAyfSMJuMNA4PuPQdyhmHw8GO2FTsDgf1Q/w4dDKDXLud6pW8qezrecn7l+bXotVqZfHixfTv3x+z2ezucERERERERERERIpE57ekrElPT8fPzw+AtLS0UrXMp4iUThfLh/6bWyuhbTYbU6dOpXbt2nh7e1O3bl1mzJjBP/PiDoeDadOmER4ejre3Nz179mTfvn0F5klKSmLo0KEEBAQQFBTEqFGjSEtLKzDmr7/+4pprrsHLy4saNWqct0r066+/plGjRnh5edG8eXMWL15cPA9cLpuXOe8la7XZybXZ3RxNycvOtWN3ODAZDHiZTQT75yWeE1Ozset6EhERERERERERERERESkF3JqEfumll3j33Xd566232LVrFy+99BIzZ87kzTffdI6ZOXMmb7zxBrNnz2bdunX4+vrSp08fsrKynGOGDh3Kjh07iIqKYtGiRaxevZoxY8Y4t6emptK7d29q1arFpk2b+N///sf06dN57733nGPWrl3LHXfcwahRo/jzzz8ZNGgQgwYNYvv27SXzZEihmIxGLB55L9ssq83N0ZS8zJy8x+xlMWEwGKjia8HDZCTHZic5PcfN0YmIiIiIiIiIiIiIiIi4OQm9du1abrzxRgYMGEBkZCS33norvXv3Zv369UBeFfTrr7/OU089xY033kiLFi345JNPiI2NZcGCBQDs2rWLJUuW8MEHH9ChQwe6dOnCm2++yfz584mNjQXg888/Jycnh48++oimTZsyZMgQHnzwQV599VVnLLNmzaJv37489thjNG7cmBkzZtCmTRveeuutEn9e5OK8zSYAMq0VrxI682ziPf85MBoNhORXQ59RNbSIiIiIiIiIiIiIiIi4n1uT0J07d2b58uXs3bsXgK1bt/L777/Tr18/AA4dOkR8fDw9e/Z03icwMJAOHToQHR0NQHR0NEFBQbRr1845pmfPnhiNRtatW+cc07VrVywWi3NMnz592LNnD8nJyc4x/9xP/pj8/Ujp4XU2AVshK6H/lYQGqOxjwWwyYrXZSVI1tIiIiIiIiIiIiIgUgre3N9u3b2f79u14e3u7OxwRKWc83Lnz//znP6SmptKoUSNMJhM2m43nn3+eoUOHAhAfHw9AaGhogfuFhoY6t8XHxxMSElJgu4eHB5UrVy4wpnbt2ufMkb+tUqVKxMfHX3Q//5adnU12drbz59TUVACsVitWq7XAWKvVisPhwG63Y7dXvOpdV/M6244702qrcM+nsx232VjgsQf7WYhNyeLEmWwqeZsxGM5/f7vdjsPhwGq1YjKZzj/ITfLfN/9+/4iIiIiIiIiIiJQFOr8lZVGDBg0AsNls2GwVr/BLRIqmKJ9xbk1Cf/XVV3z++efMmzePpk2bsmXLFiZOnEhERATDhw93Z2iX9OKLL/LMM8+cc/uyZcvw8fEpcJuHhwdhYWGkpaWRk6NK1SuVezb3mm21kZKSesGEa3ljtYPdAQYgKz2N7H88bpMDTAaw2uzEnkrF33L+OXJycsjMzGT16tXk5uaWSNxFFRUV5e4QRERERERERERELpvOb4mISHmVkZFR6LFuTUI/9thj/Oc//2HIkCEANG/enCNHjvDiiy8yfPhwwsLCAEhISCA8PNx5v4SEBFq1agVAWFgYiYmJBebNzc0lKSnJef+wsDASEhIKjMn/+VJj8rf/25QpU5g0aZLz59TUVGrUqEHv3r0JCAgoMDYrK4tjx47h5+eHl5fXpZ8YuaSErDPY7A48ff2cldHlXUpmLmRk4G0xERjoe852u9lKzOlMzuQaiajid97kfFZWFt7e3nTt2rXUvRatVitRUVH06tULs9ns7nBERERERERERESKROe3pKzJycnhv//9L5DXufafS5qKiJxPfmfownBrEjojIwOjsWAC0WQyOdsM165dm7CwMJYvX+5MOqemprJu3TrGjh0LQKdOnTh9+jSbNm2ibdu2APz666/Y7XY6dOjgHPPkk09itVqdH/5RUVE0bNiQSpUqOccsX76ciRMnOmOJioqiU6dO543d09MTT0/Pc243m83n/IJhs9kwGAwYjcZzHq9cHi+zifTsXLKtdnwsbn0Zl5is3Pz1oD3O+zqq5GvhxJlscmx2kjKsBPuf+/o0Go0YDIbzvk5Li9Icm4iIiIiIiIiIyKXo/JaUFTk5OTz33HNAXhJar1sRuZSiHCfcmhG9/vrref755/npp584fPgw33//Pa+++io33XQTAAaDgYkTJ/Lcc8/x448/sm3bNu6++24iIiIYNGgQAI0bN6Zv376MHj2a9evXs2bNGiZMmMCQIUOIiIgA4M4778RisTBq1Ch27NjBl19+yaxZswpUMj/00EMsWbKEV155hd27dzN9+nQ2btzIhAkTSvx5kUvzNuetZ5xprThrVOSvB+1tOf/b1mgwEBKQV9184kw2NrujxGITEZHCczgcJKdreQ4REREREREREREpv9yahH7zzTe59dZbGTduHI0bN+bRRx/lvvvuY8aMGc4xkydP5oEHHmDMmDG0b9+etLQ0lixZUqCV8Oeff06jRo3o0aMH/fv3p0uXLrz33nvO7YGBgSxbtoxDhw7Rtm1bHnnkEaZNm8aYMWOcYzp37sy8efN47733aNmyJd988w0LFiygWbNmJfNkSJF4nU1CZ5VwEjoyMpLXX3/d+bPBYGDBggXFvl+Hw+FMuOcn4M8nyMeMxcNIrt3OqfTsYo9LRESK7qdtcbSeEcX7qw+6OxQRERERERERERGRYuHWJLS/vz+vv/46R44cITMzkwMHDvDcc88VWHfAYDDw7LPPEh8fT1ZWFr/88gsNGjQoME/lypWZN28eZ86cISUlhY8++gg/P78CY1q0aMFvv/1GVlYWx48f5/HHHz8nnsGDB7Nnzx6ys7PZvn07/fv3L54HXoaMGDHCWXX+b1u3buWGG24gJCQELy8vIiMjuf3220lMTGT69OkYDIaLfuXPbzAYuP/++8+Zf/z48RgMBkaMGHHONm9z3ks3y2rD4XBfxW9cXBz9+vUr9v1YbXZsdgcGgwHPiyShjQYDof55F2icVDW0iEiptGxHAgCzVx0o8YupREREREREREREREqCFiiWy3LixAl69OhB5cqVWbp0Kbt27WLOnDlERESQnp7Oo48+SlxcnPOrevXqPPvsswVuy1ejRg3mz59PZmam87asrCzmzZtHzZo1z7t/Tw8TBgzk2h1Ybe5LtIaFhZ13bXBXy7TmrZPu5WHEeDaBfyFBPmY8PUzk2h2cSlM1tIhIabM9JgWAU+k5/PRX3CVGi4iIiIiIiIiIiJQ9SkLLZVmzZg0pKSl88MEHtG7dmtq1a9O9e3dee+01ateujZ+fH2FhYc4vk8mEv79/gdvytWnThho1avDdd985b/vuu++oWbMmrVu3Pu/+jUYDnmeroevXrcNzzz3H3XffjZ+fH7Vq1eLHH3/kxIkT3Hjjjfj5+dGiRQs2btxYYI7ff/+da665Bm9vb2rUqMGDDz5Ienq6c3tiYiLXX3893t7e1K5dm88///ycOP7djvvxxx+nQYMG+Pj4UKdOHaZOnYrVanVunz59Oq1ateLTTz8lMjKSwMBAhgwZwpkzZy76fDvXg75IFfQ/YwoJyEuMn0jLxma3X/I+IiJSMs5kWTl48u/PmrlrD7u1o4eIiIiIiIiIiIhIcVAS2k0cDgeZmTkl/uWqE91hYWHk5uby/fffu2TOe+65hzlz5jh//uijjxg5cuRF75O/LrQDB6+99hpXX301f/75JwMGDGDYsGHcfffd3HXXXWzevJm6dety9913O2M9cOAAffv25ZZbbuGvv/7iyy+/5Pfff2fChAnO+UeMGMGxY8dYsWIF33zzDe+88w6JiYkXjcnf35+5c+eyc+dOZs2axfvvv89rr71WYMyBAwdYsGABixYtYtGiRaxatYr//ve/F53XuR605dJJaIAg77xqaJvdwcm0nELdR0REit/O2FQAKvtasHgY2RaTwuajp90blIiIiIiIiIiIiIiLebg7gIoqK8tKj17TS3y/y6Om4+1tufTAS+jYsSNPPPEEd955J/fffz9XXXUV1113HXfffTehoaFFnu+uu+5iypQpHDlyBMirtJ4/fz4rV6684H28zUZOA3YH9O/fn/vuuw+AadOm8e6779K+fXsGDx4M5FUod+rUiYSEBMLCwnjxxRcZOnQoEydOBKB+/fq88cYbdOvWjXfffZejR4/y888/s379etq3bw/Ahx9+SOPGjS/6OJ566inn95GRkTz66KPMnz+fyZMnO2+32+3MnTsXf39/AIYNG8by5ct5/vnnLzhvfhLaqxCV0JBXDR0a4MnRpAxOpmVTxc+Ch1HXnIiIuNv2s0notrUqEeRt5utNx/l47WHa1qrk5shEREREREREpKLx8vJi/fr1zu9FRFxJWSm5bM8//zzx8fHMnj2bpk2bMnv2bBo1asS2bduKPFdwcDADBgxg7ty5zJkzhwEDBlC1atUCYz7//HP8/PycX5vXRQPgcECLFi2c4/KT4M2bNz/ntvxK5q1btzJ37twC8/Xp0we73c6hQ4fYtWsXHh4etG3b1jlHo0aNCAoKuujj+PLLL7n66qsJCwvDz8+Pp556iqNHjxYYExkZ6UxAA4SHh1+0wtpqs5Nrs2OgcO248wV6m/EyqxpaRKQ0yV8PullEIMM7RwKweFscialZboxKRERERERERCoik8lE+/btad++PSZT4c89i4gUhiqh3cTLy8zyqOlu2a8rValShcGDBzN48GBeeOEFWrduzcsvv8zHH39c5LnuueceZzvst99++5ztN9xwAx06dHD+HBIWzuHTVsCByePvl7LBYADAbDafc5v97PrIaWlp3HfffTz44IPn7KdmzZrs3bu3yPFHR0czdOhQnnnmGfr06UNgYCDz58/nlVdeKTDun3Hlx2a/yLrN+etBe5pNGI2GQsdjMBgI9ffkSFIGp85kU9X3yivgRUTkymw7m4RuXj2AZtUCaVerEhuPJPP5uqM83KuBm6MTERERERERERERcQ0lod3EYDC4pC12aWKxWKhbty7p6emXdf++ffuSk5ODwWCgT58+52z39/cvUEEMYD6T19Y013bhJO75tGnThp07d1KvXr3zbm/UqBG5ubls2rTJ2Y57z549nD59+oJzrl27llq1avHkk086b8tvL34lnOtBF6EKOl/A2WroLKuNk2nZBHkWPoktIiKulZGTy4ETaQA0qxYIwPDOkc4k9Pju9bB4qEmNiIiIiIiIiJSMnJwcZs2aBcBDDz2ExVK+chYi4l5KQsslpaSksGXLlgK3bdu2jaVLlzJkyBAaNGiAw+Fg4cKFLF68mDlz5lzWfkwmE7t27XJ+Xxj5ayRbbY4i7evxxx+nY8eOTJgwgXvvvRdfX1927txJVFQUb731Fg0bNqRv377cd999vPvuu3h4eDBx4kS8vb0vOGf9+vU5evQo8+fPp3379vz00098//33RYrrfPIroQu7HvQ/5a0N7cWRU+mcTMvBz8O1lfAiIlJ4O2NTcTggxN+TEP+8dZb6NgsjNMCThNRsFm+LY1Dram6OUkREREREREQqCqvVyuTJkwEYN26cktAi4lIqt5FLWrlyJa1bty7wNWfOHHx8fHjkkUdo1aoVHTt25KuvvuKDDz5g2LBhl72vgIAAAgICCj3ey5z3ErYWsRK6RYsWrFq1ir1793LNNdfQunVrpk2bRkREhHPMnDlziIiIoFu3btx8882MGTOGkJCQC855ww038PDDDzNhwgRatWrF2rVrmTp1apHiOp+s/Epoy+WtyRHg5YG32YTd4SApQ2tDi4i4i7MV99kqaACzychdHWoBMHftYXeEJSIiIiIiIiIiIuJyBofDUbQSUjmv1NRUAgMDSUlJOSeJmpWVxaFDh6hduzZeXl5uirB8Op2Rw9GkDHwsHtQL8XN3OC6Xa7OzMy6v5XjTiABMxsu7biQ108rhU+lgs2JMO0G9unVK3WvRarWyePFi+vfvf8662SIi5cEjX23l283HeahH/QLrP59My6bzi7+SY7Pzw/iraVkjyH1BioiIiIiIyGXT+S0pa9LT0/HzyzuvnpaWhq+vr5sjEpHS7mL50H9TJbSUafktqrOsNsrj9RT560F7epguOwEN4O/lgY/FhMPhIC0711XhiYhIEWw/Wwnd7B+V0ABV/TwZ2CIcgI9VDS0iIiIiIiIiIiLlgJLQUqZ5ehgxGgzYHQ5ycovWkrssyE9Ce5uv7K2avzY0QFp2LqfSsq84NhERKbzMHBv7Es8ABdtx5xveORKARX/FcVLHaBERERERERERESnjlISWMs1gMDirofMTtuVJZk7eY/K6zPWg/8nP0wMvswmHA77YcPSK5xMRkcLbFZ+K3ZFX9Rwa4HnO9pY1gmhVI4gcm50v1ukYLSIiIiIiIiIiImWbktBS5nmdrRLOKodJ6CxnJfSVJ6ENBgNV/SwALNoaR3xK1hXPKSIihfN3K+4ADAbDeceMOFsN/dm6I1ht5a+7h4iIiIiIiIiIiFQcSkKXoPK4ZnFp4O2shC5fJ+xtdjvZZ1uMuyIJnT+Pp4eR7Fw776zc75I5RUTk0rYdz0tCn68Vd77+zcMJ9vckITWbJdvjSyo0EREREREREamgvLy8WLFiBStWrMDLy8vd4YhIOaMkdAkwm80AZGRkuDmS8im/HXd5q4TOT6pbTEY8TK55q2ZmZuLn5UFylp35648RczrTJfOKiMjFbY9NBaBpxIWT0BYPI3deVROAj9ceLomwRERERERERKQCM5lMXHvttVx77bWYTK4phBIRyefh7gAqApPJRFBQEImJiQD4+PhcsBWnXAa7A0duDjm5kJbu4bKErbulpufgyM3BbPIgK+vKWmc7HA4yMjJITEwkPLgKrWumEX3wFG+v2M8LNzV3UcQiInI+WVYb+xLOANC8+oWT0ABDO9Tk7RX72Xgkme0xKTS7SOW0iIiIiIiIiIiISGmlJHQJCQsLA3AmosW1klKyyLU7cKRa8HRR62p3S0rPISPHRpa3BzmnzS6ZMygoiLCwMB7u5Un0/0Xz1YZjjO1WlxqVfVwyv4iInGtP/Bly7Q4q+1qICLx4a6uQAC/6Nw/nx62xfLz2MP8b3LKEohQRERERERGRisZqtfLee+8BMGbMGGdXVxERV1ASuoQYDAbCw8MJCQnBarW6O5xy5+Mfd/DbvhPc17Uut7Wv4e5wXGLGnPUcScrghZua07J2lSuez2w2O1uqXFW7Ml3qVeX3/Sd569f9vHRriyueX0REzm9bTN560E0jAgrVCWV450h+3BrLD1tjmdK/MZV9LcUdooiIiIiIiIhUQDk5OUyYMAGAESNGKAktIi6lJHQJM5lMWluhGFSrEkDM5ni2xKZzt9fFq8zKgoycXP44ega7AxrXqIJXMTymh3vV5/f9J/lm83HGda9LrSq+Lt+HiIjA9rNJ6OaFbK3dpmYQzasFsi0mhfkbjjLu2nrFGZ6IiIiIiIiIiIiIy7l18dzIyEgMBsM5X+PHjwcgKyuL8ePHU6VKFfz8/LjllltISEgoMMfRo0cZMGAAPj4+hISE8Nhjj5Gbm1tgzMqVK2nTpg2enp7Uq1ePuXPnnhPL22+/TWRkJF5eXnTo0IH169cX2+MW12scHgDAzrhUN0fiGrvi8hLQIf6ehPgXT1K9ba3KdGsQjM3u4M1f9xfLPkREBLbHFi0JbTAYGN45EoDPoo+Qa7MXV2giIiIiIiIiIiIixcKtSegNGzYQFxfn/IqKigJg8ODBADz88MMsXLiQr7/+mlWrVhEbG8vNN9/svL/NZmPAgAHk5OSwdu1aPv74Y+bOncu0adOcYw4dOsSAAQPo3r07W7ZsYeLEidx7770sXbrUOebLL79k0qRJPP3002zevJmWLVvSp08frd9chjSJyEtC709MIzvX5uZortyOswmLZoVMWFyuh3s1AOC7zcc5dDK9WPclIlIRZefa2BN/BijaMX1gi3Cq+FqITckiamfCpe8gIiIiIiIiIiIiUoq4NQkdHBxMWFiY82vRokXUrVuXbt26kZKSwocffsirr77KddddR9u2bZkzZw5r167ljz/+AGDZsmXs3LmTzz77jFatWtGvXz9mzJjB22+/TU5ODgCzZ8+mdu3avPLKKzRu3JgJEyZw66238tprrznjePXVVxk9ejQjR46kSZMmzJ49Gx8fHz766CO3PC9SdOGBXgR6m8m1O9iXkObucK5YfuvWZmeT68WlVY0gejQKwe6AN5bvK9Z9iYhURHvj07DaHAR6m6leybvQ9/Mym7jjqpoAzF17uJiiExERERERERERESkepWZN6JycHD777DMmTZqEwWBg06ZNWK1Wevbs6RzTqFEjatasSXR0NB07diQ6OprmzZsTGhrqHNOnTx/Gjh3Ljh07aN26NdHR0QXmyB8zceJE5343bdrElClTnNuNRiM9e/YkOjr6gvFmZ2eTnZ3t/Dk1Na8NtNVqxWq1XtFzIZencZgffxxKZvvxZBqG+Lg7nCuy7XheErpRqF+xv54e6F6H5bsT+WFLDGO61KJeiF+x7u9C8h+n3j8iUp5sPZYEQNMI/3OWC7mU29pG8O6qA6w7lMS2Y0k0CvMvjhBFRERERETERXR+S8qaf75WldsQkcIoynGi1CShFyxYwOnTpxkxYgQA8fHxWCwWgoKCCowLDQ0lPj7eOeafCej87fnbLjYmNTWVzMxMkpOTsdls5x2ze/fuC8b74osv8swzz5xz+7Jly/DxKdsJ0LLKM8sIGPk5ehve8VvdHc5ly7XDngQTYCBhz0YWHy7+fTavZGRbspEn5/3O8AbuXXs0vy2/iEh5sPhg3meTd+ZJFi9eXOT7N69kZMspIy98vYYhdbU2tIiIiIiISFmg81tSVmRlZTm/X7p0KV5eXm6MRkTKgoyMjEKPLTVJ6A8//JB+/foRERHh7lAKZcqUKUyaNMn5c2pqKjVq1KB3794EBBRvC2U5v6w/Y1j13Q6yvKvQv397d4dz2bbHpGJf9weVfMwMHdQLg8FQ7Pus3foMN7wTzZ9JRma0vZoGoSVfbWe1WomKiqJXr16YzeYS37+ISHH4YPYfQCo3dGlF/+ZhRb5/cJNk7vxwA38me/DGtd0I8tHxUUREREREpLTS+S0pa3Jzc1mwYAEAvXv3xsOj1KSMRKSUyu8MXRil4ohy5MgRfvnlF7777jvnbWFhYeTk5HD69OkC1dAJCQmEhYU5x6xfv77AXAkJCc5t+f/m3/bPMQEBAXh7e2MymTCZTOcdkz/H+Xh6euLp6XnO7WazWb9guEmz6pUA2BV3Bg8PjxJJ3haH3YnpADSrFojFYimRfbaoWZn+zcNYvC2et1cd4p2hbUtkv+ej95CIlBdWm5098WkAtKpV+bKObZ3qBdM4PIBdcal8vzWOMV3rujpMERERERERcTGd35Kywmw2c+ONN7o7DBEpQ4ry+WYsxjgKbc6cOYSEhDBgwADnbW3btsVsNrN8+XLnbXv27OHo0aN06tQJgE6dOrFt2zYSExOdY6KioggICKBJkybOMf+cI39M/hwWi4W2bdsWGGO321m+fLlzjJQN9UP8MZsMpGblEpuSdek7lFI7YvPWg24aEVii+32oRwMMBli8LZ6dsYW/kkVERM5vb8IZcmx2/L08qFn58pbqMBgMjOhcC4BPoo9gsztcGaKIiIiIiIiIiIhIsXB7EtputzNnzhyGDx9eoNVDYGAgo0aNYtKkSaxYsYJNmzYxcuRIOnXqRMeOHYG89hBNmjRh2LBhbN26laVLl/LUU08xfvx4Z5Xy/fffz8GDB5k8eTK7d+/mnXfe4auvvuLhhx927mvSpEm8//77fPzxx+zatYuxY8eSnp7OyJEjS/bJkCti8TBSN9gPoEwnUbfH5MXerFrJtnVvGObPgObhAMxavrdE9y0iUh7tyD+eRwReUXeOG1tVI8jHzPHkTJbvSrj0HURERERERERECsFqtTJ37lzmzp2L1Wp1dzgiUs64PQn9yy+/cPToUe65555ztr322msMHDiQW265ha5duxIWFlagZbfJZGLRokWYTCY6derEXXfdxd13382zzz7rHFO7dm1++uknoqKiaNmyJa+88goffPABffr0cY65/fbbefnll5k2bRqtWrViy5YtLFmyhNDQ0OJ98OJyTSLyEre74spmEjrXZnfGXtKV0AATe9bHYIClOxLYHpNS4vsXESlPtp09jl7pRUVeZhND2tcE4OPow1caloiIiIiIiIgIADk5OYwcOZKRI0eSk5Pj7nBEpJxx+5rQvXv3xuE4f2tJLy8v3n77bd5+++0L3r9WrVosXrz4ovu49tpr+fPPPy86ZsKECUyYMOHSAUup1iQ8gO+IKbOV0AdOpJOda8fP04Nal9m69UrUC/HnxpYRLNgSy+u/7OWD4e1LPAYRkfLi7yT0lV9UdFfHmry3+gBr9p9iX8IZ6of6X/GcIiIiIiIiIiIiIsXF7ZXQIq7UJPxsJXR82UxC51cfN4kIwGi8/NatV+LBHvUxGuCXXYlsPXbaLTGIiJR1/+xs0dwFSejqlXzo1SSvQ4uqoUVERERERERERKS0UxJaypXGZ5PQR05lcCar7K1hsT32bNWcG1px56sT7MdNrasD8NovWhtaRORy7D+R5uxsEVnF1yVzDu8cCcB3m2NIySx7n3EiIiIiIiIiIiJScSgJLeVKJV8L4YFeAOyJP+PmaIpuR0xe1dyVrh96pR7sUQ+T0cDKPSfYdCTZrbGIiJRF2467vrNFpzpVaBjqT0aOja83HnPJnCIiIiIiIiIiIiLFQUloKXfyq6F3xpWtltx2u4Mdsa5bP/RK1Kriy61t8qqhX1c1tIhIke2IdV0r7nwGg4G7O9cC4NM/jmC3O1w2t4iIiIiIiIiIiIgrKQkt5Y5zXegyloQ+fCqd9BwbXmYjdaq6pnXrlZhwXT08jAZ+23eSDYeT3B2OiEiZsi0m/6Ii13a2uKl1NQK8PDhyKoOVexNdOreIiIiIiIiIiIiIqygJLeWOsxI6tmwlobefjbdxeAAeJve/NWtU9mFwuxoAvBalamgRkcKy2R3OzyBXVkID+Fg8uL193rF57tojLp1bRERERERERCoWT09PvvrqK7766is8PT3dHY6IlDPuz3SJuFiTiLwk9O74M+Ta7G6OpvB25FfNRbi3Ffc/TbiuHmaTgbUHThF94JS7wxERKRMOnkgj02rDx2KidlU/l88/rGMkBgOs3nuCAyfSXD6/iIiIiIiIiFQMHh4eDB48mMGDB+Ph4eHucESknFESWsqdWpV98LGYyM61c/hUurvDKbTtscXTuvVKVAvyZkj7mgC89steHA6tPyoicin5rbibhAdgMhpcPn/NKj70aBQCwKfRqoYWERERERERERGR0kdJaCl3jEYDjcL8AdgZd8bN0RSOw+Fge0xe69ampagSGmBc97pYPIysP5TEWlVDi4hcUv7xvJmLW3H/0/DOkQB8s+k4Z7KsxbYfERERERERESm/cnNz+frrr/n666/Jzc11dzgiUs4oCS3lUllbF/p4ciYpmVbMJgMNQv3dHU4B4YHe3HlVXjX0q1GqhhYRuZTtZyuhXb0e9D91qVeVusG+pGXn8u2m48W2HxEREREREREpv7Kzs7ntttu47bbbyM7Odnc4IlLOKAkt5VL+utC74spGEnrH2VbcDcP8sXiUvrfluGvr4ulhZNORZFbvO+nucERESi273eE8phdnJbTBYHBWQ38SfQS7XRcIiYiIiIiIiIiISOlR+rJdIi7grIQuI0loZ+vWUtaKO19IgBfDOtYCVA0tInIxh06lk55jw8tspG6wb7Hu6+Y21fH39ODgyXR+268LhERERERERERERKT0UBJayqVGYf4YDHDiTDYnzpT+NiLbz1bNNS3GqrkrdV+3unibTWw9dpoVexLdHY6ISKmU34q7cXgAHqbi/TXLz9ODW9tVB+DjtYeLdV8iIiIiIiIiIiIiRaEktJRLPhYPalfJq0Ar7S25HQ6HM2nR7Gwb8dIo2N+TuzvlVUO/FrVP1dAiIuex7Xjxrwf9T3d3igRgxZ5EjpxKL5F9ioiIiIiIiIiIiFyKktBSbjUuI+tCJ57J5mRaDiajwdlGvLQa07UOPhYT22JS+GWXqqFFRP5tewmsB/1Ptav6cm3DYByOvLWhRUREREREREREREoDJaGl3GpSRtaFzq+Crhfsh5fZ5OZoLq6KnycjOkcCeWtD2+2qhhYRyWe3O9gRk/eZ0yyi5JZXGH72uPzVhmOkZ+eW2H5FRERERERERERELsSjMINuvvnmIk88e/ZsQkJCinw/EVfJT0KX9kro7WcTFk2rle4q6Hyjr6nDJ9FH2BWXyrKd8fRtFu7ukERKxOajySSl5dCzSai7Q5FS6khSBmeyc7F4GKkf6ldi++1WP5jaVX05dDKd7/6MYVjHWiW2bxEREREREREpuywWC3PmzHF+LyLiSoWqhF6wYAEWi4XAwMBCff3000+kpaUVd+wiF5Xf2vrAiXSyrDY3R3NhztatJVg1dyUq+Vq45+pIIG9taFVDS3l39FQGYz/bxM3vrOXeTzay9sBJd4ckpVR+Z4vG4QGYTSXXbMZoNDgTz5+sPYzDoeOyiIiIiIiIiFya2WxmxIgRjBgxArPZ7O5wRKScKVQlNMAbb7xR6Mrmb7755rIDEnGV0ABPKvtaSErPYV9CGs2rl84k746Ykl0/1BVGdanDnLWH2ZNwhsXb4xjYIsLdIZUZLyzexW/7TlLZ10wlHwuVff/15WOhkq+FKr4WgnwsWDy0aoK7nMmy8vaKA3z0+yFybHbn7T/8GUvnulXdGJmUVvlJ6GYRJd/Z4tZ21Xll2R72Jaax9sAprq6n16iIiIiIiIiIiIi4T6GS0CtWrKBy5cqFnvTnn3+mWrVqhRobExPD448/zs8//0xGRgb16tVjzpw5tGvXDgCHw8HTTz/N+++/z+nTp7n66qt59913qV+/vnOOpKQkHnjgARYuXIjRaOSWW25h1qxZ+Pn93Qrzr7/+Yvz48WzYsIHg4GAeeOABJk+eXCCWr7/+mqlTp3L48GHq16/PSy+9RP/+/Qv9uKV0MRgMNA73Z83+U+yMSymVSehTadnEpmQB0MQNSYvLFehj5t4udXjtl728/ss++jULx2Q0uDusUm9/YhrvrT5YpPv4e3pQ2c9y0YT1P38O8PbAYND/xZWw2R18tfEYryzbw8m0HACuqV+V3k1CmfrDDn7eHseMQc10gYCcI7+zRXM3XFQU4GXmlrbV+ST6CHPXHlYSWkRERKScy8m1k51rw99LVWsiInL5cnNzWbp0KQB9+vTBw6PQdYsiIpdUqCNKt27dijRply5dCjUuOTmZq6++mu7du/Pzzz8THBzMvn37qFSpknPMzJkzeeONN/j444+pXbs2U6dOpU+fPuzcuRMvLy8Ahg4dSlxcHFFRUVitVkaOHMmYMWOYN28eAKmpqfTu3ZuePXsye/Zstm3bxj333ENQUBBjxowBYO3atdxxxx28+OKLDBw4kHnz5jFo0CA2b95Ms2bNivT4pfRoEh7Amv2n2BV3xt2hnNeO2Lz1oOtU9cXPs2x9wI/sEslHaw6xPzGNRX/FcmOrwl14UpF9u/k4AFfVrsydV9XkVHoOyek5zn+TMnJIOvt9ckYOdgecyc7lTHYuR05lFGofHkYDQT55ldSVfM0XTFhX8rFQ5Wxy28tsKs6HXaasPXCSZxfuZHd83jGjTlVfnhrYmO4NQ7A74M1f95N4Jpvf9p2gR2OtDS1/czgcbI/JO6a7q7PF3Z0i+ST6CMt3JXAsKYMalX3cEoeIiIiIFL8J8zbz+/6T/PzQNdSq4uvucEREpIzKzs5m4MCBAKSlpSkJLSIuVeQjSrdu3Rg1ahSDBw/G29v7inb+0ksvUaNGDefC9wC1a9d2fu9wOHj99dd56qmnuPHGGwH45JNPCA0NZcGCBQwZMoRdu3axZMkSNmzY4KyefvPNN+nfvz8vv/wyERERfP755+Tk5PDRRx9hsVho2rQpW7Zs4dVXX3UmoWfNmkXfvn157LHHAJgxYwZRUVG89dZbzJ49+4oep7hP/rrQO88me0ub/Kq5pmWoFXe+AC8zY7rW4X9L9zDrl30MaB6ORwmugVrW2OwOvjubhB7ZOZJ+zcMvOt5ud5CSaSUp4zyJ6rSCCev829JzbOTaHZxMy+ZkWnahY6vqZ2FwuxqM6BxJaIDXFT3OsurwyXReWLyLZTsTAAjw8mBizwbc1bGWs+LZZIABLcKZs+YwC7fGKgktBRxLyiQl04rFZKRBqL9bYqgX4sc19avy276TfPbHEab0b+yWOERERESkeCWeySJqVwIOB/y8PZ77u9V1d0giIiIiIucochK6devWPProozzwwAPcdtttjBo1io4dO17Wzn/88Uf69OnD4MGDWbVqFdWqVWPcuHGMHj0agEOHDhEfH0/Pnj2d9wkMDKRDhw5ER0czZMgQoqOjCQoKciagAXr27InRaGTdunXcdNNNREdH07VrVywWi3NMnz59eOmll0hOTqZSpUpER0czadKkAvH16dOHBQsWXNZjk9Ihv8X1rrhUHA5HqWtTvCO/aq4MteL+p+GdI/ngt4McPJnOj1tjublNdXeHVGr9vv8kCanZBPmYua5xyCXHG40GKvnmVS8TXLh9ZFltnM6wcio9m+R069nkdDZJGda8ZPU/v84mt/OS1jm8u/IAH/x2kOtbRjD6mjrOCzjKu9QsK2/9up85aw5htTkwGQ3c1aEmE3s2yHvu/+X6lhHMWXOYqJ0JZObY8Laoilzy5F9U1DDM362t2od3iuS3fSeZv+EYE3s20GtUREREpBz6dVciDkfe96v3nlASWkRERERKpSInoV9//XVefvllfvzxRz7++GO6du1KvXr1uOeeexg2bBihoYWvDDt48CDvvvsukyZN4oknnmDDhg08+OCDWCwWhg8fTnx8PMA5c4aGhjq3xcfHExJSMKHj4eFB5cqVC4z5Z4X1P+eMj4+nUqVKxMfHX3Q//5adnU129t+VhqmpeclEq9WK1Wot9HMgxatmkCdmk4Ez2bkcOpFKjUqlqzXptpjTADQK9S2TrxtPI4y6OpKXo/bx+i976dck+LKrofMff1l8Hgrj6w1HAbi+eRhGhx2r1e7yfZiAKj4mqvj4FCpx7XA4OJOVy/rDyXy45jAbj5zmu80xfLc5hqvrVmHU1bXoUq9Kqbt4wxVybXa+3hzD68v3k5Se95q7pl4VpvRrSP0QP+D8r8VmYb5UC/Ii5nQWUTti6dcsrETjltJr69FkAJqE+7n1ONalbiWqV/LmeHIm3246yu3tdHGQiIiISHmzbMff56o2HE7idFomvmVsiS+R8qq8n9+S8uefr1XlNkSkMIpynLis31A9PDy4+eabufnmm0lMTOS9995j6tSpPPHEE/Tv358HH3yQ66677pLz2O122rVrxwsvvADkVVlv376d2bNnM3z48MsJrcS8+OKLPPPMM+fcvmzZMnx8Sleis6IL9TJxPN3A5z+tokVlh7vDccrIhaNJeW/BmO3rWLzHzQFdphAb+HmYOJqUybOfLqVjyJU9x1FRUS6KrPTIyIWl202AgdCMQyxefMjdIZ1jWAR09YcVcUa2njKw5sAp1hw4RZi3g+4RdtpVdeDG4k6X2pNi4PvDRuIy8pLrod4OBtWy06RSAvs2JrDvEvdv5GMk5rSRD6O24Djq+osJpGxaudMIGHEkHWXx4iNujaVtgIHjySbejtqBX8JflMPrSEREREQqrBwb/LY37+9Li9FBjg3e/iaKZpVKz/kOESmf57ekfMrKynJ+v3TpUry8KuZSfSJSeBkZGYUee0WXSa5fv545c+Ywf/58QkJCGDFiBDExMQwcOJBx48bx8ssvX/T+4eHhNGnSpMBtjRs35ttvvwUgLCyvwiwhIYHw8L/XT01ISKBVq1bOMYmJiQXmyM3NJSkpyXn/sLAwEhISCozJ//lSY/K3/9uUKVMKtO9OTU2lRo0a9O7dm4CAitHGtqxYlbWd43/G4hNRn/7X1XN3OE7rDiXBho1UD/Ji8I1d3R3OFUmqfJj/LtnLb6d8mTqsC+bLqIa2Wq1ERUXRq1cvzGZzMUTpPvM3HMfq2En9EF/GDO5cqiuLxwLHkzP55I+jfLXxOPGZNr44YCIqwcJdHWpy51XVqeRzbpvqsuDwqXT+u2Qvy3efACDI28yD19VlSPvqRXrNRsalsvydP9id6sE1112Lv5cqDio6h8PB9K0rAStDenemebVAt8ZzdaaVZf9bRVyGnapNOtKhdmW3xiMiIiIirvPLrkSs67dQLciLa+pXZf6G42QFRtK/f2N3hyYilO/zW1I+paenO7/v06cPvr6+boxGRMqC/M7QhVHkM+eJiYl8+umnzJkzh3379nH99dfzxRdf0KdPH2diZcSIEfTt2/eSSeirr76aPXsKln/u3buXWrVqAVC7dm3CwsJYvny5M+mcmprKunXrGDt2LACdOnXi9OnTbNq0ibZt2wLw66+/Yrfb6dChg3PMk08+idVqdX74R0VF0bBhQypVquQcs3z5ciZOnOiMJSoqik6dOp03dk9PTzw9Pc+53Ww26xeMUqZZtSC++zOWPQnpper/ZndC3gd8s2pBpSquyzG8cx0++P0Ix09nsWh7Ire1q3HZc5XH99D3W2IBGNyuRoG16Uur2iFmnr6hGQ/3bsj89UeZs+YwcSlZvL58P7NXH+TWttUZ1aUOtauWjV9KUzKsvPHrPj6JPozV5sDDaGBYp1o81KM+QZeRUG9RozJ1qvpy8GQ6q/af4qbWandc0R1PziA5w4qH0UCTapUwm927DnNVs5mb2lRn3rqjfL7+OF0aFH6pFBEREREp3VbsPQlAryZhdKpbhfkbjvP7gaRy93e0SFlXHs9vSfn0z9epXrciUhhFOU4UuVyxevXqfPDBBwwfPpzjx4/zzTff0Ldv3wKVfS1atKB9+/aXnOvhhx/mjz/+4IUXXmD//v3MmzeP9957j/HjxwNgMBiYOHEizz33HD/++CPbtm3j7rvvJiIigkGDBgF5ldN9+/Zl9OjRrF+/njVr1jBhwgSGDBlCREQEAHfeeScWi4VRo0axY8cOvvzyS2bNmlWgkvmhhx5iyZIlvPLKK+zevZvp06ezceNGJkyYUNSnSEqZxuF5lek74wp/dUZJ2B6TAkDTiLJfOe9tMTGqS96661+sP+rmaEqXgyfS2Hz0NCajgUGtqrk7nCIJ8DIzpmtdVk/uzuu3t6JpRABZVjuf/XGU615ZyehPNrL+UBIOR+ls+5Zrs/Np9GGufXkFH/5+CKvNwXWNQlgysStPX9/0shLQkPfZNLBl3ufLoq1xrgxZyqj843mDUH+83JyAzje8UyQAy3YmEHM6073BiIiIiIhL2OwOlu/K6wbYq0konetWwcNo4NDJdI6eKnxbRBERkXwWi4W33nqLt956q0wUz4hI2VLkSujly5dzzTXXXHRMQEAAK1asuORc7du35/vvv2fKlCk8++yz1K5dm9dff52hQ4c6x0yePJn09HTGjBnD6dOn6dKlC0uWLCmwNsHnn3/OhAkT6NGjB0ajkVtuuYU33njDuT0wMJBly5Yxfvx42rZtS9WqVZk2bRpjxoxxjuncuTPz5s3jqaee4oknnqB+/fosWLCAZs2aFeXpkVKoydkk9PHkTFIyrQR6l46rubbH5iXFm7m5baur3Nq2Oq8s28OfR0+zJ/4MDcP83R1SqfDt5uMAdGsQTEhA2VxTxWwyMqh1NW5sFUH0wVN88Nshft2dSNTOBKJ2JtCyeiCju9ahb9MwPC6jFXtxWL33BDMW7WRfYhoA9UP8eGpgE7o1CHbJ/Ne3COeN5ftYve8EpzNyLjuhLeXD9pi847m723D/U8MwfzrVqUL0wVN89scRHu/byN0hiYiIiMgV2nIsmVPpOfh7eXBV7cqYTUba1KzE+sNJrNp3gmFVark7RBERKWPMZrOzKFBExNWKnIS+VAK6qAYOHMjAgQMvuN1gMPDss8/y7LPPXnBM5cqVmTdv3kX306JFC3777beLjhk8eDCDBw++eMBS5gT6mKkW5E3M6Ux2x6XSoU4Vd4dERk4uB07kJceaViv7ldAAwf6e9GwcypId8czfcJSnr2/q7pDczmZ38N3mGABuaVP2WzYbDAY6161K57pV2Z94hg9/P8S3m2PYejyFCfP+pFqQN/d0qc3t7Wvg5+medZL3J6bxwuJd/Lo7rzqgko+ZSb0acMdVNV2aIK8f6k+jMH92x59h6Y54bm9f02VzS9mz7WwldLNSdjwf3jmS6IOnmL/+KA/1qF9qqrRFRERE5PJE7cz7O6d7wxDMZ/++6dqgKusPJ7F67wmGdVQSWkRERERKj0KdkW/Tpg3JycmFnrRLly7ExMRcdlAirpbfkntXKWnJvSsuFYcDQvw9CfEvm9Wx53P7VXlrQX//ZwxZVpubo3G/tQdOEpeSRaC3mR6NQ9wdjkvVC/HnxZtbsPY/1/FQj/pU9rUQczqTGYt20unF5by4eBdxKSXXAvh0Rg7PLNxB39dX8+vuRDyMBu7tUpuVj3ZnWKfIYqnQvv5sS+6FasldoTkcDmc77tLW2aJn4xCqBXmTnGHlx62x7g5HRERERK5Q1M54AHo2CXXe1q1B3t+a0QdOYbXZ3RKXiIiUXTabjZUrV7Jy5UpsNp3PFRHXKlSp2pYtW9i6dSuVK1cu1KRbtmwhOzv7igITcaUm4f78siuBXXFn3B0K8Hfr1tKWsLhSXesHExHoRWxKFkt3xHNjGVsD2dW+3ZTXivuGlhHltgKxqp8nD/dqwNhr6/Ld5hg++P0gB0+k83+rD/Lh74cY2CKce6+pU2yvdavNzrx1R3ntl72czrAC0LNxKE/0b0SdYL9i2We+61tE8L+le1h74CQnzmQT7O9ZrPuT0ik+NYtT6TmYjAbnBU+lhYfJyF0da/HSkt18vPYwg9tWx2AwuDssEREREbkMB0+kceBEOh5GQ4FlhppGBFDF18Kp9Bw2H0kuFd3fRESk7MjKyqJ79+4ApKWl4evr6+aIRKQ8KXS/1B49euBwOAo1Vic4pbRpEpGXGNhZSiqhnVVzEaUrYXGlTEYDg9vVYNbyfXy54ViFTkKnZllZsiPvKvVb25b9VtyX4mU2cWeHmgxpX4MVexJ5/7eD/HEwiQVbYlmwJZaOdSoz+po6dG8YgtHoms+IlXsSee6nXew/u+5zw1B/pg5sQpf6VV0y/6XUrOJDyxpBbD12mp+3x3F3p8gS2a+ULtuO5x3P64f4lcqLTYa0r8Hrv+xlR2wqm44k0y6ycBcUioiIiEjpsnxXXivujnWqEOhtdt5uNBroUr8qP2yJZdXeE0pCi4iIiEipUagk9KFDh4o8cfXq5T/pImVHfnXanoQz5NrsxdKatyi2x+Ylw5uWs0pogMHtqvPGr/tYe+AUR06lU6tKxbx6bvFfcWRZ7dQL8aNF9fL3/3whRqOBHo1D6dE4lO0xKbz/20EW/RXHHweT+ONgEnWDfbn3mjrc1LraZSfs9iee4bmfdrFyzwkAKvtaeKR3A25vV6PE39vXtwhn67HTLNwaqyR0BZV/PC+tnS0q+VoY1KoaX248xty1h5WEFhERESmjonYmAHlLrvxb1/rB/LAlltX7TjC5b6OSDk1ERERE5LwKlYSuVatWccchUqxqVPLBz9ODtOxcDp5Mp0Gov9tiybLa2JeQ1xa8tCYtrkT1Sj5cUz+Y1XtP8OWGYxX2D+BvzrbivrUCt79tVi2QWUNa83jfRsxde5gv1h3lwIl0pny3jZeX7mFYp1oM61iLKn6Fa2OdnJ7D67/s5bN1R7HZHZhNBkZeXZvx3esVqAQoSQNbRPD84l1sOJxM7OlMIoK83RKHuE9+Z4vmpfh4PrxzJF9uPMaS7fHEp2QRFujl7pBEREREpAiS0nPYeCQJKLgedL5rGuR1g9oek8rJtGyqFvJvLBERERGR4uTeclCREmI0GmgUlpd43uXmltx7E86Qa3dQycdMRDlNBNzRvgYAX286Tq7N7uZoSt6hk+lsPJKM0QA3ta64LcnzRQR580T/xqydch1PDWhMtSBvTqXn8Pov++j831+Z8t02Z0vt87Ha7Hz0+yG6/W8FH0cfwWZ30LtJKFEPd+OJ/o3dloAGCAv0on2tvMrSn/6Kc1sc4j7b8pdXqFZ6l1doEhHAVZGVybU7+HzdEXeHIyIiIiJFtGJ3InZHXpe36pV8ztke4u9Fk7Md4H7fd7KkwxMREREROS8loaXCcK4LHeveJPT2mL9bt5bXCtkejUOp4mvhxJlsVpxtmVyRfLc5rwq6a4NgQgPK54UGl8Pfy8y919Rh1WPX8uYdrWlZPZDsXDtfrD9Kz1dXMWruBqIPnMLhcADgcDj4dXcCfV5fzbOLdpKalUvj8ADmje7Ae3e3I7Jq6Wj1fn3LcAAW/RXr5kikpCWmZnHiTDZGw9/LPpRWwztHAvDF+qNk59rcG4yIiIiIFEl+K+5e52nFna9rg2AAVu2teH+Di4iIiEjppCS0VBj5CYKdbq6E3h6bVzXXNKL0tm69UhYPI7e0zVsXfv76o26OpmTZ7Q6+PduK+5Y21d0cTenkYTJyfcsIFoy/mq/u60SvJqEYDLB8dyJ3vP8H17/1O/PWHeXuj9Zzz9yNHDyRTlU/C/+9uTmLHuhC57pV3f0QCujXPByjAbYeT+HIqXR3hyMlKL8Kum6wHz6WQq1w4ja9m4YSFuDFybQcVe2LiIiIlCFZVhur9+Ullns1CbvguK5nW3L/tu8EdrujRGITEREREbkYJaGlwshvTeXudtw7ykDrVle4/WxL7hV7EolPyXJzNCUn+uApYlOy8PfyoNd51uqSvxkMBq6qXZn3727H8knduKtjTbzMRrbHpPLE99v4bd9JLCYj93ery4pHr2XIVTUxGUtf94Cqfp5cXS/vhM8iJfcqlG1lYD3ofGaTkbs61gTg47WH3RuMiIiIiBRa9IFTZOTYCA3wvOh5hHa1KuNjMXEyLcftF9+LiEjZYTabmTlzJjNnzsRsdt+SdyJSPhU5CV2nTh1OnTp1zu2nT5+mTp06LglKpDg0DPPHaICTaTkknnFPUtRqs7Mr/gwAzcpxJTTkVQZeFVkZuwO+2XTM3eGUmG/OVkHf0DICL7PJzdGUHXWC/XhuUHPW/qcHj/RqQI3K3gxoHs4vk7rxn36N8Pcq3b8EX98iAoCFW9WSuyL55/IKZcEdV9XE4mFk6/EU/jya7O5wRERERKQQonblteLu2Tj0okt6WTyMdK5bBcBZOS0iInIpFouFxx57jMceewyLxeLucESknClyEvrw4cPYbOeuJZidnU1MTIxLghIpDl5mE3WC/QD3rQt94EQaObl2/D09qFnZxy0xlKT8augvNx6rEO3AzmRZ+Xl7XiXsrW3VivtyVPa18ECP+vw2+TreHtqGmlXKxvukT9MwzCYDu+PPsDfhjLvDkRKy3dnZomwkoav4eTovmFA1tIiIiEjpZ7c7WH42CV2YTlv560Kv1rrQIiIiIlIKFHoBwx9//NH5/dKlSwkM/PuEq81mY/ny5URGRro0OBFXaxwewP7ENHbFneHahiElvv/8qrkmEQEYS2FbYVfr3zyc6Qt3cCwpk7UHTtGlfulay9fVft4WT5bVTt1gX1rVCHJ3OFKCAn3MdGsQzC+7Elm0NZZJvRu6OyQpZifOZBOfmoXBAE0jys7yCiM6R/Lt5uP8tC2OJwY0JsTfy90hiYiIiMgFbItJISE1G1+LiU5nq5wvpmv9vCT0xsPJpGXn4udZ6NN+IiJSQdlsNjZv3gxAmzZtMJnU2VFEXKfQv40OGjQIyFvDc/jw4QW2mc1mIiMjeeWVV1wanIirNQkPYOHWWLetj1TWquaulLfFxKBW1fj0jyPM33C03Ceh81tx39K2+kXbpEn5dH3LCH7ZlcjCv+J4uFcDvQbKue2xecfzOlV98S1DJ/eaVw+kTc0gNh89zbx1R5nYs4G7QxIRERGRC/jlbBV0t4bBeHpcOikQWdWXWlV8OHIqg+gDpwpVPS0iIhVbVlYWV111FQBpaWn4+vq6OSIRKU8K3Y7bbrdjt9upWbMmiYmJzp/tdjvZ2dns2bOHgQMHFmesIlescbg/ALvclITeEZufhC47VXNXKr8l97IdCSSl57g5muJz5FQ66w8nYTTAza3Virsi6tk4FC+zkUMn09nhppb/UnK2Hy+7FxUN7xwJwOfrjpKTa3dvMCIiIiJyQVE7/14PurDyq6HVkltEREQETp06w6OTP2bX7uPuDqVCKvKa0IcOHaJq1bxqxqysLJcHJFKcmpxtmXrwRBpZ1nPXNi9OdrvDmZhqFlH2khaXq1m1QJpXCyTHZue7zeX3QP/t5hgAutQPJixQ7W0rIl9PD65rlNfmf+FfsW6ORopbfiV08zKYhO7XLJxgf09OnMl2rmMvIiIiIqXLsaQMdsefwWQ00L0Iy4k514XepyS0iIiIVGx/rNvL3SPeYO3aPbz44nfY7SrGKGlFTkLb7XZmzJhBtWrV8PPz4+DBgwBMnTqVDz/80OUBirhSiL8XVf0s2B2wJ/5Mie770Kl0MnJseJmN1An2K9F9u1t+NfSXG47hcDjcHI3r2e0Ovj3bivvWtqqCrsiubxEBwKKtceXytS5/2x5z9qKiMpiEtngYGdqhJgAfrz3s3mBERERE5LzyW3G3q1WJSr6WQt+vU90qeBgNHDmVweGT6cUVnoiIiEipZbXm8vY7PzPpkbkkJ6dTt24Yz0y/HaOxyClRuUJFfsafe+455s6dy8yZM7FY/v4luFmzZnzwwQcuDU6kODQOz6uGLul1ofPXg24SHoDJWLHWir2hVQTeZhP7EtPYfDTZ3eG43B+HThFzOhN/Lw96a82tCq17oxB8LSZiTmey+ehpd4cjxSQpPYeY05nA3x02ypo7O9TEbDKw+ehptp1tLS4iIiIipUd+K+6iruvs5+lBu8hKgKqhRUREpOKJiUni/nHv8fm83wC4+aYOfPDeWGrX1nl7dyhyEvqTTz7hvffeY+jQoZhMJuftLVu2ZPfu3S4NTqQ4NDmbhC7pdaGdrbjLYNXclQrwMjOgRTgA89cfc3M0rvfN2SrogS0i8DKbLjFayjMvs4neTcMAWLhVLbnLq/yLimpX9SXAy+zmaC5PiL8XA5rnHZfnqhpaREREpFRJybCy7lASULT1oPM5W3JrXWgRERGpQJZFbWX4yDfZtes4/n5evPj8UB595EY8Pcvm+bvyoMhJ6JiYGOrVq3fO7Xa7HavV6pKgRIpTftXazlj3VEJXpPWg/2nI2Zbci/6K40xW+TlWpGXn8vO2eECtuCXP9S3zEns/bYvDZldL7vJoW/7xvIxfVDS8cySQd8HEybRs9wYjIiIiIk4r9yZiszuoH+JHZFXfIt+/a/28JPTaA6fIydXahyIiIuVRYmIKo+97lwkPfsD+/XHuDsetMjNzeOHFb5n+zJdkZGTTonktPp77IN26NXV3aBVekZPQTZo04bfffjvn9m+++YbWrVu7JCiR4pTfjnt3/BnsJZQgcjgcziR002pls3XrlWpbqxL1QvzItNr4sRxViP68LY5Mq406VX1pUzPI3eFIKdClXjCB3mZOnMlm3aFT7g5HisHfFxWV7eN565qVaFk9kBybnfnrj7o7HBERERE5K78Vd8/LXO6pSXgAVf0sZOTY2HgkyZWhiYhIOWM2m3n66ad5+umnMZtVLVpWxMUlM278e+zYcYzNmw8yctTbvPPuErKyctwdWonbty+Oe+59m0U/bcJgMDByRHfeevNewsKC3B2acBlJ6GnTpjFhwgReeukl7HY73333HaNHj+b5559n2rRpRZpr+vTpGAyGAl+NGjVybs/KymL8+PFUqVIFPz8/brnlFhISEgrMcfToUQYMGICPjw8hISE89thj5ObmFhizcuVK2rRpg6enJ/Xq1WPu3LnnxPL2228TGRmJl5cXHTp0YP369UV6LFJ21Knqi8XDSFp2LseSM0pkn8eTM0nNysViMlI/xL9E9lnaGAwGZzX0lxvKT0vu/Fbct7StjsFQsdb6lvOzeBjp1yy/JXfFvgqxvNoem5eEbl7GK6Hh72roz/44itWmKhkRERERd8vJtbNqT14b7ctpxQ1gNBqc1dCr9550WWwiIlL+mM1mWrbqx/jxD2OxWNwdjhTC8eOnGDf+PWLjkqlWrTJdr2mCzWbns89Xc9ewWaxbv8/dIZYIh8PBN99GM/q+dzly5ARVqwbwxqx7GH1vLzw8tGRmaVHkJPSNN97IwoUL+eWXX/D19WXatGns2rWLhQsX0qtXryIH0LRpU+Li4pxfv//+u3Pbww8/zMKFC/n6669ZtWoVsbGx3Hzzzc7tNpuNAQMGkJOTw9q1a/n444+ZO3dugWT4oUOHGDBgAN27d2fLli1MnDiRe++9l6VLlzrHfPnll0yaNImnn36azZs307JlS/r06UNiYmKRH4+Ufh4mIw1D8xLBJbUudH7VXMMwfyweRX7blRs3ta6G2WTgr+Mp7DibxCnLjp7KYN2hJAyGvMcmku/6lhEA/Lw9Tom9cuZ0Rg7HkjIBaFoOllcY0CKcqn4W4lOzWLYj4dJ3EBEREZFitf5QEmeyc6nqZ6F1jaDLnkfrQouISGHs3hPD/17+gZtumckjj87l11+3kZOTe+k7ilscPpLIuAnvk5CYQs2aVXnn7TH898W7+O+LdxESEkhsXDIPT5rD9Ge+JCk5zd3hFpvU1Az+88RnvPraQnJycrm6cyM+mfsAbdvUdXdo8i+XlQ275ppriIqKIjExkYyMDH7//Xd69+59WQF4eHgQFhbm/KpatSoAKSkpfPjhh7z66qtcd911tG3bljlz5rB27Vr++OMPAJYtW8bOnTv57LPPaNWqFf369WPGjBm8/fbb5OTktR2YPXs2tWvX5pVXXqFx48ZMmDCBW2+9lddee80Zw6uvvsro0aMZOXIkTZo0Yfbs2fj4+PDRRx9d1mOS0q9JeMmuC51fNdesgrbizlfFz5PeTfIqRMtDNfS3m/OqoLvUq0pEkLebo5HSpGOdKlT18+R0hpXf96vyoDzZcfZzo2ZlHwJ9yn6bKk8PE3dcVROAj9cedm8wIiIiIkLUzngAejQKxWi8/G5bXernnV/bGZdK4pksl8QmIiLlT052LvXqBWO3O4j+Yy9PTfuCGwa9yKuv/ciePTE4HCWznKVc2oGD8Yyf8D4nT6ZSp04o77w1muCqefmGrtc04fPPJnLb4M4YjQaWRW3ljjtfY+GijeXu/3DL1kPcPeJNfvttF2aziYceHMDMl4YRFOTr7tDkPDzcHcC+ffuIiIjAy8uLTp068eKLL1KzZk02bdqE1WqlZ8+ezrGNGjWiZs2aREdH07FjR6Kjo2nevDmhoX+3J+rTpw9jx45lx44dtG7dmujo6AJz5I+ZOHEiADk5OWzatIkpU6Y4txuNRnr27El0dPQF487OziY7O9v5c2pq3klpq9WK1Wq9oudEil+D0LwD0o7YlBL5/9p2/DQAjUL9Kvzr49Y2Efy0LY7v/4zhsV718DLntcbIf17KyvNjtzucSehBLcPLTNxScvo2DeGzdcf4cUsMXepUcnc44iJbjuatqdc03L/cvO9vaxvBuysPsP5wEluPnnJeqCUiIiIiJcvhcDjXg+7esMoV/b4Z6GmkaYQ/O2LPsHJXAje1jnBVmCJyEWXt/JZIrVpBfPrxJLy8KvHfl+by6687OXEylW++/YNvvv2DunVC6devNb16tlCSz4327Y/jkUc+ISU1g3r1wnjl5bvx9/cqcKyxmI2MH9eHHj2a8b+Xf2T//nhe/O93LF68iUceuZ5aNYPd+AiunM1m59PPVvHxJ6uw2x1Ur16Fp6feSoMGEecs0SvFqyifcUVOQleqVOm8654aDAa8vLyoV68eI0aMYOTIkZecq0OHDsydO5eGDRsSFxfHM888wzXXXMP27duJj4/HYrEQFBRU4D6hoaHEx+ddFRofH18gAZ2/PX/bxcakpqaSmZlJcnIyNpvtvGN27959wdhffPFFnnnmmXNuX7ZsGT4+Ppd87OJeKakAHmw+mMjixYuLdV8OB2w+ZAIMpBzaxuKT24p1f6Wd3QGVPU0kZeXy0rxltA8ueCVWVFSUmyIrmv0pcDzZA0+TA/vRP1kc86e7Q5JSpnIagAc//xXD1ZajmCtuJ/5y5Ze9RsCI6UwsixfHuDscl2leycifp4y88PVa7qynFvIiIiIi7nA8HWJTPDAbHZzZt5HFB69svgiDkR0Y+XL1X3jGbXFJjCJSOGXl/JZIVlbW2X+TCQvNZMTwRhw+ksK27SfYty+ZAwcTeOvtJbzz7lLq1Q2iefNg6tQOuqJuHVI0cXFpfPXNbrKybISH+dK/bzXWrll10fvcdGNNNm6y8Pua42z96wgj73mbjh0i6NghAo8yuFxo6plsFv10gGPHzgDQrGlVevaMZP/+Lezfv8W9wVVAGRkZhR5b5CT0tGnTeP755+nXrx9XXXUVAOvXr2fJkiWMHz+eQ4cOMXbsWHJzcxk9evRF5+rXr5/z+xYtWtChQwdq1arFV199hbd36W5tO2XKFCZNmuT8OTU1lRo1atC7d28CAlRBVNqdybLyxo4VJOcYuLp7LwK9i6+lanxqFml/rMZkNDDy5j7Oyt+K7LDPAWb9eoA9uVV5un97IO/qmaioKHr16oXZXPpb3D7+3XYglhtbVWfQ9U3dHY6UQna7g69eWU18ajY+ddrRq0mIu0MSF3h1z+9ABrd0v4ou9aq4OxyXCW2azJAPNrAl2YPZPa7Fz9PtzXJEREREKpw3VxwADtC1QQiDrm99xfNVPZxE1IcbOZjhSd++1yphIFICytr5LZH09HTn93369MHX9+9q59TUDJYv38bPS7awZ28se/cls3dfMpUr+9G7V0v69WtNZK2yXV1b2m3ffpS33vmMrCwbzZrW4KX/3oWfn1eh7jtwIMTFJ/P66z/xx7p9rFkbw9FjWTwy6Xpat6pdzJG7zpo1u3n3/xaQmpqJt7eFSQ8PpHevlu4Oq0LL7wxdGEU+w/j777/z3HPPcf/99xe4/f/+7/9YtmwZ3377LS1atOCNN964ZBL634KCgmjQoAH79++nV69e5OTkcPr06QLV0AkJCYSF5a0pGxYWxvr16wvMkZCQ4NyW/2/+bf8cExAQgLe3NyaTCZPJdN4x+XOcj6enJ56enufcbjab9QtGGVDZbKZGZW+OJWWy70QmneoWX/X6noS81q31gv3w9yncB0R5N6RDLd5ccYANh5M5djqbOsF+zm1l4T2Unp3Lkh15x4zb2tcs9fGK+1zfMoL3fzvE4h0J9G9Zzd3hyBVKzbJyJCnvSr9WNSuXq/d+h7rBRFbx4fCpDNYePM2AFuHuDklERESkwlmx5yQAfZqGu+R3zavqBOPn6UFyhpW9JzJpXj3wiucUkcIpC+e3RIACr9N/v26rVAnkttu6cNttXThwIJ5FizexdOkWkpLSmP/lGuZ/uYamTWowoH8bevZsWejkqBTOn38e5NHJn5KZmUOrVpH8b+ZwfH3OzUldTM0aIbzy8gh+/XUbr7/xE8eOnWLiw3MZ0L8tE8b3IzCw9Hb1zc628va7S/jmm7xlcxs2iODZZ4ZQo0ZVN0cmRfl8K3Ld/dKlS89ZYxmgR48eLF26FID+/ftz8GDRewalpaVx4MABwsPDadu2LWazmeXLlzu379mzh6NHj9KpUycAOnXqxLZt20hMTHSOiYqKIiAggCZNmjjH/HOO/DH5c1gsFtq2bVtgjN1uZ/ny5c4xUj41DsurWN8VV/irNi7H9tgUAJpWU4V8vvBAb65tmFcV+uXGY26OpuiWbI8nI8dGZBUf2tbSWr9yYde3zFt3bfmuRDJytDZJWbc9Ju94Xi3Im0q+FjdH41oGg4E+TfMuvlu6I97N0YiIiIhUPHEpmWyLScFggO6NXNNFyWwy0qluXvee1ftOuGROERGpmOrWDeOhBwbww/eP8+ILd9GlS2NMJiM7dh5j5ss/MPCGF5j+7Fds3Lgfu13LfF2pDRv2M+nRj8nMzKF9u3q8+vKIIieg8xkMBnr0aMG8zyYy6Ma87sY/Ld7EHUNfY8nSP3E4HJeYoeQdOXqCMffPdiagb7/9av5v9v1KQJdBRU5CV65cmYULF55z+8KFC6lcuTKQ18LB39//knM9+uijrFq1isOHD7N27VpuuukmTCYTd9xxB4GBgYwaNYpJkyaxYsUKNm3axMiRI+nUqRMdO3YEoHfv3jRp0oRhw4axdetWli5dylNPPcX48eOdVcr3338/Bw8eZPLkyezevZt33nmHr776iocfftgZx6RJk3j//ff5+OOP2bVrF2PHjiU9Pb1Q61pL2dUkIi8pvLO4k9AxefM3i9AVx/90e/saAHy76Tg5uWXrF5NvNh0H4Na21TEY1M5MLqx5tUBqVfEh02rjl12Jl76DlGo7zh7Pm1crn8fz3meT0Ct2J5a547KIiIhIWZf/90LrGkEE+1/eSebz6dogr03qqj1KQouIyJUzmz3o1rUJM/87jAXfPc6Ecf2oHRlCTk4uy5Zt4cGJH3HrbS/zwYe/EBub5O5wy6S10Xt47PFPyM620qljA2a+NAwvrysvhvD392byY4P4v3fvo3btEE6fTufZGV8zcdIcjsecckHkV87hcPDT4k3cM+pt9u2LIyjIh5f/N5yHHhiAxaKl48qiIv+vTZ06lbFjx7JixQrnmtAbNmxg8eLFzJ49G8irNO7Wrdsl5zp+/Dh33HEHp06dIjg4mC5duvDHH38QHJz3C/Jrr72G0WjklltuITs7mz59+vDOO+84728ymVi0aBFjx46lU6dO+Pr6Mnz4cJ599lnnmNq1a/PTTz/x8MMPM2vWLKpXr84HH3xAnz59nGNuv/12Tpw4wbRp04iPj6dVq1YsWbKE0NDQoj49UoY0Di+ZSugdZyuhm5XTpMXluq5RCMH+npw4k82vuxPo0bBsXMV0LCmD6IOnMBjgpjbV3R2OlHIGg4HrW0Tw1or9LNwayw1nK6OlbNp2thK6vLYxzD/heeJMNmsPnHR2rBARERGR4vfLzrwln3o1ufDScJejW/28c2ybjyZzJsuKv5faA4uIiGtUqeLPnXdewx13dGHXruP8tHgTUb/8RXz8aT6a8ysfzfmVNq1rM2BAO7pf29QlidTybvVvO3lq6hfk5tq45prGzHjmDpcnX5s3r8XcjyYw74vfmTP3VzZs2M9dw2Zxz8ge3HlHFzw8TC7dX2Glp2fxv5d/YFnUVgDatKnD09NuI7iqOsyWZQbHZdTar1mzhrfeeos9e/YA0LBhQx544AE6d+7s8gDLitTUVAIDA0lJSSEgQG+KsuBYUgbXzFyBxWRkx7N9MJuK3Bjgkk6mZdPuuV8A2P5MH/w8dbXOP720ZDfvrjzAtQ2Def+u1ixevJj+/fuX6jVzZv2yj9d+2cvV9arw+b0d3R2OlAF74s/Q5/XVWExGNjzVk0Dv0vv6lou77uWVHDyZztyR7cttgvaJ77cxb91R7riqJi/e3Nzd4YiIiIhUCGnZubR5Noocm51fJnWlXsiluwsWRfeXV3LoZDr/N6ytcwkWESkeVqu1TJzfEsmXk5PDk08+CcDzzz+PxXJlieLsbCurVu/kp8Wb2LjxgLPVs4+PJz16NGdg/7Y0a1ZT3SXP49dft/H0M19is9m5rnszpj99e7EnhI8fP8XM/y1g46YDANSpE8rjk2+iebOaxbrff9u1+zjTnp5PTEwSJpORUff0YNhd3TAVQ85GrlxR8qFFyohZrVbuu+8+pk6dyhdffHFFQYq4W/VK3vh7eXAmK5cDJ9JoFOb6iwd2xOZVWdep6qsE9Hnc3q4G7648wKq9J4hLyXJ3OJfkcDj4dvPfrbhFCqNhmD8NQv3Ym5DGsh3xDG5Xw90hyWU4k2Xl4Ml0oPy24wbo0zSMeeuOErUzgecHNcNo1B+FIiIiIsVt9d4T5NjsRFbxoW6wn8vn71q/KodOprN67wkloUVEpACLxcL//vc/l83n6Wmmd6+W9O7Vkvj40/y8ZDM/Ld5MbGwSCxduZOHCjdSsWZX+/drSr19rVbmetXTZFmY89zV2u4PevVvx1BO3lEhFcvXqVZj1+j0sWbqFN9/6iYMHE7h/7P8x6MarGHt/H/z8vIp1/3a7nflfrmH2/y0jN9dGaGgQz06/nebNaxXrfqXkFOkyArPZzLfffltcsYiUKIPB4GzJvTO2eFpybz/burVpOU5YXInIqr50qlMFhwO+2RTj7nAuacPhZI4mZeDn6aE/3KVIrm+R14Z70V9xbo5ELlf+50REoBdV/Fy3Rl9p06lOFfy9PDiZls2fx5LdHY6IiIhIhRDlbMUdWiyVYc51ofee4DIaIoqIiFyWsLAgRo64jq/mT+LtN++lf782eHmZOXr0JLP/byk33fwSjzz2Mb+u2EZOTq67w3WbRT9t4tkZeQnoAf3bMvXJW0u0JbbBYKBf39Z88fkkBvRvi8Ph4PsF67hz6Gv8umJbsf3ukJScxqOPfcJbb/9Mbq6Na69tysdzHlACupwpci37oEGDWLBgQTGEIlLymhTzutDO9aAjdEXXhQy5Kq8q9JvNMdhL+d/C32w6BkD/5mH4uHgtDinfBp5dC/r3/SdJSs9xczRyObafTUKX94uKLB5GrmuU12p86Y4EN0cjIiIiUv7l2uz8ujsRgJ6NQ4tlHx3rVMFiMnI8OZNDZ7v7iIiIQF4l6uHDhzl8+DB2u71Y9mE0Gmndug5PPXkrC398gif+czMtW9TCbncQHb2Hp6Z+wQ2DXuTV1xeyZ29sscRQWi1YsI4XXvwWh8PBTYOuYsp/bnJbC+rAQB+efOIW3nxjFDWqV+HkqTM8NfULJj/+KfHxp126r40b9zN8xJv8sW4vFosHkx+9kedn3ElAgLdL9yPuV+QsSv369Xn22WdZs2YNbdu2xdfXt8D2Bx980GXBiRS3/CT0zmJKQm+PyZu3WTlPWlyJPk3DCPQ2E5uSxZ4UAwPdHdAFZOTk8tPZKtZb26qdshRN7aq+NKsWwPaYVH7eHsfQDrqir6zJ72xRnltx5+vdJIwftsSydEc8U/o10jpNIiIiIsVo45FkUjKtVPIx07ZWpWLZh6+nB+0iK7H2wClW7z1BnWJo+S0iImVTZmYmtWvXBiAtLe2cfI+r+fp4MnBgOwYObMexYyf5afFmfl6ymRMnUvnmm2i++Saa+vXCufHG9gwc0A5LOS4E+urrtbw+axEAtw3uzEMPDigV52DatqnLJx8/yKefreKTT1exZu1uNv95kNH39uTWWzpdUZV2bq6NDz78hU8/W43D4aB2ZAjPPjuEunXUdbS8KvI7+MMPPyQoKIhNmzaxadOmAtsMBoOS0FKmNHZWQp/B4XC49CCfkmHlaFIGAE1VCX1BXmYTN7Wuxty1h4lOcP+H7IUs2R5Peo6NmpV9aB9ZPCcGpHy7vkUE22NSWbg1VknoMmjb2SR0s2rl/3h+bcNgLB5GjpzKYG9CGg3D/N0dkoiIiEi5ld+Ku3ujEDyKsfKpa4PgvCT0vpOMuLp2se1HRESksGrUqMr99/Vm9L092bBxPz/9tInVv+1k3/44Xn7lRz6f9xv3jupJ714t3VYdXFw+n7eat99ZAsDQO7sybmyfUpGAzufpaebeUT3p2aMFL/1vAVu3HuaNNxezdOkWHp98E40aVSvynHFxyUybPp8dO/K6jd54Q3seenAAXl4WV4cvpUiR37mHDh264NfBgweLI0aRYlM/1A+T0UBSeg4JqdkunXtHXF7Conolb4J8dCC9mNvb51UWb0s2cCrNtf8PrvLt5uMA3Nq2eqn6hUDKjgEtwgFYdyiJhNQsN0cjRZGRk8uBE2lAxehs4evpwTX1qgKwdEe8m6MRERERKb8cDge/7Dq7HnQxteLO17V+3rrQ0QdOkZ1rK9Z9iYiIFIXJZKRjhwbMePYOFv4whYkPDaRqFX/i4pKZ8dzXjLjnLdas2V1saxOXtDlzf3UmoEeO6F7qEtD/FBkZwttv3st/Hr8Jfz8v9uyN5d4x7zDrjZ/IyCj8efxff93G8JFvsmPHMfz8vHju2Tt4fPJNSkBXAOXr8hGRIvIym6gbnNdixNXrQu/Ib8UdUf4TFleqcXgALaoHYHcY+H5LnLvDOcfx5AzWHjgFwE2ti36VlwhA9Uo+tK1VCYcDZ2v38iY1y8rIOeuZuWS3u0NxqZ2xqTgcEBrgSYi/l7vDKRF9mua1QVISWkRERKT47E9M48ipDCwmI10bBBfrvhqH+xPs70mm1cbGw8nFui8REZHLFRDgw22DO/PVl48w9v4++Pt5ceBAPI89/gnjxr/HX38dcXeIl83hcPDe+1G8/8EvAIwZ3YvR9/YqtQnofEajkRuub88X8x6mV88W2O0OvvxqDUOHzWLNmoufA8zKyuGlmd/z1LQvSEvLolmzmnw85wGuu655CUUv7nZZSejjx4/zzjvv8J///IdJkyYV+BIpaxoX07rQ22MrTutWV7i9bXUAvt50vNRd1fb95hgcDuhUpwo1Kvu4Oxwpw64/Ww298K9YN0dSPJ75cScr9pzgnZUHOHoqw93huIyzFXcFuqioR+MQjAbYEZvKsaTy838pIiIiUposO9uKu3O9Kvh6Fu+alwaDwVkNvXrviWLdl4iIyJXy8rIw7K5ufP3VY9w1tCsWiwdb/zrC/eP+j8mPf8KBA2XronmHw8E77y5h7scrAJgwrh8jhnd3c1RFU7myP89MH8KrL48gIrwSCQmneezxT3jyqXmcOHlubuXAwXhGjX6HH37cgMFg4O5h3XjnrdGEh2upy4qkyEno5cuX07BhQ959911eeeUVVqxYwZw5c/joo4/YsmVLMYQoUryaFFcS+mzSomkFaN3qCv2bh2ExOjh4MoMNpeiqbIfDwTf/aMUtciX6twjHaIA/j54ud4m9ZTvinW3rAT5fX3avTP237fmdLSrQ8byKnyftIisDf58cFRERERHXym/F3bOYW3Hn69ogb8mVVUpCi4hIGREQ4M24sX35+stHuPGG9phMRn5fs5u7R7zJjOe+Ji6u9JxHvhCHw8Hrsxbx+bzfAHh44kDuvPMaN0d1+Tp2bMBnnz7EXUO7YjIZWbFyO3cOfY3vvv8Du92Ow+FgwYJ1jLr3HQ4dSqRKFX9mvTaS++/rg4eHyd3hSwkrchJ6ypQpPProo2zbtg0vLy++/fZbjh07Rrdu3Rg8eHBxxChSrPIroXfFui4JnZ6dy8GT6UDFqpy7En6eHrSpmlcBPX/DUTdH87eNR5I5cioDX4uJfs3D3B2OlHEh/l50rFMFgEXlqCV3UnoOT3y/DYDWNYMA+Hrj8XKz1lz+RUXNK1ASGv5uyb1MLblFREREXC7xTBZbjp0GSi4JfU39YAwG2B1/hoTUrBLZp4iIiCsEBwfy+OSb+PzTiVzXvRkOh4Ofl/zJkDtf5fVZi0hOTnN3iOdlt9v538s/8PU30QBMfmwQg2/t7OaorpyXl4VxY/vy0QfjadK4Ounp2bz8yo/cN/b/eOLJz5n58g/k5OTSqWMDPp77AO3a1XN3yOImRU5C79q1i7vvvhsADw8PMjMz8fPz49lnn+Wll15yeYAixS0/CX3oVDoZObkumXNX3N/rhwb7e7pkzoqgU4gdgMXb4kjJtLo5mjzfbsqr7OzXPBwfS/G2R5OK4fqWEQAsKictuR0OB08t2MbJtBwahPrx+b0dCAvwIik9hyXby37yMjPHxr7EM0DFqoQG6N0k72TohsNJnErLdnM0IiIiIuXLr7sScTigRfVAwgK9SmSflX0ttDj7O61acouICOTleMaNG8e4cePw8Cj95z5r1qzKczPu5MMPxtG+XT2sVhtffb2Wwbe9zIcfLSc9o/Scv7DZ7Lzw3+9Y8MN6DAYDTz5xC4NuvMrdYblU/frh/N/s+3nk4evx8fFkx45jrFq9Ew8PExPG9+N/M++mciU/d4cpblTkJLSvry85OTkAhIeHc+DAAee2kydPui4ykRIS7J+XKHY48q4GdoXtFXD9UFeo5QcNQvzIstr5cUuMu8MhM8fmrFZVK25xlb5Nw/AwGtgRm8qBE6XzKs2i+HFrLIu3xeNhNPDqba3wsXgw5KoaAHy+rvR0Nbhcu+JTsTugqp8noQEV66KiGpV9aBoRgN0By3clujscERERkXIlvxV3rxKqgs7XtcHZdaH36RyeiIiAp6cnb7/9Nm+//TaenmXnvEfjRtWZ9fo9zHrtHho1rEZGZg4ffrScwbe9zFdfryXHRcVmlys318aM575m8eLNmExGnp46mAH927o1puJiMhm55ZZOfPH5RHr1bEHTJjX4v9n3cecd12A0FjkFKeVMoV8Bzz77LOnp6XTs2JHff/8dgP79+/PII4/w/PPPc88999CxY8diC1SkOOWvC73LRetCbz/b2lvrQReNwQCD21UDYP6GY26OBpbuiCctO5calb256uzaqCJXqpKvhS7189ZiW7S1bLfkTkjNYtoPOwB44Lr6zkrhIe1rYjIaWH8oib0Jrrm4x13+bsUdgMFgcHM0Ja93k7yW3EvVkltERETEZTJycvntbBK4ZxP3JKF/33cCm91RovsWERFxtfbt6/HhB+N47tk7qFG9CqdPp/P6rEUMufNVfv55MzabvcRjys218fT0L1kWtRWTycgz02+nd+9WJR5HSQsODuSZ6UN4/72xNG6kgi7JU+gk9DPPPEN6ejqvvvoqHTp0cN7Wo0cPvvzySyIjI/nwww+LLVCR4pTfknuni9aF/rsSOsAl81Ukg1pGYPEwsiM2lW3HU9wayzdnW3Hf0qY6RmPFSz5J8bm+RV5L7h+3xuBwlM0TPw6Hg8e//YuUTCstqgcyrntd57awQC96NAoBYF4Zr4Z2Hs8r6EVFfZrlnRT9bf9J0rPdexWxSHkRl5JJ/1m/Me2H7WX2M0BERK7M7/tOkp1rp1qQN43C/Et0361qBOHv6UFyhpVtMe79m1tERNzP4XBw4sQJTpw4UWb/PjEYDFx3XXM+/2wikx8bRNWqAcTHn2bG898wfOSbrFmzu8QeW05OLk88NY8VK7djNpt44fmhXNe9eYnsWy4sLiXT3SFUWIVOQue/SevUqUOLFi2AvNbcs2fP5q+//uLbb7+lVq1axROlSDFrEuG6Sugsq419iXktditq0uJKBPmY6ds0r/Ju/gb3Ja9iT2ey5kDelem3tNGVW+JavZqGYvEwcuBEusuWAShp8zccY+WeE1g8jLwyuCVmU8FfKYZ2zPud4NvNx8lwcwukK7EtJu9zoaIezxuG+lOrig85uXZWad1AkSvmcDiY8t02dsal8kn0ET78/ZC7QxIRETdwtuJuElri3XbMJiNX18vrzKR1oUVEJCMjg5CQEEJCQsjIyHB3OFfEw8PEoBuv4qv5k/h/9u47vKn6e+D4O0mT7r1LBwUK3exRZK8qQxFQUFHE8VMEFVBU/Lq3qDhBnKDiYCgqQ/beUGgpLS2F7r33SJrk90faKspoIW2S9vN6Hh9pe3NzOnJz7z2fc86cRyOxtbEgOTmPRc9+z5zHviQmJrVVn7+uTsXi51dz8OA5FAoz3n37XoYOCWrV5xSu7XhKMcOW7OH9bYkmu9DClLWoIXtHbEMpdAzBnrqVxwm5FWhusB1VYm4Fao0WJ2sFnvYW+givw5nRXzdP9s/obIMlrzaczkKrhYH+Tvg4WRkkBqH9srOQM7KHrg3exphsA0fTchnF1byxKR6AZyJ7EOD+3+qNod1c8HWyoqK23mTbjteq1CQ1tBMP66BJaIlEQmSIaMktCPry66ks9iYW0HhZ9fZfCRy+KGZyCoIgdCRqjZZd5/IBXRLaEJrmQosktCAIeqLVaknKq6BWpTZ0KIKAhYWCe2cOZ93aRcy8ZxgKhRlnYtOYM/dLFj3zPRcv6v/+Rk2NkkXPfM+Ro+cxN5fz/pL7GDSou96fR2iZ8loVC9ZEo1JryS2vFTlOA2hRErp79+44OTld9T9BMEX+LjZYyKVUK9WkFd/Yiq+z2bp2ViFeHXN+qD4M6uKMn7MVFXX1bD7T9skrrVbb1Ip7Wl9RBS20jkk9dS25N57JNqlVeBqNlqfXxVClVDOgsxOzb/K/7HZSqYS7B/oC8OOxtLYMUW8ScyuoF4uKiAzR3RzdnZCPsr7tZykJQnuRV17LaxvjAFgU2YMpfTqh1miZ99NpsktFazBBEISOIjqjhKIqJbYWZgzwN8x9tGHddZXQpzNKKa9VGSQGQRDaB5Vaw++ns5j02UHGfrifsR/uE4ssBaNhZ2fJY3NuZt2ap7jt1v7IZFIOHU7gvvs/5bXX15GTU6KX56mqruOpRas4GXURK0sFSz+4n379uull38KNefmPOLJKa/B1suKVW0MMHU6HZNaSjV999VXs7TtmJZDQvsmkEnq42xKTWUZ8djn+LtbXva+zHbx1qz5IpRLu7OfDe9sSWXMigzv6+bTp859KLyGlsAorhYzxYZ5t+txCxzEq0A0rhYyM4hpiMsvo5eNg6JCaZeXhVI6lFGOlkPH+HT2RXWVe+h19vflgeyIxmWXEZpYR5m1ax8XYf8yD7siLinr7OOJiY05hZR1HkosY3lA5IwhC82m1Wv634SzltfWEe9vzf0O7UK/Rcj6vgrNZ5Ty6Ooq1j0RgIZcZOlRBEAShlW2P17XiHtnD7T8jbdqKt6MVXVytSS6o4vCFQm4OFde9giC0TFm1ip+Op/Pd4VRyy2ubPp9RXMPdXx3j3kF+PHdLINbmLUo/CEKrcHW159lnbueuGUP58qvt7N5zlq3bTrNz1xlunzyAWbNG4uRoc137rqysZeHTqzh7Nh1ra3OWfjCbsFBfPX8HwvX4MyabDaezkErgw+k9sRHHI4No0U99xowZuLm5tVYsgmBQwV52xGSWcS6nnAnh138BFtdQCR3qZVrJFmNzR19vlu44z8m0Ei7kV9DN7b/tflvL+qgsAG4J9RQny0KrsVKYMSbInT9jstkUk20SSegL+ZUs2ZoAwP8mBOHrfPVW9c425twS6smfMdn8eCyNd7zD2yJMvTnbmIT2sjNwJIYllUoYG+zOz8fT2R6XK5LQgnAd/ozJZue5POQyCe9N64mZTIqZDFbM7MukTw9yJrOMl/44y7tTwzv0ohdBEISOYGdDEnqMgVpxNxre3ZXkgir2nS8QSWjB6NSrNTy9LgaA9+7oabAFG8J/pRVVsfJQKmtPZlCt1LXedrU1Z1aEH7f16sTyvRf5+Xg6PxxNY+/5fJZM7UlEV2cDRy0IOr6+Lrzx+t2cS8hkxRfbOXHiAuvWH2Hz5ihmzBjCXXcNxdrKvNn7Ky+vZv7ClSQkZGFra8lHH84mKFB01TQGWaU1/G9DLADzRgXQ1090cTaUZr+Dt/bNkHfeeQeJRML8+fObPldbW8vcuXNxdnbGxsaGqVOnkpeXd8nj0tPTmTBhAlZWVri5ubFo0SLq6y+dIbt371769OmDubk53bp1Y9WqVf95/mXLltG5c2csLCwYOHAgx48fb41vUzBiQZ66JEN8Tvl170Ol1pCQo5sfGtqpYyctbpSbnQWjAnWLXn45ntFmz1urUrOpYUbv1L6d2ux5hY6psSX3pjM5NzyPvrXVqzU8tTaaunoNw7q7cveA5q3qnDnID4A/orNNrtVf43iFjjoP+p8aW3LviM8z+r9VQTA2BRV1vPynrg3346MC6OHx98I6b0crPr2rD1IJrD2ZyY/H0g0VpiAIgtAGkgsquVhQhZlUYvCFfX/PhS40qfFAQsew8lAqv0dn83t0Np/uSjJ0OB2eVqvlZGoxj/4QxYj397LqcCrVSjWBHra8f0dPDj47knmjAvBxsuLtKWGsfnAgnRwsySiu4a6vjvLSH2epqqu/9hMJQhsJCvTm4w8f4JOPHiAwsBPVNUq+XbmbO+58jzVrD6FUXvvvtaSkksef/IaEhCwcHKz49JMHRQLaSKg1Wp5aG01FbT29fBx4fJRojW5IzU5Ct+YJ6YkTJ/jiiy8ID7+0QmrBggVs3LiRdevWsW/fPrKzs5kyZUrT19VqNRMmTECpVHL48GG+++47Vq1axUsvvdS0TUpKChMmTGDkyJFER0czf/58HnroIbZt29a0zZo1a1i4cCEvv/wyp06domfPnkRGRpKfn99q37NgfIIbktDnbiAJfSG/EqVag62FGb5OV68QFK5tRn9dG+7fTmdRV69uk+fcFpdLRV09nRwsGeQvVmoKrWtYdxdsLczILa/lZJp+5tC0ls/3XiQmsww7CzOWtKBSr39nRwLcbKhRqfn9dFYrR6k/dfVqEnMbFxWJJPTgri7YmpuRX1HH6YxSQ4cjCCblpT/OUlqtItjTjjkjuv7n60MCXHjm5kAAXt0YR5SRvx8IgiAI12/nOV1hxaAuzthbyg0ayyB/ZxRmUrJKa7hYUGXQWAThnzKKq1m643zTx5/tucCJ1GIDRtRx1as1bDqTze3LDzNtxRG2xuWi1cKIHq6sfnAgfz05lGl9vTE3u3SkzJAAF7bOH8pdDYvXvz+Sxs0f7+fIxSJDfBuCcEX9+nXjm68e443X78LXx4XS0mo+/mQzM+5eyl9/nUKt1lz2cUVFFcx74muSknJwcrLhs08epnuAVxtHL1zJ1weSOZqsGyX40fReopuGgTX7p6/RaFqlFXdlZSX33HMPX331FY6Ojk2fLysr45tvvmHp0qWMGjWKvn37snLlSg4fPszRo0cB2L59O/Hx8axevZpevXpxyy238Prrr7Ns2TKUSiUAK1aswN/fnw8++ICgoCDmzZvHtGnT+PDDD5uea+nSpTz88MPMnj2b4OBgVqxYgZWVFd9++63ev1/BeAU2JKFzymopqVJe1z4aW7eGeNmJVop6MLy7K+525hRXKdkZ3zaLQtZHZQIwta830qvMuhUEfTA3kxEZ4gHAxoYKfGMUl13Gxw2rz1+9LQQPe4tmP1YikXDPQN2F5+qjaSZTZXE+txKVWou9pRxvR0tDh2NwCjMpIxq6U2yPyzVwNIJgOrbE5vDX2VzMpBLeuyP8ihe/jwzrwoQwT1RqLXNWR5H/j7l6giAIQvvReF07Jsjwo+4sFTIGdNa1ptx/vsDA0QiCjlar5YXfz1KjUjPQ34kpvTuh0cL8X6JNrrOWKauoVfH1gWSGv7eXeT+dJjqjFIWZlBn9fdi+YBirZg9gSIDLVe992lrIeXtKGD88OAAvewtRFW3EzMzMmDVrFrNmzcLMrOONJZRIJIwaGcbqH57k2Wdux8XFjtzcUl5/cz2zZn/KwYPnLrmXVVBQxtzHvyIlJR8XFzuWffYwXbq07YiNerWGs1llrDqUwpO/nGb53gsmc7+ttZ3NKuP97YkAvDwpmM4u1gaOSDD4EoC5c+cyYcIExowZc8nno6KiUKlUl3w+MDAQX19fjhw5AsCRI0cICwvD3f3vF3lkZCTl5eXExcU1bfPvfUdGRjbtQ6lUEhUVdck2UqmUMWPGNG0jdAw25mb4Ncw3vd5q6Lhs3ePEPGj9MJNJubOfrhr6lxOt354yp6yGgxcKAZjaR7TiFtpGY0vuLbE51F9hhaUh1dWreWptDPUaLTeHeDC5V8tfG7f38cZSLuN8XqXRV3w3+mcrbrGoSKexJfe2uFxxcSMIzVBcpeTF388C8NiIroRc5fxQIpGwZFo43d1tyK+o47EfT6GsN773BEEQBOH6FVcpOZmmq+Y09DzoRo0twfeJJLRgJP6MyWbf+QIUZlLemhLGq7eF4ONkSVZpDS81nFcJrSezpJo3NsUT8fZu3th8jqzSGpysFTw5OoBDz47inanhdHe3vfaO/mFogCvbFgwTVdFGzNzcnFWrVrFq1SrMzZs/D7m9MTOTcdut/Vn7y0Iem3MztjYWJCfn8cxzPzDnsS+JiUklN7eUx+Z9RXp6Ie7uDixf9jB+vq0/XqOqrp6DSYV8tPM8M78+Rs9XtzPx04O8sjGeP6KzWbI1UYx2Qjdmc/6aaFRqLeOC3ZvyCoJhGXRpyy+//MKpU6c4ceLEf76Wm5uLQqHAwcHhks+7u7uTm5vbtM0/E9CNX2/82tW2KS8vp6amhpKSEtRq9WW3SUhIuGLsdXV11NXVNX1cXq5LPqpUKlQqsTLPVPVwtyGtqJrYzBL6+7U8kRybWQpAoIeN+Dtoocaf179/brf38uDT3Rc4kFRIcn4ZPo6t1+b815MZaLXQz88BLzuF+B0KbaK/rx2OVnKKqpQcOJ/PkG7G1QZ+6fYkEnIrcLKW88qkQOrrW75i2coMJoZ7sC4qix8Op9KrU8suWg0hJkOXLA/2FMfzRjd1cURhJiW1qJpzWaUEuNsYOiRBMGov/R5LUZWS7m42PDK08zWPJQopLLurJ1NWHONkWgmvbTzLyxOD2ihaQRAEobXtiMtGo4VAD1vcbeRGcY45uIsDAMdSiqisrsVcLrv6AwShFZVUK3l1o66o6LHhXfB10CXDPpgaxl3fnOD36GyGdnPm1p6ehgzzmq50f8uYxWSWsfJQGlvj81BrdAuOu7pa88BgP27t6YlFw7Hher8nCxm8NimQcUGuPP97XFNV9L0DfXhqbADW5h2v+lYwXjKZhOl3RjD+ll78/MtB1v96jDOxacyZ+yVWVuZUV9fh6enIR0vvx93NrlVe67nltZxKK+Vkeimn0ktIyK1sem02sjE3o7ePPXYWcjafzeXVjXF0d7Wit6+D3uMxFW9uOseF/EpcbRS8fmvQdd3DFJqnJX/3BjvCZ2Rk8OSTT7Jjxw4sLJrf1tNYvP3227z66qv/+fz27duxshKzgE2VWYUEkLEzKgGPsvgWPVajhdhMGSChKOk0W7JOt0qM7d2OHTv+87nu9lLOl0l5d81+xvu2TlWQVgvfR+t+fwFmRWzZsqVVnkcQLifYVsqhaikrtpygvJvxVL6lVMCXZ3Wvi9u9azm2b+d178tXCWDG5thsBioysDHsCLxrOhSv+76VuRfZsuWCocMxGt1spMSXSvnsjwNEeotqaEG4kthiCZsSZUjQMsm9lJ3btzb7sTM6S/gqQcbqYxloC1MZ4CZea4IgCO3Bj4lSQIqfrMxorje1WrBXyChTali2bjuBDuI9RzCcny5IKa6S4mGpxacygS1b/i4OGuslYWumjP9tOEPZxdM4m8Ct5Mvd3zImGq3unHVvjpTkir+7f3W31zDSU0ugQxnS/DPs3nFGr887vzv8kSblcL6UH45lsCU6nbu7qukmmkoajFarbSq2Mzc3F93g/sG7Ezz4QCiHD2cRcyaf6uo6HB0tmHybH6dOHdbLc2i0kFsNyRUSkiskpFRIKK777+/AyVyLv63uvy62Wjyt6pFKatFqIdNJSkyxlIdXHePpcDV2Cr2EZlLiSyT8kKBbMDPVp4ajN3APU7i26urqZm9rsCR0VFQU+fn59OnTp+lzarWa/fv389lnn7Ft2zaUSiWlpaWXVEPn5eXh4aGbn+nh4cHx48cv2W9eXl7T1xr/3/i5f25jZ2eHpaUlMpkMmUx22W0a93E5ixcvZuHChU0fl5eX4+Pjw7hx47Czs2vBT0IwJubn8tnyUzQVUjvGjx/cosdeLKhCefQQFnIps6bcgkzME24RlUrFjh07GDt2LHL5v7JTPrk8ufYM0RWWfBQ5FLMrzFO8EaczSsk/ehxLuZRn7hqFjViFKbQh55RiDn17knMVCkaPG4G5mcGnZVCtrOfD5UfRUs3knp48Ny3shve5regoZ7PLKXMO4s4h/nqIsnWo1BqePr4L0DJz4nD8nMTiskZV7pk8/3s8aWoHxo+PMHQ4gmCUSqtVvPHpIUDJw0P9eXRc9xY9fjxgufsin+y5yLo0OdPGDiC0k7i+EARBMGV1KjWLo/YCah6dNNiojusHlGf59VQ2SqcujL+5h6HDETqoI8lFHDsShUQCH88cSJ9/VfKNU2vI++YEpzPK2FLswuoH+hvtfber3t8yAlV19fx6OptVh9PIKKkBQC6TMDHck9kRfgR5tn7nsinAwQtFPP97HDlltXwab8a9A314elwAVgpxP66tVVVV4ejoCEBJSQnW1mKG7r9NvxMyMgo5cvQ8Y8eE4+h4/Z3hapRqzmSVEZVWSlR6CaczyqiovbRiVyrRdU7p6+tAXz9H+vg64Gl/5dU3w8fUM+2LY1wsqOKPQhe+n90PeSvcPzdWRVVKXv/sMKDkvkG+PDUh0NAhtXuNnaGbw2BH9dGjRxMbG3vJ52bPnk1gYCDPPvssPj4+yOVydu3axdSpUwFITEwkPT2diAjdTc+IiAjefPNN8vPzcXNzA3SrzOzs7AgODm7a5t8rTHfs2NG0D4VCQd++fdm1axeTJ08GQKPRsGvXLubNm3fF+M3NzS87I0EulxvlCYbQPGG+TgBcLKxCK5GhaEEiKDG/CoBgTzsszDvgciM9udxr6OZwLxw3nSOvvI4jqaWMCtT//KzfY3Qt/G8J9cTRxlLv+xeEq4no5oa7nTl55XUcTSk1ihlxH/51ntSiajzsLHh1cphe3ttmDvLjud9iWXMyi0eHByA10psG5wvKUKm12FqY0dXNTqwC/odxoV688Ec8cdkV5FWq8G7FEQmCYKre3hZHQaWSrq7WLBwXiPw6WpvOH9uDuJwKdiXkM++XGP6cdxPONh13PpsgCIKpO3ixhGqlGnc7c3r5ORnV+eWIHu78eiqbAxeKeFHczxIMoFal5qU/zwEwc6AfA7v+d76qXA4fz+jD+E8OcDKtlK8PpTFvVEBbh9oixnaPOKeshu8Op/HTsTTKGxJeDlZyZg70474IP9zs2ra8fGSQB9v9nXlryzl+Pp7BD8cy2JdUxJJp4QzqYlxjytq7f/6dGtvfrTHp0sWTLl1aPg6goKKOqLRiTqSWcDKthLisMur/1VrbSiGjt68D/fyc6NfZkd6+ji0qkHKUy/nyvn7c9tkhTqaV8t6OC7w8KaTFsZoirVbLC3/EUFippLu7Dc9PCL6ua3ChZVpynDBYEtrW1pbQ0NBLPmdtbY2zs3PT5x988EEWLlyIk5MTdnZ2PP7440RERDBo0CAAxo0bR3BwMPfeey9LliwhNzeXF154gblz5zYliB999FE+++wznnnmGR544AF2797N2rVr2bx5c9PzLly4kFmzZtGvXz8GDBjARx99RFVVFbNnz26jn4ZgLLzsLbCzMKO8tp6k/ApCvJrfC+ZsVhkAoZ1E/xh9MzeTMaWPN98cTOGX4xl6T0LXqtRsjMkGYGpfb73uWxCaQyaVMCHMi28PpbDpTLbBk9CHLhSy6nAqAEumhWNvqZ8LkFt7efHm5nOkFVVz6GIhQwP+e3PBGMRl6VbzhXrZG9UNQmPgYmNOPz8njqcWsyM+j9k3GW9FuyAYwu6EPH47lYVEAkum9Wyan9dSUqmEpdN7MXnZIVIKq3jil9N8N3tAq3SDEQRBEFrfjnO67ntjgtyN7vxySDcXpBI4n1dJTlkNnvZiUbbQtj7dnURqUTXuduYsuko1vq+zFa/dFsLCtTF8uDOJIQGu9PJxaLtATdTZrDK+OZjCxpjspsSXv4s1DwzxZ2qfTgatPLa1kPP2lHBuCfXkuV/PkF5czYwvj3L/4M48c3MPURUtmBytVsvFgkpdwjm1hKi0YlKL/tu22N3OnH6dnejn50g/PyeCPG1v+Fqvq6sNH9zZk0d+iGLloVR6ejswuXenG9qnKfjlRAY7z+WhkEn5aHrv674GF1qPUR/JP/zwQ6RSKVOnTqWuro7IyEiWL1/e9HWZTMamTZuYM2cOERERWFtbM2vWLF577bWmbfz9/dm8eTMLFizg448/xtvbm6+//prIyMimbaZPn05BQQEvvfQSubm59OrVi61bt+LubvhKNKFtSSQSgr3sOJpczLmcliah/05aCPo3o78P3xxMYVdCPvnltXpdobkjPo+K2nq87C2IEKstBQOZ1NOTbw+lsCM+jxqlGkuFYU6aymtVPLNeN/Np5iBfhnXXX6LYSmHGlD6d+O5IGj8eTTfaJHRsw6KiMG9xPL+ccSHuHE8tZltcrkhCC8I/lNeqeP63swA8eJM/ff0cb2h/9pZyvri3L5OXHeLQhSLe25bI4vFB+gjVJBxNLmLNiQweH9WNLq7X3+5OEATB0DQaLTvjdUnosUbQ8ejfHK0VhHs7EJ1RyoHzhdzZ38fQIQkdSEJuOV/sSwbg1VtDsbO4+gLo23t3Yk9iARtjsnnyl9NseWIo1mKc2n9oNFr2JObz1YFkjiYXN31+oL8TDw3twuhAN6PqTDasuyvbFgxrqopedTiV3Qn5oipaMHq1KjWxWWVNCeeTaSWUVqsu2UYigR7utvT1c6R/Zyf6+jni7WjZKovSIkM8mDeyG5/tucBzv52hu7stwV7GMwJE35ILKnltYzwAT0d2b9ffqykzqnfpvXv3XvKxhYUFy5YtY9myZVd8jJ+f33/abf/biBEjOH369FW3mTdv3lXbbwsdR5CnLgkdn10OfZv3GK1Wy9lsXdIixIhmO7UnAQ1v1lFpJaw/lcljI7rpbd/rozIBXRW0MZ2ECx1LLx8HvB0tySypYU9iPuPDWt7iRx9e3xhPVmkNvk5WLL5F/8mOuwf68d2RNHacyyO3rBaPq8y0MZTGJHSIOHm9rMgQD97YfI7jKcUUVylxshYjKAQB4K3N58gtr6WzsxVPjdPPTM3u7ra8N60nc386xRf7kwnztmdiuJde9m3Mfj+dxaL1MajUWjJLqln7SITRVQ4KgiA0V2xWGfkVdVgrZER0Nc5kyrDurkRnlLIvqUAkoYU2o9Zoee7XWOo1WsYFu3NzqMc1HyORSHhjciin0kpIK6rm1Y1xLJnWsw2iNQ01SjW/nsrk24MpJBfqxgaaSSVMCPfkoSFdjHqhtaiKFkxBcZWSqLQSTqYVczK1hNjMMpRqzSXbWMil9PR2oF9nR/p1dqKPr6PeOgw2x4Kx3TmTVcb+8wU8svokG+cNwcGq/d23Uak1LFgTTY1KzeCuzjw0pIuhQxKuQPRzE4R/CfbUJR3O5TR/uHpGcQ0VtfUoZFIC3GxbK7QOb3rDxfCaExlotdprbN08eeW1HEgqAGBqH9GKWzAciUTSlFhobA/f1nbG57EuKhOJBN6/o2errCjv4WFL/86OqDVa1pzI0Pv+b1S9WtN0/A8T4xUuy8fJiiBPOzRa2NnQWlIQOroDSQX8ciKjqQ23PrtZTAj35JHhugvqZ9afITG3Qm/7NjZarZbley8wf000KrXuXO9Eagn7zhcYODJBEITr13i+NLyHK+Zmxtkicnh3FwAOJhWi1ujnWlsQruXHY2lEZ5RiY27Ga7eFXvsBDewt5Sy9sycSCaw9mcmW2JxWjNI05FfU8sH2RAa/s4sXfj9LcmEVthZmPDK8C/ufGcnHM3obdQL6n4Z1d2XrgmHMaLgHuOpwKrd8fIBjyUUGjkzo6H6NymTAmzt5+PuTfLEvmai0EpRqDS42CiJD3HlhQhAbHhvMmZcjWfNIBIsiAxnZw61NE9CgG/n3yYxe+DhZklFcw5O/RLfL9/ZPdiURk1mGnYUZH9zZUxSWGTGRhBaEfwlqSELH55Q3O9HZWAXdw8MWhZl4WbWWieGe2JibkVZUzRE9nXxuOJ2FRgv9OzvS2cVaL/sUhOs1qaeu+nl3Qj4VtaprbK1fxVVKnvstFoCHh3ZhgL9Tqz3XzEF+APxyIp36f60YNbQLBZXU1WuwMTejs7M4JlxJZIiuleT2uFwDRyIIhldZV89zv+qOn7MiOrfK8XPRuB4M6eZCtVLNIz+cpKymbd8j2oJao+XFP86yZGsiAP83rAsPDtG1/P9g+3m9LUAUBEFoazvi/54Hbax6ejtgZ2FGWY2KmMxSQ4cjdAA5ZTVN7/nP3tyjxR2yBnZx5rERXQFY/FssOWU1eo/RVJxKL2HU+/v4dPcFSqpV+DhZ8vKkYI4sHs3iW4LwcjC9Oe92FnLemRrO9w8MwMvegrSiaqZ/eZRX/oyjWllv6PCEDii3rJaX/4yjXqOli4s1M/r78N60cPY+PYIT/xvDF/f246GhXejt62gUuQEHKwUrZvbF3EzKvvMFfLzzvKFD0quTqcUs23MBgLemhOFpb3rHuY7E8K8IQTAyAe42mEkllNWoyCmrbdZjzja0bg0VrbhblZXCjFt76SpF9VFBqdVq/27FLaqgBSMQ7GlHF1dr6uo1bV5h+uIfZymsrCPAzYaFY7u36nPdHOqBk7WCnLJa9iQaV3VbbKbueB7sZSdWUV5FZIiuVd7+pEKq6sRNAKFje+evc2SV1uDjZMmiSP204f43M5mUT+/qTScHS1KLqpn/y2k07Wg1e41SzaOro1h9NB2JBF6eFMzz44N4bERXrBUyYrPK2CYWvQiCYIIyiqtJyK1AJpUwsoebocO5IjOZlCEBumro/aL7hNDKtFotL/4eR2VdPX18HbhnoN917Wf+mO6Ee9tTVqNi4ZqYdnVu1Fzncsq5/9vjVNbVE+Jlx+f39GHv0yOZfZM/Nu1gVraoim4bMpmMadOmMW3aNGQy4+zYYWivbtQds3r7OrBz4XDemRrOHf186OxibbRjg0K87HlnahgAn+y+0LQoztRV1KqYvyYajRam9OnUIcZVmTqRhBaEfzE3k9HNzQZANxe6Gc42bBfiZRqtbUxZ44nnX2dzKa1W3tC+YjLLuJBfiYVcyvhww8zfFYR/kkgkTGpqyd12LcX+jMlm85kcZFIJS+/shYW8dS86zM1k3NFXt/Bj9dG0Vn2ulorLFq24myPQwxZfJyuU9Rpxo1Lo0A5fLGT10XQA3p0S3ipjDBo5Wiv44l7davY9iQV8tCup1Z6rLRVXKbn766PsiM9DYSZl+d19mH2TrgLa2cacB/5RDd0e28gJgtC+Nd7w7efniKO1cc9jHBbgCogktND6tsXlsvNcHnKZhLenhF/34l+5TMrHM3pjKZdxJLmIrw4k6zlS45ZaWMW93xynvLaevn6OrHs0glvCPJG1s8XUjVXR3z0wAM+GqugZX4mqaH2ysLBg3bp1rFu3DguLlnUl6Ah2ncvjr7O5yKQS3ro9zKQKFm7v7c39gzsDsHBNNMkFlYYNSA9e/jOOzJIavB0tefXWEEOHIzSDSEILwmW0ZC60VqslrqkSWiQtWltYJ3uCPe1Q1mvYcDrrhva1PkpXTX1ziAd2Fm07n0MQrqSxJff+8wU3vNCiOfLLa3nx97MAzBvZrc3mRN01wBeA/UkFpBdVt8lzNkdsw/FcJKGvTiKRNLXkFtWJQkdVrfy7DffdA30Z3M2l1Z8ztJM9b09pWM2+K8nkV7OnFVUx9fPDnE4vxcFKzk8PDeSWsEsXBj40tAt2FmYk5VfyZ8yNnfsJgiC0tcbuRmODjbcVd6Nh3XVJ6OiMUsqq29/YB8E4lNWoeOmPOAAeHd6VHh62N7Q/fxdrXrk1GID3tyc2dSps73LKarjn62MUVtYR5GnHt/f3x0ph+pXPVzO8uyvbGqqitdq/q6KPpxQbOjShHatW1jcdsx4a4t80xtOUPD8+iP6dHamoq+eRH6JMupvdpjPZ/HYqC6kEPpzeC1txP98kiCS0IFzGP+dCX0tueS1FVUpkUgmBN3jyLFybRCJhxgBdNfQvxzOuez5grUrNn9HZAEzr66O3+AThRnVzsyXI0456jZatZ1s3uafVannut1jKalSEdrJj3qhurfp8/9TZxZqhAS5otfDzifQ2e96rUWu0TR0wxHiFaxvX0JJ7V0I+ynrjmu0tCG1hydZE0our8bK3YPEtgW32vFP6eDMrQte2cuGaaC6a6Gr2mIxSpn5+mJTCKrwdLVn/6GD6df7vPG17SzmPDNfNfPxwRxIqtTjeCIJgGsqqVRxrSI4Y8zzoRl4OlgS42aDRwsELhYYOR2inlmxNIL+iDn8Xa+aO1M/15539fIgMcUel1vLEL6epUar1sl9jVVRZx8yvj5FVWoO/izXfPzAAe8uOkYi5XFX09C+PiKpoodV8tDOJrNIaOjlY8uSYAEOHc10UZlKW3d0HN1tzkvIreWb9meu+n25IOWU1/G+DrojmsRHd6H+Za0fBOIkktCBcRrBX8yuhz2bptglws2n1FraCzm09O2FuJiUxr4KYzOtb5brrXD7ltfV42lsQ0dVZzxEKwo1prIbedKZ1W3KvPZnB7oR8FGZSlt7ZC7msbU8LGmd/rT2RYRRJzOSCSmpUaqwUMvxdbAwdjtHr4+uIi42Citp6joqZXEIHcyK1mO+OpALw9tTwNl+B/cLE4EtWs1ea2Gr23Ql5zPjyKIWVSkK87PjtscFN43AuZ/ZNnXGxUZBeXM26k5ltGKkgCML123s+H7VGS4CbDZ1drA0dTrM0VkOLltxCaziRWsyPx3QLkN+6PUxv99AkEgnvTAnH3c6c5IIq3tgcr5f9GqPyWhWzVh7nYkEVXvYWrH5oIK625oYOq801VkVP7yeqovWhqqoKiUSCRCKhqqrK0OEYjfjscr45mALA65NDTLrbgJudBZ/P7INcJmFzbI7JjS/QaLQ8tTaGshoV4d72JrsgoKMSSWhBuIzGSujUoupr3tRrbPUj5kG3HXsrORMaWjX+cvz6KigbW3FP6dOp3c3LEUxf41zowxcLKaioa5XnyCiu5rWNuovzp8d1p7t723dyGB3khrudOUVVSqNo6RzbdDy3E8eFZpBJJU2tJbfHG/73JwhtpUapblg9Dnf282Z4ww37tiSXSVl2Tx/c7cy5kF/JonUxJrOa/Zfj6Tz8fRQ1KjXDuruy5pEI3GyvPnvOSmHGYyN01VKf7k6iVtW+K5wEQWgfGkcmjDGBVtyNmpLQSQUm874imIa6ejWLf9ONMbmzn7feiwEcrRV8cEcvAH48lm7yI0sup0ap5qFVJzmbVY6ztYIfHhpIJwdLQ4dlMHYWct6dFs6q2f0vqYp+dWOcUVbD1yjVZJZUcyazlJiMUnGMNXJqjZbnN8Si1mgZH+bBqEDTeS+/kr5+Trw0STdD+Z2/EjhkQl1PvjmYwuGLRVjKZXw0ve2LaIQbI35bgnAZTtYKPOx0N8MSc69eDR2X3TgPWrRubUvT++taaP8Zk93i6p/88lr2NazsntrHW++xCcKN8nGyopePAxot/HVW/9XQGo2WRetjqFKq6d/ZkQeHdNH7czSHXCZlen/dbOjVR9MMEsM/NXa2EIuKmq+xJff2uDw0GnERLcBvpzJ5568Ek6vMbYkPd54npbAKdztz/jch2GBxuNlasPyevshlEv46m8vn+y4aLJbm0Gq1LN1xnud+093MuaOvN9/M6oeNefMqCu4e6IunvQU5ZbVNVVSCIAjGSlmvYV+i7prTFFpxNxro74S5mZScslou5JvmuAfBOK3Ym8yF/EpcbBQ8Pz6oVZ5jSIALDw/1B+DZX8+QX17bKs9jCMp6DXN+jOJ4ajG2FmZ898AAurqK7l0AI3q4XVIVvfJQKrd8vJ8Tqa1bFd2YVI7JKGVPQj7rozL5cv9F3t5yjqfXxfDAqhPc9tlBhry7m+CXthL00laGvLuHWz87xG3LDjVV2ArG6afj6URnlGJjbsbLDYnb9mDmQF+m9fVGo4V5P50is6Ta0CFdU3x2Oe9tSwTgxYnBdBHHPpNjuj0EBKGVBXnaklteS3x2OX39rjxjoDFpEdpJJC3a0gB/J7q4WJNcWMXmM9lNiazm2HA6C40W+vo5ijcuwWhNDPckOqOUjTHZ3BfRWa/7/u5IKkeTi7GUy3j/jp4Grfq9a4APn+1O4lhKMRfyK+jm1vYV2Y0aO1uEieN5sw3u6oyNuRn5FXVEZ5bSx9fR0CEJBpSUV8HT62LQaHVtPFfO7o+73dUrXE3N6fQSvm5oXfbW7WEGn7/X18+RV24N4X8bzvL+tkRCveybqtiMiUqtYfFvsayP0rXSfmJ0AAvGBCCRNP/9x0Iu44nRASz+LZbley4wo78P1s1MYAuCILS1YylFVNTV42KjoLePg6HDaTYLuYyBXZzZf76AfecLCDBAtySh/bmQX8myPRcAeGlSCA5WilZ7rqcje3DwQhHncsp5al0M380egNTEu1ypNVoWro1mb2IBFnIpK+/vL+5B/ktjVfQtYR4s/i2W1KJq7vziCLMH+7MosgeWimu3fq9RqimsrKO4SklxlfKSfxdVKSlq+Fj3byU119GZRyGTYmcpp7CyjiVbExka4EoPD3GcNTb55bUs2ZoAwKLIHu3qmlYikfDG5FAScss5m1XOnNWnWPdohNGOGK1VqZm/5jRKtYYxQe7cNcDH0CEJ10FctQvCFQR72bEnsYD4nIorblNQUUdueS0Syd8tvIW2IZFImN7fh7f/SuDn4xnNTkJrtVp+PaW7ASqqoAVjNjHcize3nONEagnZpTV46anN1sWCSt75S3cy/fyEIPycDTufztPektFB7uyIz+PHY+kGW2Gq0WibOluEeYsL+uYyN5Mxoocrm87ksC0uVyShO7h3tybSWBAfn1PO5GWHWDm7P4Ee7eMcqValZtH6M2i0cHvvTow2ksq2uwf4ciajjDUnM3jil9NsnDcEHycrQ4fVpLKunsd+PMX+8wXIpBLenBzKjAHNXzz4T9P6erNi30XSiqpZdTiVuSO76TlaQRAE/djZ0Ap4dKC7ySXAhgW4NCWhHxpqmI5JQvuh0Wh5/rdYlGoNI3q4Mincs1Wfz9xMxiczejHx04McSCpk1eFUHhji36rP2Zq0Wi0v/B7LpjM5yGUSVszsS7/OVy6U6egaq6Lf3HSONScz+PZQCrsT8pg/pjsqteaSJHJxVd0//n2dSWUzKc7WCpysFTjbmP/j34qGf5vjZK3AxUb3+cYOQA+sOsGexAIWro1mw2M3oTATzWqNyWub4qmorSfc256Zg/wMHY7eWchlrJjZl0mfHiQ2q4wXfz/LkmnhLVog3Fbe3ZrA+bxKXGzMeXdqmFHGKFybSEILwhU0JpXP5Vy5HXdjwsLfxbrZrQQF/ZnSx5v3tiUSnVFKQm55s25yx2aVcT6vEnMzKRNa+eJHEG6Eh70F/Ts7cTylmM1ncnh42I3fAKpXa3hqbQx19RqGBrgwc+D1JQH07Z6BvuyIz+PXqEyeiQxs1iplfUspqqJKqcZCLqWLi2ET86YmMsSDTWdy2B6Xx3M3B4qLgg7qRGoxO8/lIZNK+GZWP17bFE9yQRV3fH6Ez2f2ZUiAi6FDvGGf7EpqaCNpzsuTDNeG+98kEgmv3hZCQm45MZllPPJDFL/OGWyQY+m/5ZfXMnvVCeKyy7GUy1h+Tx9GBrpd9/7kMikLxnRn/ppovth3kZmD/AxejS4IgvBvWq22aR7tWBOaB91oeHdX3th8juMpxdSq1EZbHSWYhrUnMziequvC9cbk0Da5Vghwt+WFCUG8+Ecc7/yVQERXZ5MsHNFqtU2FF1IJfDS9NyN6XP95VEdxuaro+Wuim/XYxqSys40ugexs3ZBMbkgqO1ubN/27Mal8PX/T704NZ9xH+4nLLuez3UksHNejxfsQWsfexHw2nclBKtF1vjJk58DW5O1oxSd39WbWt8dZF5VJL18H7hloXAn3/ecLWHkoFYD37gjH2cbcsAEJ101kzQThCoIbTlATcstRa7SXfdOJy25oxS3mhxqEq605Y4Lc2RqXy5oTGc2qoGxsAxkZ4iFuWgpGb1JPL46nFLPxTLZektBf7E8mOqMUWwsz3p1qPKschwW44uNkSUZxDRvPZHNnv7Zvr9PYijvY0w4zmViF3BIjeriikElJKaziQn6laNvYAWm1Wt7acg6A6f19GNHDjV4+DvzfD1EcTynm/pXHeWtKmEFe2/oSm1nGF/t1bbjfmBzaqm0kr4eFXMbnDavZ43PKeX5DLEvv7GnQ4/yF/EpmfXucrNIanK0VfHt/f3rqoSXtpJ5eLN97gfN5lXy1P5mnI8VNO0EQjEt8TjnZZbVYyKXc1M30FmF1c7PB096CnLJajqUUM9wIxzwIpiG/orbpHPGpcd3xdmy7Ti0zB/mxJ7GA3Qn5zP8lmj/m3WRyCyqW773Ilw3nn+9MCReFFC3UWBX9wbZETmeU4mClwKUheexko8CloUq5KcFsY461QtYm589udha8OTmMuT+dYtnei4wMdKO36CpmcDVKNS/+cRaA2Tf5t/u290MDXFkUGci7WxN45c84gjztjKa7XXGVkqfWxQBwX4QfI8UCHJMm7rIKwhX4OVtjKZdRq9KQWlR12W0akxahnUxvRWV7MaNhFsSG01nUXqN1Tl29mj+iswFdO0dBMHa3hHogk0o4k1lGauHlj0PNFZ9dzkc7zwPwyqQQvbX31gepVMLdA3QrLn88lm6QGGIzG4/n7fsiozXYWsi5qZszANvicg0cjWAI2+LyOJ1eiqVcxvzRAQA4WCn44cEB3NrTi3qNlmfWn2Hp9kS0Wq2Bo205Zb2GRetjUGu0TAz35OZQD0OHdFleDpZ8dncfZFIJG05nsepwqsFiOZFazNTPD5NVWoO/izW/PTZYLwloAJlUwsKxusTzt4dSKKys08t+BUEQ9GVnfD4AQ7q5GkVXipaSSCRNied9iQUGjkYwZa9ujKe8tp6wTvbcP7hzmz63RCJhybRwXGwUJOZV8G7DfFdT8f2RVN7blgjACxOCuLO/6S7mNCQ7Czmv3hbKn/OG8P0DA1g6vRcvTAzmsRHduLO/D2OC3enj64ifs/V1VzVfrwnhntza0wu1RstTa2OoUba8Hbi+yGQyxo8fz/jx45HJTO99S18+3Z1ERnENXvYWLBzb3dDhtIlHh3fhllAPVGotc1ZHUVBh+GsrrVbLc7+eoaCijm5uNiy+JcjQIQk3SCShBeEKZFIJgZ66aq747Mu35D7b0I5bVEIbztAAV7zsLSitVl0z+bH7XD5lNSo87CxMckW60PG42JgzuKsuubc5Nue691NXr2bh2mhUai3jgt2Z0qeTvkLUmzv6eSOXSYjJKG1a4NOWmo7nIgl9XcaF6JJy2+LyDByJ0Nbq1RqWbNPd1HtoqD9udhZNXzM3k/HR9F7MHdkVgE92X+CptTEo6zUGifV6fbbnAgm5FThbK3j1VsPMrW+uiK7OLL4lEIA3Np/jWHJRm8fwV2wO93x9jLIaFb19Hfh1zmD8nPU75iAyxJ1wb3uqlWqW77mo130LgiDcqB3ndNel40ywFXejYQ1J6P1JIgktXJ9d5/LYfCYHmVTC21PCDNJtysXGnPfu6AnAykOp7E3Mb/MYrseG05m89EccAE+MDhCz2dux124Lwd3OnOTCKoMulLCwsGDz5s1s3rwZCwuLaz+gHUrMrWjqPPDKrSFYd5CxmxKJhPfu6ElXV2vyyuuY+9MpVGrDXq+vPZnB9vg85DIJH03vZZIL+oRLiSS0IFzF1eZCl1WryCiuASBEJKENRiaVcEdDe881JzKuum1jK+7b+3RqtzM9hPZnUk8vADbGZF/3Pj7ZlURCbgVO1gremhJmNG24/8nFxpybQ3Xtxdq6Glqj0RKXpTvOh4kk9HUZE+SORAKxWWVkldYYOhyhDa05mUFyQRVO1gr+7zJjA6RSCYsiA3l7im6e1m+ns5j17XHKalQGiLbl4rLLWL7nAgCv3hZiEnOoHhziz229dFUVc386RU5Z270mVx5K4bGfTqGs1zA22J2fHhqEk7X+W5dLJBKeapidt/pYWpt+j4IgCFeTU1bD2axyJBIYGWi6rSNv6uqCVKIbrZAtzu2EFqqqq+fF33UtbR8aYtiWtiN7uDVVYT+97gxFRt5BZXtcLk+vOwPA/YM7s2BMgIEjElqTg5WCJdN0CyVWHU7l0IVCA0fUMWk0Wv63IZZ6jZaxwe5Ni+w7ChtzM764tx825mYcTynm7S2GWxCRWljFqxvjAXhqXA9RKNJOiCS0IFxF41zo+MskoeMaquZ8nCyxtxKzhQ3pzv4+SCRw+GIRaVdonZ5fUcve87pV3FP7iFbcgumIDPFALpOQkFvB+byKFj/+VHoJn+/VVYm9dXsoLkacQLlnoC8Af0RnUVHbdgmqtOJqKurqUZhJ6eZm02bP25642prTz083O2iHaMndYVQr6/loZxIAj4/qhq3Flc+H7hrgyzez+mGtkHEkuYhpnx8ms6S6rUK9Liq1hkXrzlCv0XJziAcTwkxjDp9EIuGdKeEEethSWKlkzupT1NW3bns/jUY3F/zVjfFotXDvID9WzOzbqqvWhwW4MKCzE8p6DZ/sutBqzyMIgtASO8/pKi17+zjgamu8593XYm8lp1fDGIX950U1tNAy729PJLusFh8nS540giTqc7cE0t3dhsLKOp799YzRjoc5dKGQeT+dRq3RMrWPNy9NDDbKBeSCfg3v7srMQbp7IU+vizGZxbrtyZqTGZxMK8FKITP6zletpZubDe83dI749lAKf0RntXkMKrWG+WuiqVaqGejvxMOiC0S7IZLQgnAVV6uEFq24jUcnB0uGBejahV2pGvqP09moNVp6+zqIJJNgUuwt5Qzvrqui2NTCaugapZqn18ag0cLkXl5NlcbGaqC/E93cbKhWqvn9dNud8Da2/w7ytENugDZx7UWkaMnd4XxzIIWCijp8nay4Z6DfNbcf0cONtY9G4G5nTlJ+JbcvP9w0j90YfbHvIvE55ThYyXltcohJ3QS0VMj48t5+2FvKic4o5ZU/41rtuerq1Ty5Jrqpfd2zNwfy2m0hrd51RiKR8HSkrhp63cmMKy5EFARBaEs74nXnQWODTb+KqvEaZJ9IQgstEJ1RyqrDqQC8MTkMK4XhW9payGV8PKM3CpmUnefy27zzVnOcTi/h4e9PolRriAxx592pYUhFB78O4/nxQfg5W5FTVsurG1vvvP1KqqqqsLa2xtramqqqjnVOXVBRx9tbzgGwcGx3vBwsDRyR4dwc6tE0SuvZX89cNh/Smj7bfYHojFJsLcxYOr2X6GLajog7rYJwFYEetkgkkFde95+WPWcbWreKthDGYUZ/XUvudVGZ1P9rdoVWq21qxT2tr6iCFkzPpJ665PHGMzktWrW9ZFsCyYVVuNuZ8+qtoa0Vnt5IJJKmaugfj6W32Qr1xiR0WCe7Nnm+9mpcw83W46nFlFQpDRyN0NqKKuv4oiHp+HRkDxRmzbusCPGyZ8NjNxHoYUtBRR13fnGEXeeMb+HC+byKpuralycF42ZrerPRfJ2t+HhGLyQS+Pl4Bj8f1/8N17IaFfd9c5yNMdnIZRI+nN6TOSO6tlnCfoC/E8O6u1Kv0TZV5QuCIBhKRa2KIxd1rVTHBptuK+5Gw7q7AHDwQuF/rrEF4XJUag2Lf4tF27AIenjDbHFjEORpx7O3BALwxuZ4LuS3vMtYa0nILef+lSeoVqoZGuDCJ3f1NsgMbcFwrBRmLL2zJ1IJ/HYqi61nc9o8hurqaqqrjbtTVWt4c3M85bX1hHjZNbXu78gWju3BsO6u1Ko0PPJDFGXVbVOZH5VWwqe7dddzb0wOpVMHXgzQHol3NEG4CmtzMzo7WwNwLufSE9TGSugQL5G0MAajg9xxsVFQUFHH7oT8S74Wl11OYl4FCjMpE8O9DBShIFy/MUHuWMilpBRWEZfdvJWIhy8WsvJQKgDvTg03mbEBU3p7YyGXkpBbwan0kjZ5TtHZQj98na0I9LBFrdGy0wiTioJ+fbr7ApV19YR1smdiC9tUezlYsu7RCIYGuFCjUvPw9yf54Uhq6wR6HerVGhati0Gp1jA60I3JvToZOqTrNqKHG083zE5++Y84TuvxuJpdWsMdKw5zLKUYW3MzVs0ewO29236x39PjugPwe3TWdY2tEARB0Jf95wtRqbX4u1jT1dX0u2+FezvgYCWnoraemMxSQ4cjmICvD6RwrqGLzIsTgw0dzn/MHtyZoQEu1Ko0PPFzdKuPK2mO1MIq7v3mOGU1Kvr4OvDFvX0xN2u9cSaC8err58Qjw3VVqM9vOEtBhXHPL28PDiYV8nt0NhIJvHV7mFj8AcikEj6Z0QsfJ0vSi6t5cs1pNJrWLRCprKtnwZropi6Ot5nw9bdweQZ9ZX3++eeEh4djZ2eHnZ0dERER/PXXX01fr62tZe7cuTg7O2NjY8PUqVPJy7v0pmZ6ejoTJkzAysoKNzc3Fi1aRH19/SXb7N27lz59+mBubk63bt1YtWrVf2JZtmwZnTt3xsLCgoEDB3L8+PFW+Z4F0/P3XOi/20VW1tWTUqhrTxIikhZGQWEmbZr1/O+W3I1V0OOC3bG3NI1EnCD8k7W5GaMD3QHY2IyW3BW1KhatOwPA3QN9GdHDdCox7K3kTGpYLLL6aOu3SdNqtaKzhR41tuTeHi+S0O1ZelE1Px5LA3Qz9q6nVaCthZxv7+/Pnf280WjhxT/ieGvLuVa/wG2Orw+mEJNZhq2FGW/eHmZSbbgv57ERXYkMcUep1jBn9Sm93NA6l1PO7csPcT6vEnc7c9Y+GsFN3Vz0EG3LhXs7EBnijlYLS7efN0gMgiAIQNMivDFBbib/3gG6G9GNx/Z95wsNHI1g7NKKqvhop+59+IUJwTjbGN9MdKlUwgd39MTRSk58TrnBzxtyymq45+tjFFTUEehhy8r7BxhF+3LBcOaPCSDQw5biKiXPb4g12vnl7UGtSs0Lv8cCMCuiMz19HAwbkBFxsFKwYmZfzM2k7E0s4KNdrdtx6tU/40gvrqaTgyWvTTb+Lo5Cyxk0Ce3t7c0777xDVFQUJ0+eZNSoUdx2223ExelmHyxYsICNGzeybt069u3bR3Z2NlOmTGl6vFqtZsKECSiVSg4fPsx3333HqlWreOmll5q2SUlJYcKECYwcOZLo6Gjmz5/PQw89xLZt25q2WbNmDQsXLuTll1/m1KlT9OzZk8jISPLzL62mFDqmIE9b4NJK6HM55Wi14GFngaut8Z1Yd1TTG1py70nMJ7esFtDNKfw9WjdbVrTiFkxZY0vuTWdyrpmkeWPTObJKa/BxsuT58UFtEZ5ezRykmy27OTaH4lZu65xRXENZjQqFTEp3d9tWfa6OoDEJvf98AdXK+mtsLZiq97cnolJrGdbd9YYSj3KZlHenhvPUWF0l65f7k5n38ylqVYarSrmQX8nSHbobki9ODMbD3vTacP+bRCLh/Tt60tXVmtzyWub+dArVDbRVPXShkDtXHCGvvI7u7jZseOwmgjwN2xnoqXE9kEhga1yuUc8ZFwSh/apXa5o6co0JcjdwNPrT2E55v5gLLVyFVqvlfxvOUlev4aZuzkztY7xVbG52Frw7NRyAL/Ync+iCYRZYFFXWMfPrY2SV1tDZ2YofHhxoMt3LhNZjbibjw+m9kMsk7IjPayqqEfRv+Z4LpBZV425nzlMNnZWEv4V42fPO1DAAPtmVxM5WKjT4KzaHdVGZSCSw9M6e2FmI42B7ZNDlVZMmTbrk4zfffJPPP/+co0eP4u3tzTfffMNPP/3EqFGjAFi5ciVBQUEcPXqUQYMGsX37duLj49m5cyfu7u706tWL119/nWeffZZXXnkFhULBihUr8Pf354MPPgAgKCiIgwcP8uGHHxIZGQnA0qVLefjhh5k9ezYAK1asYPPmzXz77bc899xzbfgTEYxRcEO77fh/tMBtnB8aKuaHGpUurjYM8HfieEox605m8PjoAPYk5FNarcLdzpyhAcYzj0gQWmpEDzdszM3IKq3hdEYJff2cLrvd7oQ81pzMQCKB96f1xMbc9FZSh3vbE9rJjrNZ5fwalcnDw7q02nM1tuLu4WHb7Jm2wpUFedri42RJRnEN+88XcHNoy9o0C8YvNrOMP2N0LcuevbnHDe9PIpHw+OgAvJ0seWb9GbbE5pJXfoyv7uuHk7VCDxE3n1qj5Zn1MSjrNQzr7sod7Wjxmq2FnC/u7cfkZYc4nlLMW1vO8fKkkBbv5/fTWSxaH4NKrWWgvxNf3tfPKLrMdHe3ZXKvTmw4ncX72xP57oEBhg5JEC6hrNcQm1XKsZRiTqWVUFqt4p9LChsrnbRNH3PJx1zx69pLP/7X4y5XQXWlxzpaKfhwRi8xg+86nUgtoaxGhaOVnL5+joYOR2+GNVxDx2SWUlKlxLGN35sF07DhdBYHLxRibiblzcnG30VmXIgHdw3w5efj6Ty1Noa/nhzapn/b5bUqZq08zsWCKjztLVj90EBR4CI0CfK0Y+HYHry7NYFXN8YT0dUZb0crQ4fVrlzIr+DzfRcBeGVSCLYi8XlZt/f2JiajjFWHU1mwJpo/Hx+Cv4u13vafW1bL4g26avQ5w7sysIuz3vYtGBejuTOtVqtZt24dVVVVREREEBUVhUqlYsyYMU3bBAYG4uvry5EjRxg0aBBHjhwhLCwMd/e/V5lGRkYyZ84c4uLi6N27N0eOHLlkH43bzJ8/HwClUklUVBSLFy9u+rpUKmXMmDEcOXLkivHW1dVRV/d3K7vycl2CUqVSoVK1zcB2oW10c9G90V8sqKSyuhZzuYwzDfOQAt1txO9bTxp/jjf687yjjxfHU4pZcyKd/xvix7qTutbct4Z7olHXozH8yB9BuC4yYEygK7/H5PDH6SzCvf5btVtSreTZ9bo23LMj/OjjY2eyx6gZ/bx5ISueH4+lcd9A7+tq99scMQ3zUYM9bU32Z2Vsxga68e3hNP6KzWF0D8O05xVah1ar5e0t8YDufbW7q5XeXjcTQ91xte7LYz9FE5VWwu3LDvH1fb3p7Ky/i9xrWXk4jVPppViby3jj1qD/jPgxdX6O5iyZEspjP0ez8lAqIR423NbLq1mP1Wq1fHkglfd36FqxTQj14N2poZib3fi5m77MHeHPxphs9p0v4MiFfPq1oySQYHqq6uqJzizjRGoJJ9NKiM4oo67++jsQtJWn1pzmu/v7tdp5V3u2PS4HgBHdXdBq1KjayYWns5WM7m42nM+vZF9iHhPCPAwdUrtQUq0kKb+Sfr6OJv96K6pS8vom3fnh4yO70sleYTTnBlfzXGQ3jl4sJKWomud+jeHTGT31ljy/2v2tGqWaB76P4mxWOU7WclbO6ou7jdwkfmZC25kd4cOO+FxOpZfy1Npovm/l9+Z//v2199yGVqvl+d9iUam1jOjuwugezu36+71Ri8Z242xWKSfTSvm/70+w7v8GYq2HYheNRsvCtacprVYR4mXL3OH+4vdgYlry+zJ4Ejo2NpaIiAhqa2uxsbFhw4YNBAcHEx0djUKhwMHB4ZLt3d3dyc3NBSA3N/eSBHTj1xu/drVtysvLqampoaSkBLVafdltEhISrhj322+/zauvvvqfz2/fvh0rK7E6qT3RasHaTEZVPazasA0fGziaKAMk1OYksWWLmD2nTzt27LixHajBUiYjs7SWN37Yyp4LUkCCS8UFtmy5oJcYBcFQ3OokgIzfo9LoTTL/vgb57ryUgkop7pZagtUX2bLlokHi1AeFGsxlMlKLqvn4l630cGidWUh746WAFIrT2LIltVWeo6OxKQcwY/vZbDZaZCATBebtRkKphMPJMmQSLT2lGWzZkqH355jbA75IkJFWXM3kzw7ycKAa/zbolF9QA++d0Z3fTeyk5PSh3Zxu/ac1iHGdpGzPkrJ4Qyx5SdF4XyPPr9HC+hQph/J0L+ZRnhrG2GSya7vxtQcc4CLlcL6UF9Yc5/EQNUZeiCW0I1UqSK6QcLFcQnKFhIxK0HDpH6C1mZaudrr/HC9TcNf49/rvP1vJv/5xpa83d7t/f10C1Krh+yQpR1NK+N+qrQz1EDMoW0KrhY2nde8hDtWZrfL+aEidZFLOI+XnPdFIMox/MYUxq1XD3mwJu3Ok1Kkl+FhrmerfNuc6rWV1kpSSaimeVlq8ys+xZcs5Q4fUbFO9YGmxjG3x+bz83VYGuen32Pfv+1v1Gvg6Ucq5UikWMi0Pdq0h8cQ+EvX6rEJ7Md4JzmbKOJZSwnMrtzLCs/Xem+vq6ggJ0XVJ2r59O+bm7bcy/1i+hOOpMhRSLcOsc/nrr78MHZLRu9UFzmfLSMqvYvbnO5kVoLnh66y9ORIOp8qQS7Xc5lbCzu1b9ROs0Gaqq6ubva3Bk9A9evQgOjqasrIy1q9fz6xZs9i3b5+hw7qmxYsXs3DhwqaPy8vL8fHxYdy4cdjZiRbN7c2a/JMcSS7GuVtPRoV5sPDYbkDLfZNG4tkOZgUaA5VKxY4dOxg7dixy+Y21QYmRnGP1sQx+S1egQU24tx0PTBukp0gFwXDG1GtYu2QfpTUqXIIGMajL3y25t8TmcurIGWRSCcvvG0i4t70BI9WPMw2v5YsSTxaM76X3/Wu1Wl6J2QuomD5uMGGdTP9nZgzUGi2rl+yluEqFU9BAbuoqWiq1BxqNlhWfHwUquC+iM/fecuOtuK9kUkUdj/x4mtiscpYnKHh/aii3hLZe5ZVGo2XmypOoNCVEdHHi9fv7Gn0byRsRqdHyf6tPsT+piJ/TbfhtziAcrS7fgrJGqWbhujMcyitAIoH/3dKDWRF+bRxx8/Uuq2XMRwe5WKHBrscAht7AzHJBuJrc8lpOppZwIq2Ek6mlnM+v/M82XvYW9O/sSD8/R/p3dqSLi5VRH1u8jqbz2uYENmfKeXTyYPycxOL65krKq6Tw6GHkMglP3jlWLxVCxsT+YhF7VkWRWmvJLbcMM+q/Y2NVp1Lz04lMVuxPprhKVzkklUBGlYSPzpoxKdyDReO6m9z9pYMXijhxJAqJBD6ZOZBePg6GDqnlPFJ4f0cSf2QoeGDSIL104bnc/S21RstT62I5V5qLhVzKyll9RdcW4ZrkPhm8vPEcWzLl/N+kQXRzs2m157r99ttbbd/GorhKySufHAJUzB/bnXuH+Bs6JJPRo08pM789wekiKeP6BfLQkM7Xva/E3AoWfXEM0PC/CcHcM8BHb3EKbaexM3RzGPzMWKFQ0K1bNwD69u3LiRMn+Pjjj5k+fTpKpZLS0tJLqqHz8vLw8NDdhPLw8OD48eOX7C8vL6/pa43/b/zcP7exs7PD0tISmUyGTCa77DaN+7gcc3Pzy64KksvlN5xAE4xPiJc9R5KLScyrIsirFrVGi5O1Ah9nG3EBpmf6eA3dNdCP1ccyqFLqWqDd0c9XvC6FdkEuh1vCPPj5eAZb4vIZ2kPXxSO/opaXN+lWnM8d0ZW+/u3jpvvMiM6sPpbBzoQCimvUuNvp96ZMZkk1JdUqzKQSQrwdkZvJ9Lr/jkoOjAv24JcTGexKKGREoGjb2B78fjqLc7kV2Jqb8cTo7q36vurlJGfNIxE88fNpdp7L54k1Z3i+QsnDQ7u0ynnX90dSOZFagpVCxpJpPVEo2ve8Sznw6V19mfTZQdKLq3lq/VlWzR6A7F/tNYqrlDz4XRSn00tRmEn5eHovbgkz7jnvvi5yZg7049tDKXy06yIjAz3Eubpww7RaLalF1RxPKeJ4SgknUotJL/7vyv+urtYM8HdmgL8j/Ts7mdz8xvtv6sL2c/kcTS7m+Q3x/PJ/g0y+TXBb2ZNUBMBN3VxwsGl/M7UHdXXFQi4lr6KO5OJaAj1E4UVz1as1/HY6i493JpFVWgOAv4s1T43rzgB/J5ZuP8+akxlsPJPLznMFzBnRlf8b1gULufFfl9Qo1by8UXcNOiuiM/27uBo4ouszZ2QABy8WcTS5mKfXn2X9nMHI9dTKqfH+llar5aUNsWw+m4tcJmHFzL5EdHPTy3MI7dt9g/3ZlVjI/vMFPLshjl/1+PfZEb23I56SahWBHrY8PKyb+Fm2wMCurrw0MZgX/4jjve3n6enjyODrWPBbq1Lz9K9nUdZrGBXoxqzB/uJ6zUS15J6Q0b3SNBoNdXV19O3bF7lczq5du5q+lpiYSHp6OhEREQBEREQQGxtLfn5+0zY7duzAzs6O4ODgpm3+uY/GbRr3oVAo6Nu37yXbaDQadu3a1bSNIAR56i6yzuWUczarDIAQLztxkDRSIV72TVWgCpmUSeHGfcNUEFpiYrhufudfZ3NQqTVotVoW/xrbMEfFjnmjAgwcof4EetjRz88RtUbL2hP6b2vYeDzv7m6LuUhA61VkiC7xvD0+F41GtPQ0dXX1at7frmsU+OiIrjhat36S1kphxhf39muqun1rSwIv/RFHvVq/bUAziqt55y/dCJ5nbw7Ep4NU/tlbyfni3r5YymUcSCps+v02SiuqYurnhzmdXoq9pZyfHhpo9AnoRo+N7IqVQsaZzDK2x+dd+wGC8C9qjZb47HJWHUph7o+nGPDWLka+v5dnf43l11OZpBdXI5VAaCc7HrjJnxUz+3DyhTHsemoEb08J4/be3iaXgAaQSiW8N60n1goZx1OLWXk41dAhmYwdDceaMUHu19jSNFnIZQzqoutss/98gYGjMQ1arZatZ3OI/Gg/z6w/Q1ZpDR52Frw9JYztC4YxMdwLN1sL3pkazsZ5Q+jf2ZEalZqlO84z+oN9bDqTjVZr3OfQH+06T3pxNZ72Fjwd2XodclqbTCph6Z29sLMwIyazjI93Jul1/1qtlnf+SuDn4xlIJfDR9N6M6CES0ELzSCQSlkwNx95SzpnMMpbtEWMGr9eRi0Wsj8pEIoG3poSJBPR1mDnIj6l9vNFoYd7Pp5sWV7XEe9sSScitwNlawbtTw0VupYMw6Ktt8eLF7N+/n9TUVGJjY1m8eDF79+7lnnvuwd7engcffJCFCxeyZ88eoqKimD17NhEREQwapGurO27cOIKDg7n33nuJiYlh27ZtvPDCC8ydO7epSvnRRx8lOTmZZ555hoSEBJYvX87atWtZsGBBUxwLFy7kq6++4rvvvuPcuXPMmTOHqqoqZs+ebZCfi2B8gr10Sej4fyShQ0XbVqN2X0RnACb29MThCi0mBcEUDerijIuNOaXVKg5eKGRdVCa7EvJRyKQsvbMXCrP2dSJ9zyBfAH4+no5az8nMs1m61jGiDbf+RXR1xlohI6+8jpjMUkOHI9ygH46kkVmiu3n6wE1t17JMJpXwyq0hvDAhCIkEfjiaxiM/RFGtrNfL/rVaLc/9doZqpZoB/k7cO8h420y3hiBPO96ZGgbA53sv8ldsDgAxGaVM/fwwKYVVdHKw5Nc5g+nX2elquzIqLjbmzL6pMwBLt5/X+3uH0P4o6zVEpZXw+d6LPLDqBL1e2874Tw7wysZ4NsfmUFBRh0ImpX9nR+aO7Mqq2f2JeXkcmx4fykuTgrk51BMXm/YxO9HHyYrnJwQBsGRrAskF/20zLlwqv6KW6IxSoP0moQGGBeiqXPefLzRwJMbvYFIhk5cd4tHVp7hYUIWDlZz/jQ9i76IR3DXA9z+Jj9BO9qx9JILP7u6Nl70FWaU1zPvpNNO/ONp0/8nYxGWX8fWBFABevy0UGxNvQe/lYMnbU8IBWLb3AseSi/S27+V7L/LF/mQA3p4SxgRRJCG0kIe9Ba9PDgXg090XONMK19dVVVW4urri6upKVVWV3vdvaHX1av73eywAdw/wpY+vaIV/PSQSCW/eHkqIlx3FVUrmrI6iVqVu9uMPJBXwzUHde8eSaeG42raP82fh2gx6lpCfn899991HTk4O9vb2hIeHs23bNsaOHQvAhx9+iFQqZerUqdTV1REZGcny5cubHi+Tydi0aRNz5swhIiICa2trZs2axWuvvda0jb+/P5s3b2bBggV8/PHHeHt78/XXXxMZGdm0zfTp0ykoKOCll14iNzeXXr16sXXrVtzd2+8FhNAyXV1tkMskVNTWsytBV3kf6iWSFsZsap9OdHG1Jki0ChPaGZlUwoQwD747ksY3B1KabnotHNedHh62hg2uFdwS6slrG+PJLqtlT0I+Y4L1994c27ioqB3MzzY2FnIZIwLd2Hwmh21xefQWF3kmq7xWxWcNK+4XjA3AUtG2XQMkEgkPDe1CJwdL5q+JZldCPtO/OMo3s/rhdoMt+n8+nsGhC0VYyKUsmRreIdvO3tarE2cyy/jmYApPrYsht7yWJVsTqVGpCfGyY+X9/W/452wI/ze0K98fSSMxr4JNZ7K5rVcnQ4ckGJFqZT2n00s5llLMiZRiTmeUUKu6tMuCtUJGHz9HBvo70b+zEz19HEyiPa4+3D3Al79iczl4oZCn18Ww7tHB/2nXL/xt9znd/YFwb3s8TGyeb0sM665LQh9PKaZaWY+VwrSTjq0hOqOUJVsTOHxRl8C0Ush4aIg/Dw3rgp3F1VtWSiQSJoZ7MTrQnS/3J/P5vgscTy1m0mcHmd7Ph6cjexjNYhe1Rsvi32JRa7SMD/PQ6/WZIU0I92RPojfrozJZuDaGLU8Oxd7yxsbP/Hgsnfe26brNvDAhiOn9ffURqtAB3drTi21xuWw+k8OCNdFsfmKo3s9LCgvb7yKjFXuTSS6owsXGnGduDjR0OCbNQi5jxUzdaKczmWW89MfZZlU0l1QpeXpdDAD3DPRldDteuCf8l0Rr7P1dTER5eTn29vaUlZVhZyeSXu3R+I8PEJ/z98D1fYtG4OdsbcCI2heVSsWWLVsYP368mN8sCNdwMrWYaSuONH3c18+RtY9EtNsbhG9tOceX+5MZ2cOVlbMH6GWfWq2Wfm/spKhKyYbHBoskaSv4MyabJ34+TRdXa3Y/NcLQ4QjXacnWBJbvvUg3Nxu2PjkUMwO2LYtKK+Hh709SXKWkk4MlK2f3p7v79S2+ySqtIfLD/VTW1fPChCAeGtpFz9Gajnq1hpnfHONocnHT54Z1d2X5PX1MurLps91JvL/9PJ2drdixcLhoudfBlVWr+OFoKjvP5XM2q4z6f1XIO1rJ6d/ZiQH+uv+CPe0MerwztH8eI58fH8j/Detq6JCM1oOrTrArIZ+nxnbn8dHtZyzOv2m1Woa8u4es0hpW3t+fkYGinXCjpLwK3t+eyLY4XVt2hUzK3QN9mTeq23UnjrNLa3h3awJ/RGcDYGtuxhOjA5g1uLPBO199czCF1zfFY2thxq6Fw01ysdqVVNbVM/7jA6QXVzOppxefzOh1Xa1iVSoVr373F6sv6JKET4zqxsJxptuyXDAOJVVKxn20n4KKOh4c4s+LE4P1tu+qqipsbGwAqKysxNq6/dzvTi6o5OaPD6Cs1/DJXb25taeXoUNqFw4kFTDr2+NotPDW7WHcPfDKi2y0Wi2P/XiKv87m0sXVms2PD23zxe2C/rUkH9pxr6oEoYUa50ID2FqY4dtBZgYKgmB8+vg64tVQaWEpl/HBHT3bbQIa4K4BupPZvecLyCiu1ss+c8trKapSIpNKLjm+C/ozsocrCpmU5IIqLuRXGDoc4TrkltXy7SFdu6xnbw40eEKmr58jGx4bjL+LNVmlNUz9/DCHL7Z8xb5Wq6vgqayrp4+vA7PbsMW4MTKTSfns7j5N7yvT+nrzzax+Jp2ABph9kz/O1gpSi6r5NSrT0OEIBlJUWceSrQnc9O5u3t9+nuiMUuo1WrzsLbitlxdv3h7KjgXDOPXiWL68rx8PDe1CuLeDwY93htbJwZIXJ+racr+//TxJeeJ9/HKqlfUcvKB7H2ov1aBXIpFImqqh94m50ABkllTz9LoYIj/az7a4PKQS3Xvo7qeH88qtITdUuezlYMnHM3qz/tEIwjrZU1FXz5tbzhH50X52ncsz2LzozJJqPtiuq+xdfEtQu0pAA9iYm/HRjF7IpBI2xmTze3TWde1n17l8frqgex+5f3BnFoztrs8whQ7K0VrBkqm6tvHfHEy5ruugjkar1fLC72dR1msY1t2VSaIdvt4MDXDl6Ujd4pqX/zzLqfSSK267PiqTv87mYiaV8PH03iIB3QF17CsrQWiBxrnQACFedte1GlIQBEEfpFIJMyP8kEjg5UnBdHZpP6tUL8ffxZoh3VzQauGXE+l62Wdspq4Vd4CbTYdpr9nWbC3kDO7mDNBUGSKYlo92nqdWpaGfnyNjgoyj4snP2Zrf5gymn58jFbX1zPr2OL+dalmCcV1UJvvPF6Awk7JkWvtexNNcLjbm/Pn4EH56aCDvTQtvF1XD1uZmzBmhq978ZFcSdfXNn1cmmL788lre3BzPkHf3sHzvRSrr6gn0sGXJ1HAOPjuSw4tH8/GM3twz0I8Ad1txbXcZd/bzYUQPV5T1Gp5eF0O9WnPtB3UwB5MKqavX0MnBksB2OBbn34Z3dwFgf1LHTkIXVtbxyp9xjHp/H+ujMtFoITLEnW3zh/H+HT3xdtRfwUK/zk78MfcmlkwLx8XGnJTCKh787iSzVp5o80WeWq2Wl/6Io1qppn9nR2b092nT528rfXwdebKhq8GLv8e1eBH04QuFPLH2DBok3N7Lk5cmBov3GEFvRga6NS3SX7TuDBW1KgNHZNx+j87i8MUizM2kvHFbqHgt6tmc4V25JdQDlVrLY6tPUVBR959t0oqqeOXPOAAWjO1OmBiH1yGZ/t0FQWgjQZ5/X1SKedCCIBjanOFdiX5xHDMGdIy5UjMH6b7PNScyUNbf+E3Qs9m68QqhncTxvDWNC/YAYFtcroEjEVoqKa+CtSczAFg8PtCoLtgdrRWsfmggE8I9Uam1LFwbwye7kppVFZRbVsvrm+IBWDCmO93cbFo7XJPhYmPO4G4uRvW7vlEzB/nhYWdBdlktPx3TzyImwbhll9bw8h9nGbJkD18dSKFGpSaskz1f3tuXLU8M5c7+PnpNELVnEomEd6aEY2thRkxmGV/sTzZ0SEZnR7xukd3YYPd2dey8ksHdXJBJJSQXVOmtO5EpKa9VsXR7IsOW7GHV4VSUag2Duzqz4bHBfHFvPwKuc0TItUilEu7s58Oep4fz6PCuKGRS9p8vIPKjA7y6MY6y6rZJQm2OzWF3Qj4KmZS3p4QhbceL+B4b0ZV+fo5U1tWzYE10sxfhnE4v4aHvT6Ks1xDmqOGtySHt+uckGMYLE4LwdbIiq7SG1zbGGzoco1VareSNTecAeGJ0AL7O4vxP3yQSCe/d0ZOurtbkltcy76dTqP5xvKxXa1iwJpoqpZoBnZ14dLgY79JRiSS0IDRT8D/atYqkhSAIhiaRSLC36jjz00cHueNma05hpZLt8Tee0DybpauEDhPH81aluykLZzLLyC6tMXQ4Qgss2ZbYVNnT18/J0OH8h4VcxqczevPIcN0s56U7zvPM+jOXXPT+m1ar5X8bYqmoraentz0PD+3Ybbg7Agu5jMdHdwNg2Z4LVCvrDRyR0FoyiqtZ/Fssw9/bw3dH0lDWa+jj68DK2f35c95NjAvxEImA6+Bhb8Erk0IAXXeMhNxyA0dkPNQaLbsT8gHd+U5HYGchp4+vA9CxqqFrVWq+2p/MsCV7+GT3BaqVasK97Vn94EB+engQvX0d2yQOWws5z90SyPYFwxgb7I5ao2XloVRGvL+HH46mtWq3grJqFa/8qUt2PTayK93c2nflv5lMyofTe2FrbsbJtBKW7714zcck5lZw/8oTVCvVDO7qxP3dNR1+tIPQOqzNzfjgzp5IJLoOT40LooRLvfNXAkVVSgLcbHh4aBdDh9Nu2Zib8cW9ulFOx1KKeeevhKavLdtzkVPppdiam7F0uuhA1pGJd0NBaCYHKwUhXnYozKT069w2FxmCIAiCjlwmbWr59uPRG69mi21IQotFRa3L1dacvg035sTFsek4mVrMjvg8ZFIJiyIDDR3OFUmlEhbfEsQbk0ORNtyEmb3yBOVXaEv3R3Q2uxLykcskLJnWU9wY7CDu7OeDr5MVhZVKVh1ONXQ4gp4lF1Ty9LoYRry/l5+Pp6NSaxno78SPDw3k1zmDGdnDrUNUqLamKX06MSbIDZVay1NrY6662KcjOZ1eQlGVElsLMwb4G99irdYyLEA3F3p/B5gLXa/W8PPxdEa8t5c3t5yjtFpFV1drPr+nD3/MvYkhAS4GiauzizVf3deP1Q8OpLu7DSXVKl78/SwTPz3I4QutMyP27b/OUVhZR1dX66ZRF+2dj5MVr08OBeDjXUlXnXeaVlTFzG+OUVajorevA8vv6oWZOM0UWlH/zk78X0NidfFvZyiq/G8b5JaQSqX069ePfv36IZWa/h/v8ZRifjmh6+r11pQwFOIF2aq6udnw/h09Ad288j+iszidXsInu5MAeH1yqOhE1MGJV6AgtMDK2f3Z8sRQceAUBEEwgBkDfJFK4EhyERfyK697P/nltRRU1CGVXNrlQmgdkSGiJbcp0Wq1vLVF17bszn4+JtGueuYgP76Z1R8rhYyDFwq54/Mj/6m8z6+o5ZWNullUT4wKoEcHmN0p6MhlUuaP0c12/GJfMmU1YnZee3A+r4Infj7NmKW6maxqjZahAS6sfSSCNY9EcFM7ay1vSBKJhLemhOFgJScuu5zle65dEdgR7DinW1w3socb8g60qGlYd10S+vCFona7IEGj0bLpTDbjPtzP4t9iyS2vxcvegiXTwtk2fxi3hHkaxfFlSIALW54Yymu3heBgJScht4K7vz7GIz+cJL1If+3SjyYXNSVz3pkajrmZTG/7NnaTe3fitl5eqDVa5v8STWXdfzuq5JbVcs/XxyioqCPQw5ZV9w/A2tzMANEKHc2Csd3p4W5LYaWS5zfENms00ZVYWlpy4sQJTpw4gaWlpR6jbHvKeg3/2xALwIz+PvTv3HEWihnSzaEezB2pW6T07K9nmPfTadQaLZN6enFbLy8DRycYWsc5UxYEPXCztTCJm7GCIAjtkZeDJaMCde0Ob2S2Z2MVdDc3GywVHecmiqGMC9H9zo6lFFNSpTRwNMK1bI/P41R6KZZyGQsaEnemYGSgG2sficDN1pzEvAomLzvU1HZfq9Xy0u9xlFarCPGy49EOUsEj/O22Xp0IcLOhrEbFNwfEXFtTdjarjDmroxj34X7+jMlGo4XRgW5seGwwPzw4sENVpLYlN1sLXr1V15b7091JxGWXGTgiw9vZ0OFlTAdpxd0otJM9jlZyKurqic4oNXQ4eqXVatmbmM+kzw4y76fTJBdW4WSt4MWJwex+egR39vMxui4qZjIp90V0Zu/TI7h/cGdkUgnb4vIYs3QfS7YmXDZp2hK1KjXPNyRz7h7o2yGTOa/dFkonB0vSi6t55c+4S75WXKVk5jfHyCypobOzFd8/OKBDjcwSDMtCLmPp9J7IZbrX/YbTWYYOySh8dSCZpPxKnK0VPHeL8Xb1ao8Wju3B0AAXalUaskpr8LK34I3bQo1i4ZZgWMZ19iQIgiAIgnAV9wzyBWB9VAa1KvV17UO04m5bfs7WBHrYotZo2dUwO1EwTvVqDUu26mY4PTjEHzc7CwNH1DKhnezZMPcmurvbkF9Rx/QvjrAnMZ/NsTlsjcvFTCrhvWk9O1TFmqAjk0pYOLY7oGsRd6MtC4W2F51RyoOrTjDx04P8dVbXWePmEA82PT6Eb+7v32YzWTuyW3t6cXOIB/UaXVtuZX37rIJtjuSCSi4WVGEmlTC8oTK4o5BJJQxtaMm9L7H9tOSOSithxpdHuX/lCeKyy7ExN2P+mAD2PzOSB4f4YyE37oWrDlYKXrk1hL+eHMrQABeUag3L915k1Pt7WR+ViUZzfRWSy/dcILmgCldbc569uWMmc+wt5Xw4vRdSCayPymTTmWwAKmpVzPr2OBfyK/G0t2D1QwNxszWtc2fB9IV42TN/jO4c9+U/4v7TDaqjSSuq4pNduhbQL0wMwsFKYeCIOhaZVMInM3rj52yFQiblgzt7iYU5AiCS0IIgCIIgmJBhAa54O1pSXlvPpjM517WPs1nlAIR6iSR0WxnX0JJ7u2jJbdTWnszkYkEVjlZyHhnexdDhXJdODpasnzOYm7o5U6VU89B3J1n8q66C57GR3Qj2Ei34O6qbQz0I7WRHlVLNin2inbCpOJFazL3fHGPyskPsSshHKtElQ7cvGMaKe/uKBWVtSCKR8MbtoThZK0jIreDThjl/HdHOhlbcg7o4Y2/Z8W6uNrbk3p9k+knoxNwKHvruJFM/P8yxlGIUZlIeGuLP/mdGMn9Md2xMrK1yd3dbvn9gAF/d14/OzlbkV9Tx9LoYbl9+6KozjS/nfF4Fnze8X756a0iH/FtvNMDficdGdAPg+d9iSS6o5MHvThKbVYaTtYIfHhwoxvYJBvPIsC709nWgoq6eRetjrmvRSXV1NZ07d6Zz585UV+uvnX9b0mq1vPhHHHX1Gm7q5szkXp0MHVKH5GitYNv8Yex/ZiQRXZ0NHY5gJEQSWhAEQRAEkyGTSrhrgK4a+sdjade1j8YWvWHe4sZ1W4lsaMm9P6mAGuX1VbALrataWc9HO88D8PioAGwtTPdGo52FnJX3D2BaX2/UGi0VdfX0cLdl3shuhg5NMCCJRMJT43oA8N2RNHLLag0ckXAlWq2WQxcKmf7FEe5YcYQDSYXIpBKm9fVm58LhfHJXb7q7i7nuhuBiY87rt4UCsHzvRc5klho2IAPZGa/r7DImyM3AkRjGsAAXQNddqNhER62kF1WzYE00N3+8n53n8pBKYHo/H/Y+PYIXJgbjZG261XMSiYSxwe5sWzCMxbcEYmNuRkxmGVOWH2bBmuhmvf9pNFoW/xaLSq1lTJAbt4R6tEHkxu3JMQH09LanvLaeWz4+wPGUYmzNzfj+gQFibJ9gUGYyKUvv7IWFXMqhC0X8cLTl90m0Wi1paWmkpaXd0GxpQ9p4Jof95wtQmEl5Y3KYaAFtQBZyGR72ojOE8DeRhBYEQRAEwaTc2c8HuUzC6fTSFs8kLKioI7e8FokEgj1FRWRbCfa0w9vRklqVhn3nTb9qpj369mAK+RV1+DhZNrW9N2UKMynvTQvnuVsC6ePrwEczeqEwE5c+Hd2I7q7083NEWa/p0FWcxkqr1bInMZ+pnx/mnq+PcSylGLlMt/hs79MjeP+OnnRxFTf6DW1CuCcTwz1RN7TlrqvvWIvLiquUnEwrBjrePOhGbnYWBHrYotXCAROrhs6vqOWlP84yeuleNpzOQquF8WEebF8wnHenhePlYGnoEPXG3EzGI8O7svvp4dzZzxuJBDaczmLk+3v5dFfSVUcb/XQ8nai0EqwVMl4T8zwBkMukfDSjN1YKGXX1GizkUr65v7/oyCEYBX8Xa54fHwTA23+d42JBpYEjaltlNSpe2xgPwNwR3fB3sTZwRIIg/JO4EyMIgiAIgklxtTUnsqG984/H0lv02LMNSesuLtZYm1h7PVMmkUgYFyxachur4iolK/YlA/D0uB6Ymxn33MPmkkgkPDq8K789dhNBYtGJgO5v4ulIXTX0mhMZpBeZZrvB9kaj0bI9Lpfblh1i9soTnEovRWEmZVaEH/sWjeTtKWH4OIk2p8bktdtCcbFRkJRfyUc7O9aCjt0J+Wi0EORp16Hb7w7v0dCS+3yhgSNpnrIaFUu2JjB8yV6+P5KGSq1laIALG+cNYfk9fdt1JaubrQVLpvXkz7lD6OfnSI1KzQc7zjP6g31sic35T9Vjblkt7/6VAMCiyB7tKjF/o/xdrPlwei96+zrw5b39GODvZOiQBKHJzIF+DA1woValYeHaGOrVGkOH1GaWbE2gsLKOLq7WPDrCNMdKCUJ7JpLQgiAIgiCYnHsG+gHwx+ksKuvqm/24s5kNrbjFivU219iSe1dCPioTvCDOKq1h5aEUk207eTWf7k6isq6e0E52TAr3MnQ4gtCqBnVxZmiAC/UaLR/tOm/ocDo0tUbLpjPZjP/kAP/3QxRnMsuwlMt4eKg/B58Zyau3hYrkh5Fyslbw5u1hAHyx72KLZ82ash3xusV0YztoK+5GwwP+ngttzK1b6+rVfH0gmWFL9rB870VqVGp6+Tjw08MD+eHBgR1qPE+Ytz3rHo3gk7t642lvQVZpDY/9eIoZXx69pLvUK3/GUVFXT08fB+6N6Gy4gI1UZIgHGx67qWk2uiAYC6lUwpJp4dhamBGTUcrney8aOqQ2EZVWwk/HdcUJb04OazcLqgWhPRFJaEEQBEEQTM6gLk50dbWmSqnm99NZzX5cYyW0aJvW9vp1dsLZWkFZjYrjKcWGDqdFjiUXMfGTA7y6MZ5pnx8mu7TG0CHpTXpRNasb5oY9d3MQUqlotyi0f42zoX8/ncWF/AoDR9Px1Ks1bDidybgP9zHvp9Mk5FZgY27GYyO6cvDZkfxvQjBudmKOnLGLDPHg9t6d0Gjh6XUxV23t217UqtRNlb9jgzv2jNy+nR2xlMsoqKjjXI7xHUc1Gi1/xmQzZuk+3th8jrIaFQFuNnx5b182PDaYwV1dDB2iQUgkEm7t6cXup0Ywf0wAFnIpx1KKmfjpQRb/FsuaE+lsjcvFTCrhnSlhyMR5oSCYFE97S16/LRSAj3clcTarZePLTI1KreF/G2LRamFaX28iujobOiRBEC5DJKEFQRAEQTA5EomkqRp69dG0ZldgnM0qB0QS2hBkUgljgnTV0NtMqCX3mhPpzPzmGCXVKiQSSC6s4o4VR0huJ3O2PtiR2NSSckhAx7whK3Q8vXwcGBvsjkYLS3eIaui2oqzXsOZEOqOX7mPBmhguFlRhZ2HGk6MDOPjsSJ65ORBnG3NDhym0wMuTgnGzNSe5oIoPticaOpxWd+RiETUqNe525oR26thjHszNZE03+/cb2VzoIxeLmLz8EE/8fJqM4hrcbM15d2oYfz05lHEhHmK+MWCpkDF/THd2PTWCST290Grh5+PpPPtrLAD/N6yLGGUiCCbqtl5e3BLqQb1Gy8K10e16kdi3B1NIyK3A0UreNBNbEATjI5LQgiAIgiCYpKl9vDE3k5KQW8Gp9NJrbl9cpSSroYI1xEvcVDGEyFBdEnp7XB4ajfG2bgRdm9g3NsXz7K+xqNRaJoR5smPBcLq4WJNVWsOdXxwhPrvc0GHekLNZZfwRnQ3AszcHGjgaQWhbT43rjkQCW2Jz232ViKHVqtT8cDSNke/v5dlfY0krqsbJWsGiyB4cem4UC8Z2x8FKYegwhevgYKXg7Sm6ttxfH0zhZKppdTppqe3xeQCMCXIXiUxgePfGudDGkYROyqvgwVUnuOuro5zJLMNaIeOpsd3Zu2gE0/v7YiYTt0D/rZODJZ/e1Zt1j0Y0Lazo7GzFE6MDDByZIAjXSyKR8MbkUFxszDmfV9msBZcSiYTg4GCCg4NN5v0to7iaD3fqvrfnxwfhZC3OJQXBWIkzMEEQBEEQTJK9lZxJPXXza388lnbN7RuTDP4u1thayFs1NuHyBnd1wVohI7e8ljNGnPQpr1Xx4Hcn+PpgCgDzxwTw2d296eZmw9pHIwj2tKOwUsmML48QlWa6N9zf+SsBgMm9vER3AKHDCfSw49aG95COUMFpCDVKNd8cTGH4e3t48fezZJXW4GJjzv/GB3Hw2ZHMHdlNvB+3A6OD3JnW1xttQ1vuamW9oUNqFUWVdexoSEKPDXY3cDTGoXEm7onUYqrqDPd7zy+vZfFvZ4j8aD+7EvKRSSXcO8iPvYtG8vjoAKwUZgaLzVT07+zEn3OH8NNDA1k/ZzAWcjFTVRBMmbONOe80LBL76kAyx5KLrrq9lZUVcXFxxMXFYWVl1RYh3hCtVstLf5ylVqVhoL8T0/p6GzokQRCuQiShBUEQBEEwWfcM9AVg05kcSquVV902NkvMgzY0C7mMET3cANhupC2504qqmLL8MHsTCzA3k/LZ3b2ZP6Z704pwFxtzfv6/QfTzc6S8tp6ZXx/ngJG1oWyOA0kFHLxQiEImbZqPKwgdzYIx3ZFJJexJLDDpBSXGoq5eTWphFYcuFLJszwWGLtnN65viySuvw9PegldvDeHgsyN5eFgXkRRqZ16cGIynvQWpRdUs2dr+FnVEpZUw8dODFFbW4WJjLmZONujsbIWPkyUqtZaj10hwtIbKunqW7jjP8Pf28vPxDDRaiAxxZ/uCYbw+ORRXW9HevyWkUgmDu7ngIsYiCEK7MCbYnen9fNBq4al1MVQacLGQvv11Npc9iQXIZRLevD3MZKq3BaGjEld+giAIgiCYrF4+DoR42RGXXc76qEweGtrlits2VkKHdfAZfoY2LsSdzbE5bIvL5RkjawF9NLmIOaujKKlW4W5nzlf39SPc2+E/29lbyvn+wQE88kMUB5IKeXDVST65qxc3h3q2fdDXQaPRNlVBzxzkh4+T8a92F4TW0NnFmjv6evPLiQze25bIzw8PEjexrqJaWU9WSQ2ZpTW6/5fUkFVaQ1ZJNVmlNeRX1KH916QFb0dLHhvRjal9O2FuJirr2it7SznvTA1n1rfHWXU4lZtDPRjUxfQTtVqtllWHU3lz8znqNVq6uFjz+cy+4m+5gUQiYViAKz8eS2f/+QJGB7VNhbhKrWHNiQw+2plEYWUdAH18HXh+fBD9Oju1SQyCIAim4IWJQRy6WEhmSQ1vbIrnnanhhg7phpXXqnjlzzgA5gzvSjc3GwNHJAjCtRi0Evrtt9+mf//+2Nra4ubmxuTJk0lMvHTVbG1tLXPnzsXZ2RkbGxumTp1KXl7eJdukp6czYcIErKyscHNzY9GiRdTXX7q6Z+/evfTp0wdzc3O6devGqlWr/hPPsmXL6Ny5MxYWFgwcOJDjx4/r/XsWBEEQBEF/JBIJ9wz0A+CnY+lo/333+x/OZjdUQnuJSmhDGhnohlwm4WJBFRfyKw0dTpOfj6cz8+tjlFSrCPe25895Qy6bgG5kpTDj61n9uCXUA6Vaw2M/nmLdyYy2C/gGbDyTTVx2ObbmZswb1c3Q4QiCQT0+OgCFTMrR5GIOXWj7Sj5jUlajIi67jO1xuaw8lMIbm+J59IcoJn16kD6v7yD4pW2M/XA/s1ee4IXfz7Ji30U2xmRzKr2UvHJdAtpCLqWrqzUjerjy3rRw9jw9grsH+oqkXQcwvLsrdw3wAWDR+hiDtmfWh6q6eh7/+TSvboynXqNlfJgHf8y7iR4etoYOzag0zYVOKmz159JqtWyLyyXyo/288PtZCivr6Oxsxef39OHXOYNFAloQBOFfbC3kvH9HTyQS+OVEBrsT8i67XXV1NSEhIYSEhFBdXd3GUbbMB9sSya/QHf8fGymuZQXBFBi0Enrfvn3MnTuX/v37U19fz/PPP8+4ceOIj4/H2toagAULFrB582bWrVuHvb098+bNY8qUKRw6dAgAtVrNhAkT8PDw4PDhw+Tk5HDfffchl8t56623AEhJSWHChAk8+uij/Pjjj+zatYuHHnoIT09PIiMjAVizZg0LFy5kxYoVDBw4kI8++ojIyEgSExNxc3MzzA9IEARBEIRrurWXF29tOUdyYRVHLhYxuJvLf7YprVaSUVwDQIhox21QdhZyBnd1Yd/5ArbF5dLNzbAXjvVqDW9tSeDbQ7r5zxPDPXlvWk8sFddOmJibyfj0rt4s/i2WdVGZLFp/horaeh4Y4t/aYV+3uno1723TLfp8dERXnKwVBo5IEAyrk4Mldw/0ZdXhVN7bnshN3ZzbZTW0VqulqEqpq14uqSGrtPpf1cw1VDQjaWhrYUYnB0u8HS3xdrSik4MlnRwtmz7nZK1olz8/oXmeHx/E/vOFZBTX8PZf53hjcpihQ7ouSXkVPLo6iosFVZhJJSweH8QDN3UWf9uXEdHVGTOphJTCKtKLqvF1bp3uKqfSS3h7yzlOpJYA4GSt4MnRAdw1wBeFmZg0KAiCcCWDujjz4E3+fH0whWfWx7J9geN/rgG1Wi3x8fFN/zZWMRmlfH80DYA3JoeJ+fWCYCIMmoTeunXrJR+vWrUKNzc3oqKiGDZsGGVlZXzzzTf89NNPjBo1CoCVK1cSFBTE0aNHGTRoENu3byc+Pp6dO3fi7u5Or169eP3113n22Wd55ZVXUCgUrFixAn9/fz744AMAgoKCOHjwIB9++GFTEnrp0qU8/PDDzJ49G4AVK1awefNmvv32W5577rk2/KkIgiAIgtASNuZmTO7txeqj6fx4LP2ySei47HIA/JytsLeUt3WIwr+MC3Fn3/kCtsflMteAq5fLa1U8/tNp9p3XzXReOLY7j4/q1qKbzGYyKe9ODcfOUs43B1N4bVM8FbX1PDG6ZftpK6uPppNZUoO7nTkP3GS8yXJBaEtzR3ZjzYkMYjJK2Xkun7HBbdNSVp/UGi35FbWXJJYzS2rIbGiVnV1aQ61Kc839OFsrmpLKjYnlTv9INov3UOFqbC3kLJkWzj1fH2P10XRuDvFkSMB/z8uM2R/RWSz+LZZqpRp3O3OW3d1HVNheha2FnD5+jhxPKWZfUgH3Ovvpdf+phVW8ty2RzbE5AJibSXloqD+PDO+KnYU4HgmCIDTH05E92He+gKT8Sl74PZZld/cxymvVq6lXa1j8WyxaLdzeu5PJnV8IQkdmVDOhy8p0bTKdnHQn+FFRUahUKsaMGdO0TWBgIL6+vhw5coRBgwZx5MgRwsLCcHf/+0ZBZGQkc+bMIS4ujt69e3PkyJFL9tG4zfz58wFQKpVERUWxePHipq9LpVLGjBnDkSNHWuvbFQRBEARBT+4Z6Mfqo+lsi8slv6IWN1uLS74emyVacRuTscHuvPD7WWIyy8gpq8HT3rLNY0gtrOLB705wsaAKC7mUpXf2YnzY9c10lkolvDAhCHtLOUt3nOfDnecpq1HxwoQgpFLjubgvr1Xx2e4kABaM6d6sam9B6Ahcbc25/6bOfL73Ih9sT2R0oJtRvXav5EJ+JV/uv8iR5CJySmup11y9ckUiAXdbi7+TzI4NSeaGZLOXgyVWCqO6RSCYoJu6uXDvID9+OJrGs7+eYev8odiaQLJQWa/hzc3xfHdEV2E1uKszn9zVGxcbcwNHZvyGd3fleEox+88XcO8g/SShi6uUfLIriR+PpaFSa5FIYFofbxaO626Q80ZBEARTZiGXsfTOXty+/BBbYnP5Myab23p1MnRYLbLqcCrxOeXYW8r534QgQ4cjCEILGM0VpkajYf78+dx0002EhoYCkJubi0KhwMHB4ZJt3d3dyc3Nbdrmnwnoxq83fu1q25SXl1NTU0NJSQlqtfqy2yQkJFw23rq6Ourq6po+Li/XVVipVCpUKlVLvnVBEKDpdSNeP4IgXI9uLpb08XXgVHopPx9N47ERXS75+pkMXeu+IA8bcZwxAo4WMnr76H5fW2OzmTnQt02f/2hyMY//EkNpjQp3O3NW3N2b0E52N/y3MWdYZ6zkEt7Yksi3h1Ioq6njjVuDMZMZR5vIz3cnUVKtoouLNbeFu4vXgiD8wwMRvvxwJI2E3Ar+OJ3BxPDrW5TSFmKzylixP4Ud5/L5Z8dEM6kED3sLOjlY4OVgSSf7xoSz7mNPO4trtK3ViuOCoBdPjenKnsR8MktqeGNTHG/cFmLokK4qp6yWx3+JISZTt2hxznB/nhzVDZlUIl4TzXBTF0feAw5fLKSqpu6G2mPXqtR8dySdFftTqGwYETAswJlF47oT2DCPW/xOBGMn7m8JxijQ3Yq5I7rw8e6LvPj7Wfr42OFhp1u8/8+/VWPMbeSU1bJ0x3kAFo0LwN5canQxCkJH05LXoNEkoefOncvZs2c5ePCgoUNplrfffptXX331P5/fvn07VlatMwNHEDqCHTt2GDoEQRBMVLBCwilkrDqQhG9VAv8sYjt+QQZIqMpMYMuWcwaLUfibj0T3+/r5QDxORWfb7HkP50lYlyJFo5Xga63loYAq0mMOkh6jn/27Avd0lfDTRSm/nsrmQmom9wVoMPS4wtI6+Dpa9zoY5VzO9m1br/kYQehohrlJ2JIh4+2NZ9BmnEZmRMXQWi2cL5ewM0vC+bK/DyhhjhqGeGhxt9RirwCpRAVU6L5YB+RCSS6UAHGGCFzosG73hE9LzFhzMgvHynSCHI1zxmRCqYTvk6RU1UuwlGmZGaAhUJnEtq1Jhg7NZGi0YGMmo7JOzYp1W+l2HY2HNFo4USBhS4aUUqXu4OttreVWXw09HPJIPpVHsp7jFoTWJu5vCcbGTwt+NjLSKut5+Mu9PBqkQSKB2trapm22bduGhYXFVfbS9r5OkFKtlOJvq8U67wxbtpwxdEiC0OFVV1c3e1ujSELPmzePTZs2sX//fry9vZs+7+HhgVKppLS09JJq6Ly8PDw8PJq2OX78+CX7y8vLa/pa4/8bP/fPbezs7LC0tEQmkyGTyS67TeM+/m3x4sUsXLiw6ePy8nJ8fHwYN24cdnZ2LfwJCIKgUqnYsWMHY8eORS43/nZtgiAYn9EqNZve209JjQrrbv0Z2cMVgIpaFYVH9gAwe/JoHK0UhgxTaBBSVM0fHx3kYoWMwSNG42DVusf+erWGt7eeZ01yOgATwjx45/YQLOT6b0k9HoiIy2PBujPEFEvZUOTKsrt6GrTN7Qt/xKHSZNHH14Fn7ulvcjPABKEtDKur58jSA+RXq6j16MkdfQ3fplCj0bIzIZ8vDqRwJlPXfUsmlTApzIP/G+pPgLuNgSMUhCsr25zA90fT2ZBtxcNTBmNnRDPFNRoty/Yls+LoRbRaCPGy5dMZPfFxFEUF12NPdSx/nslB5RLA+LEBLXrsgaRClmw7T0JeJQBe9hYsHNONSeGeJjEaQRD+TdzfEoxZ8IAqblt+hIQyKHUN4Z4BPlRVVTV9PTIyEmtrawNGeKkd8fnEHonGTCph2azB4txXEIxEY2fo5jBoElqr1fL444+zYcMG9u7di7+//yVf79u3L3K5nF27djF16lQAEhMTSU9PJyIiAoCIiAjefPNN8vPzcXNzA3Qrzezs7AgODm7aZsuWLZfse8eOHU37UCgU9O3bl127djF58mRA1x58165dzJs377Kxm5ubY27+39lAcrlcnGAIwg0QryFBEK6XXC7njn7efHUghTUnsxgX6gVAQrqutaK3oyVu9sZzMdXRdfOwp4e7LYl5FRy4WMyUPt7XftB1KqtR8fjPMew/XwDAU2O7M29Ut1ZNxE7s5Y2DtQUPf3+SgxeKeOD703x7f3/sDXAD/kJ+BeuisgD434QgFAqxEEMQLsdRLuexEd14c8s5lu1NZmo/H8zNDDM7XaXW8Ed0Niv2XeRCvi4xY24mZUZ/Hx4a2gUfJ5EoE4zfon6iGgAALolJREFU4vHB7E8qJLWomre3JfH+HT0NHRIAJVVK5q+JZl/DecFdA3x5eVJwqyxM6yhGBLrx55kcDl4s4rnxwc16TFx2Ge/8lcCBpEIAbC3MmDeyG7MGdxa/C6FdEPe3BGMU6OXAc7cE8urGeN7dep7hPdxxt1Lg5+cH6PIkxvJ3W1lXz+tbdKNS/29YF4K9HQ0ckSAIjVpynDBoY8C5c+eyevVqfvrpJ2xtbcnNzSU3N5eamhoA7O3tefDBB1m4cCF79uwhKiqK2bNnExERwaBBgwAYN24cwcHB3HvvvcTExLBt2zZeeOEF5s6d25QkfvTRR0lOTuaZZ54hISGB5cuXs3btWhYsWNAUy8KFC/nqq6/47rvvOHfuHHPmzKGqqorZs2e3/Q9GEARBEITrctcA3Wzh3Yn5ZJboWsPEZelW54V6XUdvQKFVRYa4A7AtLrfVniOlsIrblx9i//kCLOUyPr+nD4+PDmiTSuAhAS6sfmggdhZmRKWVcNeXRymoqGv15/23JVsT0WhhXLA7ff2c2vz5BcGU3Bvhh7udOVmlNfxyPKPNn79GqWbVoRRGvLeXp9fFcCG/ElsLM+aO7Mqh50bx6m2hIgEtmAxLhYz37+iJRALrozLZdS7v2g9qZdEZpUz89CD7zhdgbibl/Tt68vaUMJH0vEFDA3QdiM5mlVNYefVznazSGhaujWbipwc5kFSIXCbhwSH+7F80kkeGdxW/C0EQhFY2K6Izg7s6U6NS89TaaBTmFqSmppKammpUY0Y/3HGenLJafJwseXxUy7psCIJgPAyahP78888pKytjxIgReHp6Nv23Zs2apm0+/PBDJk6cyNSpUxk2bBgeHh789ttvTV+XyWRs2rQJmUxGREQEM2fO5L777uO1115r2sbf35/NmzezY8cOevbsyQcffMDXX39NZGRk0zbTp0/n/fff56WXXqJXr15ER0ezdetW3N3d2+aHIQiCIAjCDeviasNN3ZzRamlKHsRm6Sqhw7xFEtrYjAvRjT3Zd76AGqVa7/s/fKGQycsOkVxQhae9BesejeCWME+9P8/V9PVz5Jf/i8DFRkF8Tjl3fnGErNKaNnv+k6nFbI/PQyqBZ27u0WbPKwimykIuY17DTa7P9lxolWPT5ZTVqPhsdxJD3t3NKxvjySqtwcXGnGdvDuTQc6NYFBmIi81/O3EJgrHr19mJh4bout4991sspdVKg8Sh1Wr54Ugqd6w4TFZpDZ2drfh97k1M69t6nVg6Eldbc0K8dKPpDjZUNv9bWY2Kd/5KYOT7e/ntVBZaLUzq6cXup0bw4sRgHK1FpxZBEIS2IJVKeO+Ontiam3EqvZQv9icbOqT/OJtVxspDKQC8flsolgqxQEkQTJVEq9VqDR1Ee1BeXo69vT1lZWViJrQgXAeVSsWWLVsYP3680bR9EQTBNG2JzeGxH0/hYmPOkcWjiPxwP8mFVXz3wACGd3c1dHjCP2i1Woa8u4es0hq+uLcvkQ1JaX1YfTSNV/6Mo16jpZePA1/e2xc3Owu97b+lUgqrmPn1MbJKa/Cyt+CHhwbS1bV151lptVqmrTiiq8Ie4MPbU8Jb9fkEob1Q1msY9cFeMktqeO6WQB4d3rXVniu/vJZvDqXw49F0KuvqAfBxsuT/hnXljr7eoiJQaBdqVWomfHKAiwVVTO7lxUczerfp81cr63n+t1h+j84GdJ1Y3rujJ3YW4rpTn97dmsDney9ye+9OfDi9V9PnlfUaVh9N49PdSZRUqwAY6O/E8+OD6OnjYJhgBaEViftbgqn4NSqTp9bFIJdJ+GPuEIK99JvT0Gi0VCrrKa9RUVH7j//X/uPjunoqalWU1+g+X16r+zivrJYqpZqJ4Z58dncfvcYlCMKNa0k+1KAzoQVBEARBEPRtbLA7rrbmFFTUseFUFsmFVQCE6vmCSrhxEomEcSHurDyUyra4XL0koevVGl7fFM93R9IAmNzLi3emhhs8kePvYs26RyOY+c0xkguquHPFEb5/cAAhrdgmfkd8HlFpJVjIpcwf073VnkcQ2huFme418/S6GFbsu8jdA331nqxKL6pmxf6LrI/KRFmvAaCHuy1zRnRlYrgnZjKDNi0TBL2ykOvack/9/DC/R2dzc6gnN4fqb+HZ1VwsqGTO6ijO51Uik0p47uZAHhrq3yZjOTqaYQGufL73IgeSCtBotEgksOlMDu9tSyS9WDcmp5ubDYtvCWRUoJv4HQiCIBjYlD6d2BaXy9aYdCIGDSDAzZYDB/ZjaWmJVqulrl6jSwzXNCSKa/9OGOs+1iWTr5RgrlTWcyPlj07WCl6aGKy/b1gQBIMQSWhBEARBENoVuUzKjP4+fLr7Av/f3p3HVVXt/x9/H+Z5UBkEUXHCeTbE2RxwoiivOeVQDplQknrLm+ZQmV8tS+1GXq2k380hzam8phmOKaFi5hCOSWgKmgoIAiKc3x/kuZ3rhHoU0Nfz8eDhOXuvvdZnn4cIrs9enzV93SFJkp+7g8pSxrRECq3jqwXbkxSbeFZX8wvuKfGSfjlPkYv3aNufZSD/Hhqkke2qlphJTj8PRy17IUQDP9upg6cz1Gfej1owuJmaVrb8Ps1X8wtMf/+HtAqUTzGuAgdKo6ca+evjzcd0/FyWPt12Qq90ssyDHIlnMvTx5uNas++0Cv6clGtSyVMj21VV+yBvWVmVjH+vAEtrVNFTL7Stqo83H9eEVfv1WGAZlbnP5Zf/s++MXv3qZ2VdyZe3q73+2a+xHgu0/M9cFGpSyVPOdtb6I/OKFsb/pq/2/K6fT6ZJKizXPbpTDfVqUoGHbACghDAYDHrn6XraefS0Tp48rISTUuf3N+lyga0ycvKUl2+ZArp2NlZyc7CVm4ONXB3//NPBRm4OtmZ/ujrYys3xv8cCyjjKlaolQKlHEhoAADx0+jxWUR9tOqbzWYX7Dtb1Zz/okqpZ5cJJ6AtZV7TzxAW1qFburvr59Vymhn6+W7/+kSVHW2t90LvhA1tldSfKuthr8fDmGhKzS7uSLurZT+P1rwFNLV4qflnCKR0/lyVPJ1u9cB9LCQMPK2srg0Z3ClLEoj369IcTGtyi8j3tV7o76YKiNx/XxkNnTcfa1vDSyHZV9VhgmRLzsAxwP0V1rK7YxFQdSc3UG6sP6KP7VF4zL79A09Ye0md/7iUZHFhGH/ZrJG9XHsi6n+xsrBRStZy+T0zVG6sPSpKc7Kz1QpuqGtYmUE52TEECQElTzsVebz5RV0++Xfj+t/PZsrL7b/LZyiC52Nv8mRy+lkC2lZvj9UnkvyaQryWVXR1sir0qGYDixW+AAADgoePv4aj2Qd6K/XOynyR0yWVtZVDHWt5auvuU1h9Muask9A9H/9DIhQnKyLkqP3cHzR/U9L6Wub5Xbg62+n/PB2vEFwnacuSchn6+S7P7NFK3euUt0n/2lXx9sOGIJCny8erseQncpa51fVW7vJt+OZOhuVuO6x/dat3R9UajUZuPnNPHm45rZ9IFSZLBIHWrV14vtq3KzyY8cuxtrDWzV0OFR2/Xf/adUde6p9Wjvp9FxziTnq3IRT8p4beLkqQRbatqbOcarL59QDrV9tb3iamytjKo72MBGtWhhrxcqUYEACVZh9o+ptdfDH1MPmU8TEllZztrHpYEcE/4LRwAADyUnm1eyfS6HhP9JVrn2oUrlr/7JVXGO9w06t9xSRq0YKcycq6qUUUPrYpsWaIT0Nc42llr/sCm6l6vvPLyjYpctEdLd520SN+fbT+hs5dyVcHTUc82r2iRPoFHkZWVQWNDC8twfx6XpNSMnCJdl19g1Dc/n1b3OT/ouQW7tDPpgmytDerTLEAbx7TTR/0ak4DGI6teBXdFtCus0PHGqgM6dynXYn1vP/aHesz5QQm/XZSrg43mD2yqcV1rkoB+gHo1CdBH/Rrru1fa6O3weiSgAaCUaVKpjIJ8XeXn4SgXexsS0ADuGSuhAQDAQ6lNDS/VLu+mM+nZalzRs7jDwS20ql5OTnbWOpOeo/2/p6t+BY/bXpOXX6A3v/lF//7xN0mF+7dOe7peqSr1ZWdjpTl9G8nVwUZLdp3Uq8v36VLuVQ1pFXjXfV7IuqK5m49LKtwT296m9HweQEnUPshbjSt6aE9ymv658ZjeCq9707a5V/O1POF3/Wvrcf12/rKkwlK0/R6rqKGtq8jXnVLAgFRYpeO7X1J1KOWSJqzar7nPNrmnSe6CAqOiNx/T+xuOqMAo1SrvprnPNlalss4WjBpFYWVlUPf6lqnsAgAAgNKPJDQAAHgoWVsZ9NWLIcovMMqVcsQlmoOttdoFeWnt/hStP5hy2yR0+uU8jVyUoO3HzstgKEy2vti2aql8StvayqBpT9crXLG17YTeWvOLMrLzFNWx+l3dzz83HtOl3Kuq4+emMAuXOAUeRQaDQWNDg9RvfryW7ErW8DZVFFDGyaxNZu5VLYr/TZ9sK6xCIEmeTrYa3CJQA0Mq3dNe0sDDyM7GSjOfaaAn/7ld6w+m6uufT+vJhv531Vfa5SsavfRn037rzzStoDefrFuqHkoDAAAAHlbUJAIAAA8tJzsbEtClRGidwpLc6w+m3rLd8XOZCo/eru3HzsvJzlr/eraJRrarVioT0NcYDAa93q2WxnYuLPs7O/aopnzziwoK7qw0+ckLl/XvH5MkSeO61pSVVen9TICSpEXVcmpZrazy8o2aHXvUdPxC1hW9/91htfy/jXpn7SGdvZSr8u4OeqNHbW0f97hGdaxOAhq4iTp+7nrp8eqSpImrD+psEcvd/9X+U+nq8eEP2njorOxtrDSjZ33N+FsDEtAAANyhcuXKqVy5csUdBoCHECuhAQAAUOza1/SWrbVBx85m6vi5TFX1crmuzbaj5xSxcI8ycq7K38NR8wc2VW0/t2KI1vIMBoMiH68uVwdbTfr6oGJ2JOlSzlVN71mvyHtZzvzusPLyjWpdvZxaV/e6zxEDj5axnYO0/dgOrdhzSuEN/RV7KFVLdp5Udl6+JKlKOWeNaFtV4Y38ZWfDs95AUYxsX1UbElN04PcMvb5yv+YPbFqkh8qMRqMW7zypyV8f1JX8AlUs46To/uy1DgDA3XB2dta5c+eKOwwADyn+dwwAAIBi5+Zgq+ZVykqS1h9MMTtnNBr1+Y4kDV6wSxk5V9WkkqdWRbR8aBLQfzWoRWXN7NVA1lYGLd9zShGL9ij3av5trzvwe7pW7T0tSXqtS837HSbwyGlU0VMda3mrwCg9+2m8FmxPUnZevur6uym6f2NtGN1WzzQLIAEN3AFbayvN7NVQttYGfZ94Viv2/H7ba7Kv5GvMsp/1+sr9upJfoI61fPTNS61IQAMAAAAlEP9DBgAAQIlwrST3d38pyZ2XX6AJqw5o0tcHlV9g1NON/bVoWLC8XO2LK8z7rmeTCoru31h21lZafzBVQz/frctXrt7ymunrDkmSnmzox0Q8cJ+M6Rwk6z/L3IdUKat/D3lM30S2Urd65U3HAdyZIF9XRXUs3I5i8jcHdSY9+6ZtT/yRpaeit2vFnt9lZSh86GregCZyd2TrFQAAAKAkIgkNAACAEqFzbR8ZDNLek2lKSc9R2uUrGvTZTi2MT5bBULjP8cxeDWRv8/Dv9Rhax1cLnmsmJztrbTv6h579JF7pl/Nu2Hbb0XPadvQP2VobNLZz0AOOFHh01CrvptURLbXmpVZaPLy5Wlf3KtX70QMlxQttqqhBgIcu5VzVuOX7ZTQar2uz7sAZPfHhDzqUcknlXOy1cGhzvdiuqqx4AAQAgHuSnZ2tdu3aqV27dsrOvvnDYABwN0hCAwAAoETwdnNQowAPSdK8rb8q/KPt2nH8vJztrDVvQFONaFv1kUr4tKxWTl8MDZabg432JKep97w4nbuUa9amoMCo//u2cBX0s80rKaCMU3GECjwy6vq7U20AsDAbayvN7FVfdjZW2nLknJbuPmk6l5dfoKn/+UUjvtijS7lX9VjlMvrPy60UUrVsMUYMAMDDo6CgQFu2bNGWLVtUUFBQ3OEAeMiQhAYAAECJca0k92fbTyjp/GX5ezhq+cgW6lTbp5gjKx6NK3pq6YgQebna61DKJfWau0OnLl42nf9m32kdPJ0hF3sbvfR49WKMFACAu1fN21VjOxeW5X5rTaJ+T8tWakaO+s3/UfO3nZAkDW9TRQuHBcvHzaE4QwUAAABQRCShAQAAUGJ0/jMJLUlNK3lqdWRL1fR1K8aIil9NXzcteyFE/h6OSjp/Wb3mxunY2UzlXs3Xe98dliSNaFtFZZztijlSAADu3pBWVdS4oocyc69q5BcJ6j7nB+1KuigXexvNfbaxXu9WS7bWTGMBAAAApYVNcQcAAAAAXBNYzlmvdglSZs5VjepY/ZHY/7koKpdz1lcvhujZT+J1/FyWnvlXnLrW9dXJC9nydrXX860CiztEAADuibWVQe/1aqCus7fp51PpkqSavq76+NkmCiznXMzRAQAAALhTPEIKAACAEmVku2p6tUtNEtD/o7y7o5a+EKK6/m66kHVFC+OTJUmvdKohJzueLQUAlH5VvFw0+Yk6srU26G9NKmjlyJYkoAEAAIBSitkqAAAAoJQo62KvRcOaa+jnu7XzxAVV9XJWryYVijssAAAspu9jFdWzcQXZ2bBuAgAAACjNSEIDAAAApYibg63+3/OP6eu9p9W6RjnZsD8mAOAhQwIaAIAHx8nJqbhDAPCQIgkNAAAAlDIOttZ6pllAcYcBAAAAACjFnJ2dlZWVVdxhAHhIFeujpVu3blVYWJj8/PxkMBi0atUqs/NGo1ETJ05U+fLl5ejoqI4dO+ro0aNmbS5cuKD+/fvLzc1NHh4eGjJkiDIzM83a7Nu3T61bt5aDg4MCAgI0Y8aM62JZtmyZatasKQcHB9WrV09r1661+P0CAAAAAAAAAAAAwMOuWJPQWVlZatCggT766KMbnp8xY4bmzJmjuXPnKj4+Xs7OzgoNDVVOTo6pTf/+/XXw4EFt2LBBa9as0datWzV8+HDT+YyMDHXu3FmVKlVSQkKC3n33XU2ePFnz5s0ztdmxY4f69u2rIUOG6KefflJ4eLjCw8N14MCB+3fzAAAAAAAAAAAAAPAQMhiNRmNxByFJBoNBK1euVHh4uKTCVdB+fn4aM2aMxo4dK0lKT0+Xj4+PYmJi1KdPHyUmJqp27dratWuXmjZtKklat26dunXrplOnTsnPz08ff/yxxo8fr5SUFNnZ2UmSxo0bp1WrVunQoUOSpN69eysrK0tr1qwxxdO8eXM1bNhQc+fOLVL8GRkZcnd3V3p6utzc3Cz1sQCPjLy8PK1du1bdunWTra1tcYcDAAAAAAAA3BHmt1Da5OTkqGfPnpKk5cuXy8HBoZgjAlDS3Uk+tMTuCX3ixAmlpKSoY8eOpmPu7u4KDg5WXFyc+vTpo7i4OHl4eJgS0JLUsWNHWVlZKT4+Xk899ZTi4uLUpk0bUwJakkJDQzV9+nRdvHhRnp6eiouL0+jRo83GDw0Nva48+F/l5uYqNzfX9D4jI0NS4S8aeXl593r7wCPn2vcN3z8AAAAAAAAojZjfQmmTk5Nj2po0JydH1tbWxRwRgJLuTn7GldgkdEpKiiTJx8fH7LiPj4/pXEpKiry9vc3O29jYqEyZMmZtAgMDr+vj2jlPT0+lpKTccpwbmTZtmqZMmXLd8e+++05OTk5FuUUAN7Bhw4biDgEAAAAAAAC4a8xvobT469an69evZyU0gNu6fPlykduW2CR0SfePf/zDbPV0RkaGAgIC1LlzZ8pxA3chLy9PGzZsUKdOnShXBAAAAAAAgFKH+S2UNllZWabXoaGhcnZ2LsZoAJQG1ypDF0WJTUL7+vpKklJTU1W+fHnT8dTUVDVs2NDU5uzZs2bXXb16VRcuXDBd7+vrq9TUVLM2197frs218zdib28ve3v7647b2tryCwZwD/geAgAAAAAAQGnG/BZKi7/+PeXvLYCiuJN/J6zuYxz3JDAwUL6+voqNjTUdy8jIUHx8vEJCQiRJISEhSktLU0JCgqnNxo0bVVBQoODgYFObrVu3mtUo37Bhg4KCguTp6Wlq89dxrrW5Ng4AAAAAAAAAAAAAoGiKNQmdmZmpvXv3au/evZKkEydOaO/evUpOTpbBYFBUVJTefvttff3119q/f78GDhwoPz8/hYeHS5Jq1aqlLl26aNiwYdq5c6e2b9+uyMhI9enTR35+fpKkfv36yc7OTkOGDNHBgwf15Zdfavbs2WaltEeNGqV169Zp5syZOnTokCZPnqzdu3crMjLyQX8kAAAAAAAAAAAAAFCqFWs57t27d6t9+/am99cSw4MGDVJMTIxeffVVZWVlafjw4UpLS1OrVq20bt06OTg4mK5ZuHChIiMj1aFDB1lZWalnz56aM2eO6by7u7u+++47RUREqEmTJipXrpwmTpyo4cOHm9q0aNFCixYt0oQJE/T666+revXqWrVqlerWrVvkezEajZLurBY6gP/Ky8vT5cuXlZGRQdkXAAAAAAAAlDrMb6G0+eue0BkZGcrPzy/GaACUBtfyoNfyordiMBalFW7r1KlTCggIKO4wAAAAAAAAAAAAAOC+OXnypCpUqHDLNiShLaSgoECnT5+Wq6urDAZDcYcDlDoZGRkKCAjQyZMn5ebmVtzhAAAAAAAAAHeE+S0AwMPOaDTq0qVL8vPzk5XVrXd9LtZy3A8TKyur22b8Adyem5sbv6QDAAAAAACg1GJ+CwDwMHN3dy9Su1unqAEAAAAAAAAAAAAAuAMkoQEAAAAAAAAAAAAAFkMSGkCJYG9vr0mTJsne3r64QwEAAAAAAADuGPNbAAD8l8FoNBqLOwgAAAAAAAAAAAAAwMOBldAAAAAAAAAAAAAAAIshCQ0AAAAAAAAAAAAAsBiS0AAAAAAAAAAAAAAAiyEJDQAAAAAAAAAAAACwGJLQwCNm2rRpatasmVxdXeXt7a3w8HAdPnzYrE1OTo4iIiJUtmxZubi4qGfPnkpNTTVr8/LLL6tJkyayt7dXw4YNbzjW0qVL1bBhQzk5OalSpUp69913ixTjsmXLVLNmTTk4OKhevXpau3btTduOGDFCBoNBs2bNum2/RYn5mmPHjsnV1VUeHh5FihkAAAAAAAAPhiXmt37++Wf17dtXAQEBcnR0VK1atTR79myzPlasWKFOnTrJy8tLbm5uCgkJ0fr1628bn9Fo1MSJE1W+fHk5OjqqY8eOOnr0qFmbqVOnqkWLFnJycrqj+ad9+/apdevWcnBwUEBAgGbMmHFdm1mzZikoKEiOjo4KCAjQK6+8opycnCKPAQCAJZCEBh4xW7ZsUUREhH788Udt2LBBeXl56ty5s7KyskxtXnnlFX3zzTdatmyZtmzZotOnT+vpp5++rq/nn39evXv3vuE43377rfr3768RI0bowIEDio6O1gcffKB//vOft4xvx44d6tu3r4YMGaKffvpJ4eHhCg8P14EDB65ru3LlSv3444/y8/Mr8v3fKuZr8vLy1LdvX7Vu3brI/QIAAAAAAODBsMT8VkJCgry9vfXFF1/o4MGDGj9+vP7xj3+YzV1t3bpVnTp10tq1a5WQkKD27dsrLCxMP/300y3jmzFjhubMmaO5c+cqPj5ezs7OCg0NNUsEX7lyRb169dKLL75Y5PvOyMhQ586dValSJSUkJOjdd9/V5MmTNW/ePFObRYsWady4cZo0aZISExP16aef6ssvv9Trr79e5HEAALAEg9FoNBZ3EACKz7lz5+Tt7a0tW7aoTZs2Sk9Pl5eXlxYtWqS//e1vkqRDhw6pVq1aiouLU/Pmzc2unzx5slatWqW9e/eaHe/Xr5/y8vK0bNky07EPP/xQM2bMUHJysgwGww3j6d27t7KysrRmzRrTsebNm6thw4aaO3eu6djvv/+u4OBgrV+/Xt27d1dUVJSioqKKdM83i/ma1157TadPn1aHDh0UFRWltLS0IvULAAAAAACAB+9e57euiYiIUGJiojZu3HjTserUqaPevXtr4sSJNzxvNBrl5+enMWPGaOzYsZKk9PR0+fj4KCYmRn369DFrHxMTU+T5p48//ljjx49XSkqK7OzsJEnjxo3TqlWrdOjQIUlSZGSkEhMTFRsba7puzJgxio+P1w8//HDbMQAAsBRWQgOPuPT0dElSmTJlJBU+BZqXl6eOHTua2tSsWVMVK1ZUXFxckfvNzc2Vg4OD2TFHR0edOnVKv/32202vi4uLMxtbkkJDQ83GLigo0IABA/T3v/9dderUKXJMRbFx40YtW7ZMH330kUX7BQAAAAAAwP1hqfmt9PR0Ux83UlBQoEuXLt2yzYkTJ5SSkmI2tru7u4KDg+9obu1G4uLi1KZNG1MCWiqcNzt8+LAuXrwoSWrRooUSEhK0c+dOSdKvv/6qtWvXqlu3bvc0NgAAd4okNPAIKygoUFRUlFq2bKm6detKkulJyv/di8bHx0cpKSlF7js0NFQrVqxQbGysCgoKdOTIEc2cOVOSdObMmZtel5KSIh8fn1uOPX36dNnY2Ojll18ucjxFcf78eQ0ePFgxMTFyc3OzaN8AAAAAAACwPEvNb+3YsUNffvmlhg8fftOx3nvvPWVmZuqZZ565aZtr/d9ufutu3Gze7K/j9uvXT2+++aZatWolW1tbVa1aVe3ataMcNwDggSMJDTzCIiIidODAAS1ZssTifQ8bNkyRkZHq0aOH7Ozs1Lx5c1O5ISsrKyUnJ8vFxcX09c477xSp34SEBM2ePVsxMTE3LendtWtXU793slJ62LBh6tevn9q0aVPkawAAAAAAAFB8LDG/deDAAT355JOaNGmSOnfufMM2ixYt0pQpU7R06VJ5e3tLkhYuXGg2v7Vt27a7juF/1alTx9Rv165di3zd5s2b9c477yg6Olp79uzRihUr9J///EdvvfWWxWIDAKAobIo7AADFIzIyUmvWrNHWrVtVoUIF03FfX19duXJFaWlpZk+LpqamytfXt8j9GwwGTZ8+Xe+8845SUlLk5eVl2oumSpUq8vT0NNuT+VoZI19fX6Wmppr19dext23bprNnz6pixYqm8/n5+RozZoxmzZqlpKQkffLJJ8rOzpYk2draFjnmjRs36uuvv9Z7770nqXAPn4KCAtnY2GjevHl6/vnni9wXAAAAAAAA7i9LzG/98ssv6tChg4YPH64JEybccJwlS5Zo6NChWrZsmVmZ7SeeeELBwcGm9/7+/qYKgKmpqSpfvrzZ2A0bNizyva1du1Z5eXmSCre4u3ZfN5o3u3ZOkt544w0NGDBAQ4cOlSTVq1dPWVlZGj58uMaPHy8rK9alAQAeDJLQwCPGaDTqpZde0sqVK7V582YFBgaanW/SpIlsbW0VGxurnj17SpIOHz6s5ORkhYSE3PF41tbW8vf3lyQtXrxYISEh8vLykiRVq1btuvYhISGKjY1VVFSU6diGDRtMYw8YMOCGe0YPGDBAzz33nCSZxrtTcXFxys/PN71fvXq1pk+frh07dtx1nwAAAAAAALAsS81vHTx4UI8//rgGDRqkqVOn3nCsxYsX6/nnn9eSJUvUvXt3s3Ourq5ydXU1OxYYGChfX1/Fxsaaks4ZGRmKj4/Xiy++WOR7rFSp0nXHQkJCNH78eOXl5ZkWXmzYsEFBQUHy9PSUJF2+fPm6RLO1tbWkws8NAIAHhSQ08IiJiIjQokWLtHr1arm6upr2i3F3d5ejo6Pc3d01ZMgQjR49WmXKlJGbm5teeuklhYSEqHnz5qZ+jh07pszMTKWkpCg7O9u0qrl27dqys7PTH3/8oa+++krt2rVTTk6OFixYoGXLlmnLli23jG/UqFFq27atZs6cqe7du2vJkiXavXu35s2bJ0kqW7asypYta3aNra2tfH19FRQUdMu+bxdzrVq1zNrv3r1bVlZWpv2EAAAAAAAAUPwsMb914MABPf744woNDdXo0aNNfVhbW5sWUCxatEiDBg3S7NmzFRwcbGpzbYwbMRgMioqK0ttvv63q1asrMDBQb7zxhvz8/BQeHm5ql5ycrAsXLig5OVn5+fmmeapq1arJxcXlhn3369dPU6ZM0ZAhQ/Taa6/pwIEDmj17tj744ANTm7CwML3//vtq1KiRgoODdezYMb3xxhsKCwszJaMBAHgQDEYefwIeKTfbR3nBggUaPHiwJCknJ0djxozR4sWLlZubq9DQUEVHR5uVK2rXrt0NE8onTpxQ5cqV9ccffygsLEz79++X0WhUSEiIpk6dalai6GaWLVumCRMmKCkpSdWrV9eMGTPUrVu3m7avXLmyoqKizFZP38jtYv5fMTExioqKUlpa2m1jBgAAAAAAwINhifmtyZMna8qUKdf1UalSJSUlJUm6+VzSoEGDFBMTc9P4jEajJk2apHnz5iktLU2tWrVSdHS0atSoYWozePBgff7559ddu2nTJrVr1+6mfe/bt08RERHatWuXypUrp5deekmvvfaa6fzVq1c1depU/fvf/9bvv/8uLy8vhYWFaerUqWalyQEAuN9IQgMAAAAAAAAAAAAALMbq9k0AAAAAAAAAAAAAACgaktAAAAAAAAAAAAAAAIshCQ0AAAAAAAAAAAAAsBiS0AAAAAAAAAAAAAAAiyEJDQAAAAAAAAAAAACwGJLQAAAAAAAAAAAAAACLIQkNAAAAAAAAAAAAALAYktAAAAAAAKBIYmJiZDAYZDAYFBUVdcu2lStX1qxZs4rUb7t27Uz97t27957jBAAAAAAUL5LQAAAAAIBHwuDBg02JTltbW/n4+KhTp0767LPPVFBQcEd9xcTEyMPDw6Lxbd68WQaDQWlpaRbt19Lc3Nx05swZvfXWWxbrc8WKFdq5c6fF+gMAAAAAFC+S0AAAAACAR0aXLl105swZJSUl6dtvv1X79u01atQo9ejRQ1evXi3u8EoFg8EgX19fubq6WqzPMmXKyMvLy2L9AQAAAACKF0loAAAAAMAjw97eXr6+vvL391fjxo31+uuva/Xq1fr2228VExNjavf++++rXr16cnZ2VkBAgEaOHKnMzExJhSuWn3vuOaWnp5tWVk+ePFmSlJubq7Fjx8rf31/Ozs4KDg7W5s2bTf3+9ttvCgsLk6enp5ydnVWnTh2tXbtWSUlJat++vSTJ09NTBoNBgwcPliStW7dOrVq1koeHh8qWLasePXro+PHjpj6TkpJkMBi0dOlStW7dWo6OjmrWrJmOHDmiXbt2qWnTpnJxcVHXrl117tw503WDBw9WeHi4pkyZIi8vL7m5uWnEiBG6cuXKHX+uZ8+eVVhYmBwdHRUYGKiFCxeanTcajZo8ebIqVqwoe3t7+fn56eWXX77jcQAAAAAApQNJaAAAAADAI+3xxx9XgwYNtGLFCtMxKysrzZkzRwcPHtTnn3+ujRs36tVXX5UktWjRQrNmzTKVpT5z5ozGjh0rSYqMjFRcXJyWLFmiffv2qVevXurSpYuOHj0qSYqIiFBubq62bt2q/fv3a/r06XJxcVFAQICWL18uSTp8+LDOnDmj2bNnS5KysrI0evRo7d69W7GxsbKystJTTz11XQnxSZMmacKECdqzZ49sbGzUr18/vfrqq5o9e7a2bdumY8eOaeLEiWbXxMbGKjExUZs3b9bixYu1YsUKTZky5Y4/w8GDB+vkyZPatGmTvvrqK0VHR+vs2bOm88uXL9cHH3ygf/3rXzp69KhWrVqlevXq3fE4AAAAAIDSwaa4AwAAAAAAoLjVrFlT+/btM72Piooyva5cubLefvttjRgxQtHR0bKzs5O7u7upLPU1ycnJWrBggZKTk+Xn5ydJGjt2rNatW6cFCxbonXfeUXJysnr27GlKwFapUsV0fZkyZSRJ3t7eZvtN9+zZ0yzWzz77TF5eXvrll19Ut25d0/GxY8cqNDRUkjRq1Cj17dtXsbGxatmypSRpyJAhZqu9JcnOzk6fffaZnJycVKdOHb355pv6+9//rrfeektWVkV7bv3IkSP69ttvtXPnTjVr1kyS9Omnn6pWrVpmn42vr686duwoW1tbVaxYUY899liR+gcAAAAAlD6shAYAAAAAPPKMRqMMBoPp/ffff68OHTrI399frq6uGjBggM6fP6/Lly/ftI/9+/crPz9fNWrUkIuLi+lry5YtpvLZL7/8st5++221bNlSkyZNMkt838zRo0fVt29fValSRW5ubqpcubKkwsTuX9WvX9/02sfHR5LMVhv7+PiYrU6WpAYNGsjJycn0PiQkRJmZmTp58uRt47omMTFRNjY2atKkielYzZo1zRLpvXr1UnZ2tqpUqaJhw4Zp5cqV7MENAAAAAA8xktAAAAAAgEdeYmKiAgMDJRXusdyjRw/Vr19fy5cvV0JCgj766CNJuuV+yZmZmbK2tlZCQoL27t1r+kpMTDSV1h46dKh+/fVXDRgwQPv371fTpk314Ycf3jK2sLAwXbhwQfPnz1d8fLzi4+NvGIutra3p9bWE+v8e+98S3g9KQECADh8+rOjoaDk6OmrkyJFq06aN8vLyiiUeAAAAAMD9RRIaAAAAAPBI27hxo/bv328qe52QkKCCggLNnDlTzZs3V40aNXT69Gmza+zs7JSfn292rFGjRsrPz9fZs2dVrVo1s6+/lu0OCAjQiBEjtGLFCo0ZM0bz58839SnJrN/z58/r8OHDmjBhgjp06KBatWrp4sWLFrv3n3/+WdnZ2ab3P/74o2mP6qKqWbOmrl69qoSEBNOxw4cPKy0tzaydo6OjwsLCNGfOHG3evFlxcXHav3//Pd8DAAAAAKDkYU9oAAAAAMAjIzc3VykpKcrPz1dqaqrWrVunadOmqUePHho4cKAkqVq1asrLy9OHH36osLAwbd++XXPnzjXrp3LlysrMzFRsbKyppHWNGjXUv39/DRw4UDNnzlSjRo107tw5xcbGqn79+urevbuioqLUtWtX1ahRQxcvXtSmTZtMeydXqlRJBoNBa9asUbdu3eTo6ChPT0+VLVtW8+bNU/ny5ZWcnKxx48ZZ7PO4cuWKhgwZogkTJigpKUmTJk1SZGRkkfeDlqSgoCB16dJFL7zwgj7++GPZ2NgoKipKjo6OpjYxMTHKz89XcHCwnJyc9MUXX8jR0VGVKlWy2L0AAAAAAEoOVkIDAAAAAB4Z69atU/ny5VW5cmV16dJFmzZt0pw5c7R69WpZW1tLKtwn+f3339f06dNVt25dLVy4UNOmTTPrp0WLFhoxYoR69+4tLy8vzZgxQ5K0YMECDRw4UGPGjFFQUJDCw8O1a9cuVaxYUVLhKueIiAjVqlVLXbp0UY0aNRQdHS1J8vf315QpUzRu3Dj5+PiYksFLlixRQkKC6tatq1deeUXvvvuuxT6PDh06qHr16mrTpo169+6tJ554QpMnT77jfhYsWCA/Pz+1bdtWTz/9tIYPHy5vb2/TeQ8PD82fP18tW7ZU/fr19f333+ubb75R2bJlLXYvAAAAAICSw2A0Go3FHQQAAAAAAHiwBg8erLS0NK1atarI18TExCgqKuq6UtuWkJSUpMDAQP30009q2LChxfsHAAAAADw4rIQGAAAAAABFlp6eLhcXF7322msW67Nr166qU6eOxfoDAAAAABQv9oQGAAAAAABF0rNnT7Vq1UpSYYltS/nkk0+UnZ0tSabS5QAAAACA0oty3AAAAAAAAAAAAAAAi6EcNwAAAAAAAAAAAADAYkhCAwAAAAAAAAAAAAAshiQ0AAAAAAAAAAAAAMBiSEIDAAAAAAAAAAAAACyGJDQAAAAAAAAAAAAAwGJIQgMAAAAAAAAAAAAALIYkNAAAAAAAAAAAAADAYkhCAwAAAAAAAAAAAAAshiQ0AAAAAAAAAAAAAMBi/j+8onCsfAao9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2400x350 with 1 Axes>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.plot(df_nixtla_new, y_hat_new, engine='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4728fa68-ba5d-494a-96fd-128917c67baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x350 with 1 Axes>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.plot(df_nixtla_new, y_hat_new2, engine='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0d1e0cb6-2117-420f-8324-1b6bb17d333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_pred = y_hat_new.groupby('ds')[['LSTM-median', 'LSTM-lo-90', 'LSTM-hi-90']].sum().reset_index()\n",
    "gr_true = df_nixtla_new[(df_nixtla_new.ds>'2022-05-01')].groupby('ds').y.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3b33632e-570f-4061-a058-49baee2a560a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(gr_pred.ds, gr_pred['LSTM-median'], label='pred')\n",
    "plt.plot(gr_pred.ds, gr_pred['LSTM-lo-90'], ls='--', color='black', lw=.5, label=' low pred')\n",
    "plt.plot(gr_pred.ds, gr_pred['LSTM-hi-90'], ls='--', color='green', lw=.5, label='high pred')\n",
    "plt.plot(gr_true.ds, gr_true['y'], label='true')\n",
    "plt.title('Aggregated sales comparison: transfer learning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c21362-0870-47d1-aece-5fb4dd09215d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_basic",
   "language": "python",
   "name": "env_basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
